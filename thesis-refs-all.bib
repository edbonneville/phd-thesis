@book{Aalen2008,
  title = {Survival and Event History Analysis: A Process Point of View},
  author = {Aalen, O. and Borgan, O. and Gjessing, H.},
  year = {2008},
  publisher = {Springer},
  address = {New York},
  doi = {10.1007/978-0-387-68560-1}
}

@article{aalenEmpiricalTransitionMatrix1978,
  title = {An {{Empirical Transition Matrix}} for {{Non-Homogeneous Markov Chains Based}} on {{Censored Observations}}},
  author = {Aalen, Odd O. and Johansen, S{\o}ren},
  year = {1978},
  journal = {Scandinavian Journal of Statistics},
  volume = {5},
  number = {3},
  eprint = {4615704},
  eprinttype = {jstor},
  pages = {141--150},
  publisher = {[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]},
  issn = {0303-6898},
  urldate = {2024-07-16},
  abstract = {A product limit estimator is suggested for the transition probabilities of a non-homogeneous Markov chain with finitely many states. The estimator is expressed as a product integral and its properties are studied by means of the theory of square integrable martingales.},
  file = {C:\Users\efbonneville\Zotero\storage\7RF6LLU4\Aalen and Johansen - 1978 - An Empirical Transition Matrix for Non-Homogeneous.pdf}
}

@article{acosta-medinaComparisonPretransplantationPrediction2023,
  title = {Comparison of {{Pretransplantation Prediction Models}} for {{Nonrelapse Mortality}} in {{Patients}} with {{Myelofibrosis Undergoing Allogeneic Stem Cell Transplantation}}},
  author = {{Acosta-Medina}, Aldo A. and Baranwal, Anmol and Johnson, Isla McKerrow and {Kharfan-Dabaja}, Mohamed A. and Murthy, Hemant and Palmer, Jeanne M. and Sproat, Lisa and Mangaonkar, Abhishek and Shah, Mithun V. and Hogan, William J. and Litzow, Mark R. and Tefferi, Ayalew and Alkhateeb, Hassan B.},
  year = {2023},
  month = jun,
  journal = {Transplantation and Cellular Therapy},
  volume = {29},
  number = {6},
  pages = {360.e1-360.e8},
  issn = {2666-6367},
  doi = {10.1016/j.jtct.2023.02.002},
  urldate = {2024-10-14},
  abstract = {Allogeneic stem cell transplantation (alloSCT) is the only known curative treatment for myelofibrosis (MF). Risk assessment remains important for patient counseling and predicting survival outcomes for relapse and nonrelapse mortality (NRM). Outcome-prediction tools can guide decision-making. Their use in MF has relied on their extrapolation from other malignancies. The primary objective of this study was to assess the performance of the Hematopoietic cell Transplantation Comorbidity Index (HCT-CI), the augmented HCT-CI (aHCT-CI), and the Endothelial Activation and Stress Index (EASIX) in predicting NRM in patients with MF undergoing alloSCT. We retrospectively reviewed patients with MF undergoing alloSCT between 2012 and 2020 at the Mayo Clinic. Data were abstracted from the electronic medical record. EASIX score was calculated before starting conditioning therapy and analyzed based on log2- transformed values. We evaluated the log2-EASIX scores by quartiles to assess the effect of increasing values on NRM. NRM was evaluated using competing risk analyses. We used the Kaplan-Meier and log-rank methods to evaluate OS. The Fine-Gray model was used to determine risk factors for NRM. The performance of HCT-CI and aHCT-CI was compared by evaluation of model concordance given the high correlation between HCT-CI and aHCT-CI (r~=~.75). A total of 87 patients were evaluated. The median duration of follow-up after alloSCT was 5 years (95\% confidence interval [CI], 4.4 to 6.31 years). Patients with a high HCT-CI score had significantly increased cumulative incidence of NRM at 3 years (35.5\% versus 11.6\%; P~=~.011) after alloSCT. A progressively increasing 3-year NRM was observed with increasing aHCT-CI risk category, and patients with a high or very high aHCT-CI score had significantly higher 3-year NRM compared to those with intermediate-risk or low-risk aHCT-CI scores at 3 years post-alloSCT (31.9\% versus 6.52\%; P~=~.004). An increasing log2-EASIX score quartile was not associated with 3-year NRM (19.0\% versus 10.1\% versus 25\% versus 14.3\%; P~=~.59), and the EASIX score was not found to be a predictor of post-transplantation NRM. A high HCT-CI was associated with significantly worse 3-year overall survival (OS) (hazard ratio [HR], 4.41; 95\% CI, 1.97 to 9.87; P {$<$} .001). A high or very high aHCT-CI was significantly associated with poor 3-year OS (HR, 3.99; 95\% CI, 1.56 to 10.22; P~=~.004). An increasing log2-EASIX score quartile group was not associated with 3-year OS (3-year OS rate, 66.7\% versus 80.4\% versus 64.6\% versus 76.2\%; P~=~.57). The EASIX score should not be used routinely in patients with MF. Both the HCT-CI and the aHCT-CI are accurate in predicting long-term survival outcomes in this patient population. Further studies are important to validate our findings of the role of EASIX in predicting NRM in patients with MF or other myeloproliferative neoplasms undergoing alloSCT. {\copyright} 2023 American Society for Transplantation and Cellular Therapy. Published by Elsevier Inc.},
  keywords = {EASIX,Hematopoietic cell transplantation,Myelofibrosis,Nonrelapse mortality},
  file = {C:\Users\efbonneville\Zotero\storage\DBZUJQMS\S2666636723010692.html}
}

@article{adesMyelodysplasticSyndromes2014,
  title = {Myelodysplastic Syndromes},
  author = {Ad{\`e}s, Lionel and Itzykson, Raphael and Fenaux, Pierre},
  year = {2014},
  month = jun,
  journal = {The Lancet},
  volume = {383},
  number = {9936},
  pages = {2239--2252},
  issn = {0140-6736},
  doi = {10.1016/S0140-6736(13)61901-7},
  urldate = {2020-10-26},
  abstract = {Myelodysplastic syndromes are clonal marrow stem-cell disorders, characterised by ineffective haemopoiesis leading to blood cytopenias, and by progression to acute myeloid leukaemia in a third of patients. 15\% of cases occur after chemotherapy or radiotherapy for a previous cancer; the syndromes are most common in elderly people. The pathophysiology involves cytogenetic changes with or without gene mutations and widespread gene hypermethylation at advanced stages. Clinical manifestations result from cytopenias (anaemia, infection, and bleeding). Diagnosis is based on examination of blood and bone marrow showing blood cytopenias and hypercellular marrow with dysplasia, with or without excess of blasts. Prognosis depends largely on the marrow blast percentage, number and extent of cytopenias, and cytogenetic abnormalities. Treatment of patients with lower-risk myelodysplastic syndromes, especially for anaemia, includes growth factors, lenalidomide, and transfusions. Treatment of higher-risk patients is with hypomethylating agents and, whenever possible, allogeneic stem-cell transplantation.},
  langid = {english}
}

@article{al-wahshAccountingCompetingRisk2021,
  title = {Accounting for the {{Competing Risk}} of {{Death}} to {{Predict Kidney Failure}} in {{Adults With Stage}} 4 {{Chronic Kidney Disease}}},
  author = {{Al-Wahsh}, Huda and Tangri, Navdeep and Quinn, Rob and Liu, Ping and Ferguson, Thomas, {\relax MS} and Fiocco, Marta and Lam, MSc, Ngan N., {\relax MD} and Tonelli, Marcello and Ravani, Pietro},
  year = {2021},
  month = may,
  journal = {JAMA Network Open},
  volume = {4},
  number = {5},
  pages = {e219225},
  issn = {2574-3805},
  doi = {10.1001/jamanetworkopen.2021.9225},
  urldate = {2023-10-06},
  abstract = {Kidney failure risk prediction has implications for disease management, including advance care planning in adults with severe (ie, estimated glomerular filtration rate [eGFR] category 4, [G4]) chronic kidney disease (G4-CKD). Existing prediction tools do not account for the competing risk of death.To compare predictions of kidney failure (defined as estimated glomerular filtration rate [eGFR] \&lt;10 mL/min/1.73 m2 or initiation of kidney replacement therapy) from models that do and do not account for the competing risk of death in adults with G4-CKD.This prognostic study linked population-based laboratory and administrative data (2002-2017) from 2 Canadian provinces (Alberta and Manitoba) to compare 3 kidney risk models: the standard Cox regression, cause-specific Cox regression, and Fine-Gray subdistribution hazard model. Participants were adults with incident G4-CKD (eGFR 15-29 mL/min/1.73 m2). Data analysis occurred between July and December 2020.The performance of kidney risk models at prespecified times and across categories of baseline characteristics, using calibration, reclassification, and discrimination (for competing risks). Predictive characteristics were age, sex, albuminuria, eGFR, diabetes, and cardiovascular disease.The development and validation cohorts included 14\,619 (7070 [48.4\%] men; mean [SD] age, 74.1 [12.8] years) and 2295 (1152 [50.2] men; mean [SD] age, 71.9 [14.0] years) adults, respectively. The 3 models had comparable calibration up to 2 years from entry. Beyond 2 years, the standard Cox regression overestimated the risk of kidney failure. At 4 years, for example, risks predicted from standard Cox were 40\% for people whose observed risks were less than 30\%. At 2 years (risk cutoffs 10\%-20\%) and 5 years (risk cutoffs 15\%-30\%), 788 (5.4\%) and 2162 (14.8\%) people in the development cohort were correctly reclassified into lower- or higher-risk categories by the Fine-Gray model and incorrectly reclassified by standard Cox regression (the opposite was observed in 272 patients [1.9\%] and 0 patients, respectively). In the validation cohort, 115 (5.0\%) individuals and 389 (16.9\%) individuals at 2 and 5 years, respectively, were correctly reclassified into lower- or higher-risk categories by the Fine-Gray model and incorrectly reclassified by the standard Cox regression; the opposite was observed in 98 (4.3\%) individuals and 0 individuals, respectively. Differences in discrimination emerged at 4 to 5 years in the development cohort and at 1 to 2 years in the validation cohort (0.85 vs 0.86 and 0.78 vs 0.8, respectively). Performance differences were minimal during the entire follow-up in people at lower risk of death (ie, aged {$\leq$}65 years or without cardiovascular disease or diabetes) and greater in those with a higher risk of death. At 5 years, for example, in people aged 65 years or older, predicted risks from standard Cox were 50\% where observed risks were less than 30\%. Similar miscalibration was observed at 5 years in people with albuminuria greater than 30 mg/mmol, diabetes, or cardiovascular disease.In this study, predictions about the risk of kidney failure were minimally affected by consideration of competing risks during the first 2 years after developing G4-CKD. However, traditional methods increasingly overestimated the risk of kidney failure with longer follow-up time, especially among older patients and those with more comorbidity.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\BYH5X2RH\\Al-Wahsh et al. - 2021 - Accounting for the Competing Risk of Death to Pred.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\FKNE3R8S\\2779439.html}
}

@article{allignolSoftwareFittingNonstandard2010,
  title = {Software for Fitting Nonstandard Proportional Subdistribution Hazards Models},
  author = {Allignol, Arthur and Beyersmann, Jan},
  year = {2010},
  month = oct,
  journal = {Biostatistics},
  volume = {11},
  number = {4},
  pages = {674--675},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxq018},
  urldate = {2023-11-21},
  abstract = {Fine and Gray (1999) suggested a Cox model for the subdistribution hazard, which is the hazard attached to the cumulative incidence function of interest in a competing risks setting. They also noted that standard Cox software may be used to fit the model provided that the potential censoring times are known for individuals observed to experience a competing event (`censoring complete data') and that otherwise an inverse probability of censoring weighting method can be used. Beyersmann and Schumacher (2008) extended the methods of Fine and Gray (1999) to allow for random time-dependent covariates.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\JSVP427M\\Allignol and Beyersmann - 2010 - Software for fitting nonstandard proportional subd.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\YW28CVPQ\\371401.html}
}

@article{allignolStatisticalIssuesAnalysis2016,
  title = {Statistical Issues in the Analysis of Adverse Events in Time-to-Event Data},
  author = {Allignol, Arthur and Beyersmann, Jan and Schmoor, Claudia},
  year = {2016},
  journal = {Pharmaceutical Statistics},
  volume = {15},
  number = {4},
  pages = {297--305},
  issn = {1539-1612},
  doi = {10.1002/pst.1739},
  urldate = {2024-12-11},
  abstract = {The aim of this work is to shed some light on common issues in the statistical analysis of adverse events (AEs) in clinical trials, when the main outcome is a time-to-event endpoint. To begin, we show that AEs are always subject to competing risks. That is, the occurrence of a certain AE may be precluded by occurrence of the main time-to-event outcome or by occurrence of another (fatal) AE. This has raised concerns on `informative' censoring. We show that, in general, neither simple proportions nor Kaplan--Meier estimates of AE occurrence should be used, but common survival techniques for hazards that censor the competing event are still valid, but incomplete analyses. They must be complemented by an analogous analysis of the competing event for inference on the cumulative AE probability. The commonly used incidence rate (or incidence density) is a valid estimator of the AE hazard assuming it to be time constant. An estimator of the cumulative AE probability can be derived if the incidence rate of AE is combined with an estimator of the competing hazard. We discuss less restrictive analyses using non-parametric and semi-parametric approaches. We first consider time-to-first-AE analyses and then briefly discuss how they can be extended to the analysis of recurrent AEs. We will give a practical presentation with illustration of the methods by a simple example. Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Aalen-Johansen,competing risks,safety,survival},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\9JK3BYNU\\Allignol et al. - 2016 - Statistical issues in the analysis of adverse even.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\YH69TBMX\\pst.html}
}

@article{allignolUnderstandingCompetingRisks2011,
  title = {Understanding Competing Risks: A Simulation Point of View},
  shorttitle = {Understanding Competing Risks},
  author = {Allignol, Arthur and Schumacher, Martin and Wanner, Christoph and Drechsler, Christiane and Beyersmann, Jan},
  year = {2011},
  month = jun,
  journal = {BMC Medical Research Methodology},
  volume = {11},
  number = {1},
  pages = {86},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-11-86},
  urldate = {2023-10-06},
  abstract = {Competing risks methodology allows for an event-specific analysis of the single components of composite time-to-event endpoints. A key feature of competing risks is that there are as many hazards as there are competing risks. This is not always well accounted for in the applied literature.},
  langid = {english},
  keywords = {Baseline Hazard,Compete Risk Analysis,Empirical Power,Simulation Algorithm,Simulation Point},
  file = {C:\Users\efbonneville\Zotero\storage\SG4QVEEM\Allignol et al. - 2011 - Understanding competing risks a simulation point .pdf}
}

@article{alsefriBayesianJointModelling2020a,
  title = {Bayesian Joint Modelling of Longitudinal and Time to Event Data: A Methodological Review},
  shorttitle = {Bayesian Joint Modelling of Longitudinal and Time to Event Data},
  author = {Alsefri, Maha and Sudell, Maria and {Garc{\'i}a-Fi{\~n}ana}, Marta and {Kolamunnage-Dona}, Ruwanthi},
  year = {2020},
  month = apr,
  journal = {BMC Medical Research Methodology},
  volume = {20},
  number = {1},
  pages = {94},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-00976-2},
  urldate = {2024-08-28},
  abstract = {In clinical research, there is an increasing interest in joint modelling of longitudinal and time-to-event data, since it reduces bias in parameter estimation and increases the efficiency of statistical inference. Inference and prediction from frequentist approaches of joint models have been extensively reviewed, and due to the recent popularity of data-driven Bayesian approaches, a review on current Bayesian estimation of joint model is useful to draw recommendations for future researches.},
  langid = {english},
  keywords = {Bayesian estimation,Dynamic prediction,Joint models,Longitudinal outcomes,Time-to-event},
  file = {C:\Users\efbonneville\Zotero\storage\G4YD6HTR\Alsefri et al. - 2020 - Bayesian joint modelling of longitudinal and time .pdf}
}

@article{andersenCompetingRisksEpidemiology2012,
  title = {Competing Risks in Epidemiology: Possibilities and Pitfalls},
  shorttitle = {Competing Risks in Epidemiology},
  author = {Andersen, Per Kragh and Geskus, Ronald B and {de Witte}, Theo and Putter, Hein},
  year = {2012},
  month = jun,
  journal = {International Journal of Epidemiology},
  volume = {41},
  number = {3},
  pages = {861--870},
  issn = {0300-5771},
  doi = {10.1093/ije/dyr213},
  urldate = {2023-10-06},
  abstract = {Background In studies of all-cause mortality, the fundamental epidemiological concepts of rate and risk are connected through a well-defined one-to-one relation. An important consequence of this relation is that regression models such as the proportional hazards model that are defined through the hazard (the rate) immediately dictate how the covariates relate to the survival function (the risk).Methods This introductory paper reviews the concepts of rate and risk and their one-to-one relation in all-cause mortality studies and introduces the analogous concepts of rate and risk in the context of competing risks, the cause-specific hazard and the cause-specific cumulative incidence function.Results The key feature of competing risks is that the one-to-one correspondence between cause-specific hazard and cumulative incidence, between rate and risk, is lost. This fact has two important implications. First, the na{\"i}ve Kaplan--Meier that takes the competing events as censored observations, is biased. Secondly, the way in which covariates are associated with the cause-specific hazards may not coincide with the way these covariates are associated with the cumulative incidence. An example with relapse and non-relapse mortality as competing risks in a stem cell transplantation study is used for illustration.Conclusion The two implications of the loss of one-to-one correspondence between cause-specific hazard and cumulative incidence should be kept in mind when deciding on how to make inference in a competing risks situation.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\INB7RQTG\\Andersen et al. - 2012 - Competing risks in epidemiology possibilities and.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\DC7IKSBH\\829598.html}
}

@article{andersenCompetingRisksMultistate2002,
  title = {Competing Risks as a Multi-State Model},
  author = {Andersen, Per Kragh and Abildstrom, Steen Z and Rosth{\o}j, Susanne},
  year = {2002},
  month = apr,
  journal = {Statistical Methods in Medical Research},
  volume = {11},
  number = {2},
  pages = {203--215},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1191/0962280202sm281ra},
  urldate = {2021-03-16},
  abstract = {This paper deals with the competing risks model as a special case of a multi-state model. The properties of the model are reviewed and contrasted to the so-called latent failure time approach. The relation between the competing risks model and right-censoring is discussed and regression analysis of the cumulative incidence function briefly reviewed. Two real data examples are presented and a guide to the practitioner is given.},
  langid = {english}
}

@article{andersenInterpretabilityImportanceFunctionals2012,
  title = {Interpretability and Importance of Functionals in Competing Risks and Multistate Models},
  author = {Andersen, Per Kragh and Keiding, Niels},
  year = {2012},
  journal = {Statistics in Medicine},
  volume = {31},
  number = {11-12},
  pages = {1074--1088},
  issn = {1097-0258},
  doi = {10.1002/sim.4385},
  urldate = {2023-10-06},
  abstract = {The basic parameters in both survival analysis and more general multistate models, including the competing risks model and the illness--death model, are the transition hazards. It is often necessary to supplement the analysis of such models with other model parameters, which are all functionals of the transition hazards. Unfortunately, not all such functionals are equally meaningful in practical contexts, even though they may be mathematically well defined. We have found it useful to check whether the functionals satisfy three simple principles, which may be used as criteria for practical interpretability. Copyright {\copyright} 2011 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2011 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {competing risks,illness-death model,survival analysis},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\B2GIRF3I\\Andersen and Keiding - 2012 - Interpretability and importance of functionals in .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\86U5AD6M\\sim.html}
}

@book{andersenModelsMultiStateSurvival2023,
  title = {Models for {{Multi-State Survival Data}}: {{Rates}}, {{Risks}}, and {{Pseudo-Values}}},
  shorttitle = {Models for {{Multi-State Survival Data}}},
  author = {Andersen, Per Kragh and Ravn, Henrik},
  year = {2023},
  month = oct,
  publisher = {CRC Press},
  abstract = {Multi-state models provide a statistical framework for studying longitudinal data on subjects when focus is on the occurrence of events that the subjects may experience over time. They find application particularly in biostatistics, medicine, and public health. The book includes mathematical detail which can be skipped by readers more interested in the practical examples. It is aimed at biostatisticians and at readers with an interest in the topic having a more applied background, such as epidemiology. This book builds on several courses the authors have taught on the subject. Key Features:  Intensity-based and marginal models Survival data, competing risks, illness-death models, recurrent events Includes a full chapter on pseudo-values Intuitive introductions and mathematical details Practical examples of event history data Exercises Software code in R and SAS and the data used in the book can be found on the book's webpage.},
  googlebooks = {ex7QEAAAQBAJ},
  isbn = {978-0-429-64226-5},
  langid = {english},
  keywords = {Mathematics / Probability & Statistics / General,Reference / General}
}

@article{andrinopoulouDynamicPredictionOutcome2015,
  title = {Dynamic Prediction of Outcome for Patients with Severe Aortic Stenosis: Application of Joint Models for Longitudinal and Time-to-Event Data},
  shorttitle = {Dynamic Prediction of Outcome for Patients with Severe Aortic Stenosis},
  author = {Andrinopoulou, Eleni-Rosalina and Rizopoulos, Dimitris and Geleijnse, Marcel L. and Lesaffre, Emmanuel and Bogers, Ad J. J. C. and Takkenberg, Johanna J. M.},
  year = {2015},
  month = may,
  journal = {BMC Cardiovascular Disorders},
  volume = {15},
  number = {1},
  pages = {28},
  issn = {1471-2261},
  doi = {10.1186/s12872-015-0035-z},
  urldate = {2023-11-04},
  abstract = {Physicians utilize different types of information to predict patient prognosis. For example: confronted with a new patient suffering from severe aortic stenosis (AS), the cardiologist considers not only the severity of the AS but also patient characteristics, medical history, and markers such as BNP. Intuitively, doctors adjust their prediction of prognosis over time, with the change in clinical status, aortic valve area and BNP at each outpatient clinic visit. With the help of novel statistical approaches to model outcomes, it is now possible to construct dynamic event prediction models, employing longitudinal data such as AVA and BNP, and mimicking the dynamic adjustment of prognosis as employed intuitively by cardiologists. We illustrate dynamic prediction of patient survival and freedom from intervention, using baseline patient characteristics and longitudinal BNP data that are becoming available over time, from a cohort of patients with severe aortic stenosis.},
  keywords = {Aortic valve disease,Brain natriuretic peptide,Individualized prediction,Survival,Valvular disease},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\XILL96XZ\\Andrinopoulou et al. - 2015 - Dynamic prediction of outcome for patients with se.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\8DQTG2ZZ\\s12872-015-0035-z.html}
}

@article{antunesDealingMissingInformation2021,
  title = {Dealing with Missing Information on Covariates for Excess Mortality Hazard Regression Models -- {{Making}} the Imputation Model Compatible with the Substantive Model},
  author = {Antunes, Lu{\'i}s and Mendon{\c c}a, Denisa and Bento, Maria Jos{\'e} and Njagi, Edmund Njeru and Belot, Aur{\'e}lien and Rachet, Bernard},
  year = {2021},
  month = oct,
  journal = {Statistical Methods in Medical Research},
  volume = {30},
  number = {10},
  pages = {2256--2268},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/09622802211031615},
  urldate = {2023-05-13},
  abstract = {Missing data is a common issue in epidemiological databases. Among the different ways of dealing with missing data, multiple imputation has become more available in common statistical software packages. However, the incompatibility between the imputation and substantive model, which can arise when the associations between variables in the substantive model are not taken into account in the imputation models or when the substantive model is itself nonlinear, can lead to invalid inference. Aiming at analysing population-based cancer survival data, we extended the multiple imputation substantive model compatible-fully conditional specification (SMC-FCS) approach, proposed by Bartlett et~al. in 2015 to accommodate excess hazard regression models. The proposed approach was compared with the standard fully conditional specification multiple imputation procedure and with the complete-case analysis using a simulation study. The SMC-FCS approach produced unbiased estimates in both scenarios tested, while the fully conditional specification produced biased estimates and poor empirical coverages probabilities. The SMC-FCS algorithm was then used for handling missing data in the evaluation of socioeconomic inequalities in survival from colorectal cancer patients diagnosed in the North Region of Portugal. The analysis using SMC-FCS showed a clearer trend in higher excess hazards for patients coming from more deprived areas. The proposed algorithm was implemented in R software and is presented as Supplementary Material.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\P4RIW6KI\Antunes et al. - 2021 - Dealing with missing information on covariates for.pdf}
}

@article{archerDevelopmentExternalValidation2022,
  title = {Development and External Validation of a Risk Prediction Model for Falls in Patients with an Indication for Antihypertensive Treatment: Retrospective Cohort Study},
  shorttitle = {Development and External Validation of a Risk Prediction Model for Falls in Patients with an Indication for Antihypertensive Treatment},
  author = {Archer, Lucinda and Koshiaris, Constantinos and {Lay-Flurrie}, Sarah and Snell, Kym I. E. and Riley, Richard D. and Stevens, Richard and Banerjee, Amitava and {Usher-Smith}, Juliet A. and Clegg, Andrew and Payne, Rupert A. and Hobbs, F. D. Richard and McManus, Richard J. and Sheppard, James P.},
  year = {2022},
  month = nov,
  journal = {BMJ},
  volume = {379},
  pages = {e070918},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj-2022-070918},
  urldate = {2023-11-06},
  abstract = {Objective To develop and externally validate the STRAtifying Treatments In the multi-morbid Frail elderlY (STRATIFY)-Falls clinical prediction model to identify the risk of hospital admission or death from a fall in patients with an indication for antihypertensive treatment. Design Retrospective cohort study. Setting Primary care data from electronic health records contained within the UK Clinical Practice Research Datalink (CPRD). Participants Patients aged 40 years or older with at least one blood pressure measurement between 130 mm Hg and 179 mm Hg. Main outcome measure First serious fall, defined as hospital admission or death with a primary diagnosis of a fall within 10 years of the index date (12 months after cohort entry). Model development was conducted using a Fine-Gray approach in data from CPRD GOLD, accounting for the competing risk of death from other causes, with subsequent recalibration at five and 10 years using pseudo values. External validation was conducted using data from CPRD Aurum, with performance assessed through calibration curves and the observed to expected ratio, C statistic, and D statistic, pooled across general practices, and clinical utility using decision curve analysis at thresholds around 10\%. Results Analysis included 1 772 600 patients (experiencing 62 691 serious falls) from CPRD GOLD used in model development, and 3 805 366 (experiencing 206 956 serious falls) from CPRD Aurum in the external validation. The final model consisted of 24 predictors, including age, sex, ethnicity, alcohol consumption, living in an area of high social deprivation, a history of falls, multiple sclerosis, and prescriptions of antihypertensives, antidepressants, hypnotics, and anxiolytics. Upon external validation, the recalibrated model showed good discrimination, with pooled C statistics of 0.843 (95\% confidence interval 0.841 to 0.844) and 0.833 (0.831 to 0.835) at five and 10 years, respectively. Original model calibration was poor on visual inspection and although this was improved with recalibration, under-prediction of risk remained (observed to expected ratio at 10 years 1.839, 95\% confidence interval 1.811 to 1.865). Nevertheless, decision curve analysis suggests potential clinical utility, with net benefit larger than other strategies. Conclusions This prediction model uses commonly recorded clinical characteristics and distinguishes well between patients at high and low risk of falls in the next 1-10 years. Although miscalibration was evident on external validation, the model still had potential clinical utility around risk thresholds of 10\% and so could be useful in routine clinical practice to help identify those at high risk of falls who might benefit from closer monitoring or early intervention to prevent future falls. Further studies are needed to explore the appropriate thresholds that maximise the model's clinical utility and cost effectiveness.},
  chapter = {Research},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  pmid = {36347531},
  file = {C:\Users\efbonneville\Zotero\storage\Y4HGZVNR\Archer et al. - 2022 - Development and external validation of a risk pred.pdf}
}

@article{armandDiseaseRiskIndex2012,
  title = {A Disease Risk Index for Patients Undergoing Allogeneic Stem Cell Transplantation},
  author = {Armand, Philippe and Gibson, Christopher J. and Cutler, Corey and Ho, Vincent T. and Koreth, John and Alyea, Edwin P. and Ritz, Jerome and Sorror, Mohamed L. and Lee, Stephanie J. and Deeg, H. Joachim and Storer, Barry E. and Appelbaum, Frederick R. and Antin, Joseph H. and Soiffer, Robert J. and Kim, Haesook T.},
  year = {2012},
  month = jul,
  journal = {Blood},
  volume = {120},
  number = {4},
  pages = {905--913},
  issn = {0006-4971},
  doi = {10.1182/blood-2012-03-418202},
  urldate = {2022-11-01},
  abstract = {The outcome of allogeneic HSCT varies considerably by the disease and remission status at the time of transplantation. Any retrospective or prospective HSCT study that enrolls patients across disease types must account for this heterogeneity; yet, current methods are neither standardized nor validated. We conducted a retrospective study of 1539 patients who underwent transplantation at Dana-Farber Cancer Institute/Brigham and Women's Hospital from 2000 to 2009. Using multivariable models for overall survival, we created a disease risk index. This tool uses readily available information about disease and disease status to categorize patients into 4 risk groups with significantly different overall survival and progression-free survival on the basis of primarily differences in the relapse risk. This scheme applies regardless of conditioning intensity, is independent of comorbidity index, and was validated in an independent cohort of 672 patients from the Fred Hutchinson Cancer Research Center. This simple and validated scheme could be used to risk-stratify patients in both retrospective and prospective HSCT studies, to calibrate HSCT outcomes across studies and centers, and to promote the design of HSCT clinical trials that enroll patients across diseases and disease states, increasing our ability to study nondisease-specific outcomes in HSCT.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\RGANG76J\\Armand et al. - 2012 - A disease risk index for patients undergoing allog.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\DUSBPDCF\\A-disease-risk-index-for-patients-undergoing.html}
}

@article{armandValidationRefinementDisease2014,
  title = {Validation and Refinement of the {{Disease Risk Index}} for Allogeneic Stem Cell Transplantation},
  author = {Armand, Philippe and Kim, Haesook T. and Logan, Brent R. and Wang, Zhiwei and Alyea, Edwin P. and Kalaycio, Matt E. and Maziarz, Richard T. and Antin, Joseph H. and Soiffer, Robert J. and Weisdorf, Daniel J. and Rizzo, J. Douglas and Horowitz, Mary M. and Saber, Wael},
  year = {2014},
  month = jun,
  journal = {Blood},
  volume = {123},
  number = {23},
  pages = {3664--3671},
  issn = {0006-4971},
  doi = {10.1182/blood-2014-01-552984},
  urldate = {2022-11-03},
  abstract = {Because the outcome of allogeneic hematopoietic cell transplantation (HCT) is predominantly influenced by disease type and status, it is essential to be able to stratify patients undergoing HCT by disease risk. The Disease Risk Index (DRI) was developed for this purpose. In this study, we analyzed 13\,131 patients reported to the Center for International Blood and Marrow Transplant Research who underwent HCT between 2008 and 2010. The DRI stratified patients into 4 groups with 2-year overall survival (OS) ranging from 64\% to 24\% and was the strongest prognostic factor, regardless of age, conditioning intensity, graft source, or donor type. A randomly selected training subgroup of 9849 patients was used to refine the DRI, using a multivariable regression model for OS. This refined DRI had improved prediction ability for the remaining 3282 patients compared with the original DRI or other existing schemes. This validated and refined DRI can be used as a 4- or 3-group index, depending on the size of the cohort under study, for prognostication; to facilitate the interpretation of single-center, multicenter, or registry studies; to adjust center outcome data; and to stratify patients entering clinical trials that enroll patients across disease categories.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\YTKNSM6Z\\Armand et al. - 2014 - Validation and refinement of the Disease Risk Inde.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\MFT5J76N\\Validation-and-refinement-of-the-Disease-Risk.html}
}

@article{austinFineGraySubdistributionHazard2021,
  title = {Fine-{{Gray}} Subdistribution Hazard Models to Simultaneously Estimate the Absolute Risk of Different Event Types: {{Cumulative}} Total Failure Probability May Exceed 1},
  shorttitle = {Fine-{{Gray}} Subdistribution Hazard Models to Simultaneously Estimate the Absolute Risk of Different Event Types},
  author = {Austin, Peter C. and Steyerberg, Ewout W. and Putter, Hein},
  year = {2021},
  journal = {Statistics in Medicine},
  volume = {40},
  number = {19},
  pages = {4200--4212},
  issn = {1097-0258},
  doi = {10.1002/sim.9023},
  urldate = {2021-09-20},
  abstract = {The Fine-Gray subdistribution hazard model has become the default method to estimate the incidence of outcomes over time in the presence of competing risks. This model is attractive because it directly relates covariates to the cumulative incidence function (CIF) of the event of interest. An alternative is to combine the different cause-specific hazard functions to obtain the different CIFs. A limitation of the subdistribution hazard approach is that the sum of the cause-specific CIFs can exceed 1 (100\%) for some covariate patterns. Using data on 9479 patients hospitalized with acute myocardial infarction, we estimated the cumulative incidence of both cardiovascular death and non-cardiovascular death for each patient. We found that when using subdistribution hazard models, approximately 5\% of subjects had an estimated risk of 5-year all-cause death (obtained by combining the two cause-specific CIFs obtained from subdistribution hazard models) that exceeded 1. This phenomenon was avoided by using the two cause-specific hazard models. We provide a proof that the sum of predictions exceeds 1 is a fundamental problem with the Fine-Gray subdistribution hazard model. We further explored this issue using simulations based on two different types of data-generating process, one based on subdistribution hazard models and other based on cause-specific hazard models. We conclude that care should be taken when using the Fine-Gray subdistribution hazard model in situations with wide risk distributions or a high cumulative incidence, and if one is interested in the risk of failure from each of the different event types.},
  langid = {english},
  keywords = {cause-specific hazard function,competing risks,cumulative incidence function,subdistribution hazard,survival analysis},
  file = {C:\Users\efbonneville\Zotero\storage\IGNAJMHV\Austin et al. - 2021 - Fine-Gray subdistribution hazard models to simulta.pdf}
}

@article{austinImputethenexcludeExcludethenimputeLessons2023,
  title = {Impute-Then-Exclude versus Exclude-Then-Impute: {{Lessons}} When Imputing a Variable Used Both in Cohort Creation and as an Independent Variable in the Analysis Model},
  shorttitle = {Impute-Then-Exclude versus Exclude-Then-Impute},
  author = {Austin, Peter C. and Giardiello, Daniele and {van Buuren}, Stef},
  year = {2023},
  journal = {Statistics in Medicine},
  volume = {42},
  number = {10},
  pages = {1525--1541},
  issn = {1097-0258},
  doi = {10.1002/sim.9685},
  urldate = {2024-10-03},
  abstract = {We examined the setting in which a variable that is subject to missingness is used both as an inclusion/exclusion criterion for creating the analytic sample and subsequently as the primary exposure in the analysis model that is of scientific interest. An example is cancer stage, where patients with stage IV cancer are often excluded from the analytic sample, and cancer stage (I to III) is an exposure variable in the analysis model. We considered two analytic strategies. The first strategy, referred to as ``exclude-then-impute,'' excludes subjects for whom the observed value of the target variable is equal to the specified value and then uses multiple imputation to complete the data in the resultant sample. The second strategy, referred to as ``impute-then-exclude,'' first uses multiple imputation to complete the data and then excludes subjects based on the observed or filled-in values in the completed samples. Monte Carlo simulations were used to compare five methods (one based on ``exclude-then-impute'' and four based on ``impute-then-exclude'') along with the use of a complete case analysis. We considered both missing completely at random and missing at random missing data mechanisms. We found that an impute-then-exclude strategy using substantive model compatible fully conditional specification tended to have superior performance across 72 different scenarios. We illustrated the application of these methods using empirical data on patients hospitalized with heart failure when heart failure subtype was used for cohort creation (excluding subjects with heart failure with preserved ejection fraction) and was also an exposure in the analysis model.},
  copyright = {{\copyright} 2023 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {missing data,Monte Carlo simulations,multiple imputation},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\6JL3XTF7\\Austin et al. - 2023 - Impute-then-exclude versus exclude-then-impute Le.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\TT4PX3Y8\\sim.html}
}

@article{austinMultipleImputationCompeting2024,
  title = {Multiple Imputation with Competing Risk Outcomes},
  author = {Austin, Peter C.},
  year = {2024},
  month = jun,
  journal = {Computational Statistics},
  issn = {1613-9658},
  doi = {10.1007/s00180-024-01518-w},
  urldate = {2024-10-03},
  abstract = {In time-to-event analyses, a competing risk is an event whose occurrence precludes the occurrence of the event of interest. Settings with competing risks occur frequently in clinical research. Missing data, which is a common problem in research, occurs when the value of a variable is recorded for some, but not all, records in the dataset. Multiple Imputation (MI) is a popular method to address the presence of missing data. MI uses an imputation model to generate M (M\,{$>$}\,1) values for each variable that is missing, resulting in the creation of M complete datasets. A popular algorithm for imputing missing data is multivariate imputation using chained equations (MICE). We used a complex simulation design with covariates and missing data patterns reflective of patients hospitalized with acute myocardial infarction (AMI) to compare three strategies for imputing missing predictor variables when the analysis model is a cause-specific hazard when there were three different event types. We compared two MICE-based strategies that differed according to which cause-specific cumulative hazard functions were included in the imputation models (the three cause-specific cumulative hazard functions vs. only the cause-specific cumulative hazard function for the primary outcome) with the use of the substantive model compatible fully conditional specification (SMCFCS) algorithm. While no strategy had consistently superior performance compared to the other strategies, SMCFCS may be the preferred strategy. We illustrated the application of the strategies using a case study of patients hospitalized with AMI.},
  langid = {english},
  keywords = {Competing risks,Missing data,Monte Carlo simulations,Multiple imputation,Survival analysis},
  file = {C:\Users\efbonneville\Zotero\storage\7L9893R2\Austin - 2024 - Multiple imputation with competing risk outcomes.pdf}
}

@article{austinPracticalRecommendationsReporting2017,
  title = {Practical Recommendations for Reporting {{Fine-Gray}} Model Analyses for Competing Risk Data},
  author = {Austin, Peter C. and Fine, Jason P.},
  year = {2017},
  journal = {Statistics in Medicine},
  volume = {36},
  number = {27},
  pages = {4391--4400},
  issn = {1097-0258},
  doi = {10.1002/sim.7501},
  urldate = {2023-10-06},
  abstract = {In survival analysis, a competing risk is an event whose occurrence precludes the occurrence of the primary event of interest. Outcomes in medical research are frequently subject to competing risks. In survival analysis, there are 2 key questions that can be addressed using competing risk regression models: first, which covariates affect the rate at which events occur, and second, which covariates affect the probability of an event occurring over time. The cause-specific hazard model estimates the effect of covariates on the rate at which events occur in subjects who are currently event-free. Subdistribution hazard ratios obtained from the Fine-Gray model describe the relative effect of covariates on the subdistribution hazard function. Hence, the covariates in this model can also be interpreted as having an effect on the cumulative incidence function or on the probability of events occurring over time. We conducted a review of the use and interpretation of the Fine-Gray subdistribution hazard model in articles published in the medical literature in 2015. We found that many authors provided an unclear or incorrect interpretation of the regression coefficients associated with this model. An incorrect and inconsistent interpretation of regression coefficients may lead to confusion when comparing results across different studies. Furthermore, an incorrect interpretation of estimated regression coefficients can result in an incorrect understanding about the magnitude of the association between exposure and the incidence of the outcome. The objective of this article is to clarify how these regression coefficients should be reported and to propose suggestions for interpreting these coefficients.},
  copyright = {{\copyright} 2017 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {competing risks,cumulative incidence function,subdistribution hazard model,survival analysis},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\Z69I5NFC\\Austin and Fine - 2017 - Practical recommendations for reporting Fine-Gray .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\HTNSJL9J\\sim.html}
}

@article{baghestaniImproperFormWeibull2019,
  title = {An Improper Form of {{Weibull}} Distribution for Competing Risks Analysis with {{Bayesian}} Approach},
  author = {Baghestani, A. R. and {Hosseini-Baharanchi}, F. S.},
  year = {2019},
  month = oct,
  journal = {Journal of Applied Statistics},
  volume = {46},
  number = {13},
  pages = {2409--2417},
  publisher = {Taylor \& Francis},
  issn = {0266-4763},
  doi = {10.1080/02664763.2019.1597027},
  urldate = {2023-10-06},
  abstract = {In survival analysis, individuals may fail due to multiple causes of failure called competing risks setting. Parametric models such as Weibull model are not improper that ignore the assumption of multiple failure times. In this study, a novel extension of Weibull distribution is proposed which is improper and then can incorporate to the competing risks framework. This model includes the original Weibull model before a pre-specified time point and an exponential form for the tail of the time axis. A Bayesian approach is used for parameter estimation. A simulation study is performed to evaluate the proposed model. The conducted simulation study showed identifiability and appropriate convergence of the proposed model. The proposed model and the 3-parameter Gompertz model, another improper parametric distribution, are fitted to the acute lymphoblastic leukemia dataset.},
  keywords = {Bayesian analysis,Competing risks,improper distribution,parametric survival modeling,Weibull distribution},
  file = {C:\Users\efbonneville\Zotero\storage\WKJ6IFHV\Baghestani and Hosseini-Baharanchi - 2019 - An improper form of Weibull distribution for compe.pdf}
}

@article{bakerSystematicReviewReporting2024,
  title = {A {{Systematic Review}} of {{Reporting}} and {{Handling}} of {{Missing Data}} in {{Observational Studies Using}} the {{UNOS Database}}},
  author = {Baker, William L. and Moore, Timothy and Baron, Eric and Kittleson, Michelle and Parker, William F. and Jaiswal, Abhishek},
  year = {2024},
  month = nov,
  journal = {The Journal of Heart and Lung Transplantation},
  issn = {1053-2498},
  doi = {10.1016/j.healun.2024.10.023},
  urldate = {2024-11-18},
  abstract = {Background Missing data can undermine a registry's ability to draw valid inferences by decreasing study power and introducing bias. We evaluated how missing data are reported and addressed in observational studies of adult heart transplantation (HT) recipients using the United Network for Organ Sharing (UNOS) database. Methods We conducted a systematic literature search of Medline from January 1st, 2018 through August 22nd, 2023 and included studies that used the UNOS database to evaluate adult ({$\geq$}18 years) de novo HT recipients. We collected details on the study population, timeframe, primary endpoint, use of missing data, and whether and what methods were used to handle missing data. Approaches were classified as variable selection, complete case analysis (CCA), missing indicator method, single imputation, or multiple imputation. Results 229 studies were included from 2018-2023. 67 (29.3\%) studies limited their cohorts to those without missing data for the outcome or key variables and 93 (40.6\%) reported missing data in their final cohort. 78 (34.1\%) studies reported how they handled missing data in their statistical modeling. Of these, CCA was most used (n=41, 52.6\%) followed by multiple imputation (n=22, 28.2\%), and other methods (n=15, 19.2\%) including a combination of missing indicator method (for binary variables) and single imputation. Thirty-one (13.5\%) studies reported removing covariates from their analysis because of missing data. Only four (5.1\%) studies reported sensitivity analysis to evaluate different missing data approaches. Conclusions Merely a third of the identified UNOS database studies reported how they handled missing data in their analysis, with strategies varying. Although no singular approach to handling missing data exists, methods are available that can improve upon the most used approaches by, for example, reducing bias and increasing statistical power. Though these improvements are contingent on the missing data mechanism and the goals of the analysis. Future best practices should include explicit reporting of missingness, detailed methods, and sensitivity checks to maximize the credibility of UNOS database analyses.},
  keywords = {Heart transplantation,Missing Data,UNOS},
  file = {C:\Users\efbonneville\Zotero\storage\C2E6V38W\S1053249824019326.html}
}

@article{bakoyannisModellingCompetingRisks2010,
  title = {Modelling Competing Risks Data with Missing Cause of Failure},
  author = {Bakoyannis, Giorgos and Siannis, Fotios and Touloumi, Giota},
  year = {2010},
  journal = {Statistics in Medicine},
  volume = {29},
  number = {30},
  pages = {3172--3185},
  issn = {1097-0258},
  doi = {10.1002/sim.4133},
  urldate = {2021-03-16},
  abstract = {When competing risks data arise, information on the actual cause of failure for some subjects might be missing. Therefore, a cause-specific proportional hazards model together with multiple imputation (MI) methods have been used to analyze such data. Modelling the cumulative incidence function is also of interest, and thus we investigate the proportional subdistribution hazards model (Fine and Gray model) together with MI methods as a modelling approach for competing risks data with missing cause of failure. Possible strategies for analyzing such data include the complete case analysis as well as an analysis where the missing causes are classified as an additional failure type. These approaches, however, may produce misleading results in clinical settings. In the present work we investigate the bias of the parameter estimates when fitting the Fine and Gray model in the above modelling approaches. We also apply the MI method and evaluate its comparative performance under various missing data scenarios. Results from simulation experiments showed that there is substantial bias in the estimates when fitting the Fine and Gray model with naive techniques for missing data, under missing at random cause of failure. Compared to those techniques the MI-based method gave estimates with much smaller biases and coverage probabilities of 95 per cent confidence intervals closer to the nominal level. All three methods were also applied on real data modelling time to AIDS or non-AIDS cause of death in HIV-1 infected individuals. Copyright 2010 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {competing risks,cumulative incidence,Fine and Gray model,missing cause of failure,multiple imputations}
}

@article{bakoyannisPracticalMethodsCompeting2012,
  title = {Practical Methods for Competing Risks Data: {{A}} Review},
  shorttitle = {Practical Methods for Competing Risks Data},
  author = {Bakoyannis, Giorgos and Touloumi, Giota},
  year = {2012},
  month = jun,
  journal = {Statistical Methods in Medical Research},
  volume = {21},
  number = {3},
  pages = {257--272},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280210394479},
  urldate = {2021-09-28},
  abstract = {Competing risks data arise naturally in medical research, when subjects under study are at risk of more than one mutually exclusive event such as death from different causes. The competing risks framework also includes settings where different possible events are not mutually exclusive but the interest lies on the first occurring event. For example, in HIV studies where seropositive subjects are receiving highly active antiretroviral therapy (HAART), treatment interruption and switching to a new HAART regimen act as competing risks for the first major change in HAART. This article introduces competing risks data and critically reviews the widely used statistical methods for estimation and modelling of the basic (estimable) quantities of interest. We discuss the increasingly popular Fine and Gray model for subdistribution hazard of interest, which can be readily fitted using standard software under the assumption of administrative censoring. We present a simulation study, which explores the robustness of inference for the subdistribution hazard to the assumption of administrative censoring. This shows a range of scenarios within which the strictly incorrect assumption of administrative censoring has a relatively small effect on parameter estimates and confidence interval coverage. The methods are illustrated using data from HIV-1 seropositive patients from the collaborative multicentre study CASCADE (Concerted Action on SeroConversion to AIDS and Death in Europe).},
  langid = {english},
  keywords = {cause-specific hazard,competing risks,cumulative incidence,Fine and Gray model}
}

@article{bartlettMissingCovariatesCompeting2016,
  title = {Missing Covariates in Competing Risks Analysis},
  author = {Bartlett, Jonathan W. and Taylor, Jeremy M. G.},
  year = {2016},
  month = oct,
  journal = {Biostatistics},
  volume = {17},
  number = {4},
  pages = {751--763},
  publisher = {Oxford Academic},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxw019},
  urldate = {2020-03-31},
  abstract = {Abstract.  Studies often follow individuals until they fail from one of a number of competing failure types. One approach to analyzing such competing risks data},
  langid = {english},
  keywords = {missing-data},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\DUN3VNLD\\Bartlett and Taylor - 2016 - Missing covariates in competing risks analysis.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\P9DP2TVT\\2800456.html}
}

@article{bartlettMultipleImputationCovariates2015,
  title = {Multiple Imputation of Covariates by Fully Conditional Specification: {{Accommodating}} the Substantive Model},
  shorttitle = {Multiple Imputation of Covariates by Fully Conditional Specification},
  author = {Bartlett, Jonathan W and Seaman, Shaun R and White, Ian R and Carpenter, James R},
  year = {2015},
  month = aug,
  journal = {Statistical Methods in Medical Research},
  volume = {24},
  number = {4},
  pages = {462--487},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280214521348},
  urldate = {2020-04-29},
  abstract = {Missing covariate data commonly occur in epidemiological and clinical research, and are often dealt with using multiple imputation. Imputation of partially observed covariates is complicated if the substantive model is non-linear (e.g. Cox proportional hazards model), or contains non-linear (e.g. squared) or interaction terms, and standard software implementations of multiple imputation may impute covariates from models that are incompatible with such substantive models. We show how imputation by fully conditional specification, a popular approach for performing multiple imputation, can be modified so that covariates are imputed from models which are compatible with the substantive model. We investigate through simulation the performance of this proposal, and compare it with existing approaches. Simulation results suggest our proposal gives consistent estimates for a range of common substantive models, including models which contain non-linear covariate effects or interactions, provided data are missing at random and the assumed imputation models are correctly specified and mutually compatible. Stata software implementing the approach is freely available.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\QH7629W2\Bartlett et al. - 2015 - Multiple imputation of covariates by fully conditi.pdf}
}

@manual{bartlettSmcfcsMultipleImputation2022,
  type = {Manual},
  title = {Smcfcs: {{Multiple}} Imputation of Covariates by Substantive Model Compatible Fully Conditional Specification},
  author = {Bartlett, Jonathan W. and Keogh, Ruth H. and Bonneville, Edouard F.},
  year = {2022}
}

@article{beesleyMultipleImputationMissing2016,
  title = {Multiple Imputation of Missing Covariates for the {{Cox}} Proportional Hazards Cure Model},
  author = {Beesley, Lauren J. and Bartlett, Jonathan W. and Wolf, Gregory T. and Taylor, Jeremy M. G.},
  year = {2016},
  journal = {Statistics in Medicine},
  volume = {35},
  number = {26},
  pages = {4701--4717},
  issn = {1097-0258},
  doi = {10.1002/sim.7048},
  urldate = {2024-11-18},
  abstract = {We explore several approaches for imputing partially observed covariates when the outcome of interest is a censored event time and when there is an underlying subset of the population that will never experience the event of interest. We call these subjects `cured', and we consider the case where the data are modeled using a Cox proportional hazards (CPH) mixture cure model. We study covariate imputation approaches using fully conditional specification. We derive the exact conditional distribution and suggest a sampling scheme for imputing partially observed covariates in the CPH cure model setting. We also propose several approximations to the exact distribution that are simpler and more convenient to use for imputation. A simulation study demonstrates that the proposed imputation approaches outperform existing imputation approaches for survival data without a cure fraction in terms of bias in estimating CPH cure model parameters. We apply our multiple imputation techniques to a study of patients with head and neck cancer. Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {cure models,fully conditional specification,multiple imputation},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\8KJXBUVH\\Beesley et al. - 2016 - Multiple imputation of missing covariates for the .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\6TZGYRUG\\sim.html}
}

@article{behningRandomSurvivalForests2024,
  title = {Random {{Survival Forests With Competing Events}}: {{A Subdistribution-Based Imputation Approach}}},
  shorttitle = {Random {{Survival Forests With Competing Events}}},
  author = {Behning, Charlotte and Bigerl, Alexander and Wright, Marvin N. and Sekula, Peggy and Berger, Moritz and Schmid, Matthias},
  year = {2024},
  journal = {Biometrical Journal},
  volume = {66},
  number = {6},
  pages = {e202400014},
  issn = {1521-4036},
  doi = {10.1002/bimj.202400014},
  urldate = {2024-08-25},
  abstract = {Random survival forests (RSF) can be applied to many time-to-event research questions and are particularly useful in situations where the relationship between the independent variables and the event of interest is rather complex. However, in many clinical settings, the occurrence of the event of interest is affected by competing events, which means that a patient can experience an outcome other than the event of interest. Neglecting the competing event (i.e., regarding competing events as censoring) will typically result in biased estimates of the cumulative incidence function (CIF). A popular approach for competing events is Fine and Gray's subdistribution hazard model, which directly estimates the CIF by fitting a single-event model defined on a subdistribution timescale. Here, we integrate concepts from the subdistribution hazard modeling approach into the RSF. We develop several imputation strategies that use weights as in a discrete-time subdistribution hazard model to impute censoring times in cases where a competing event is observed. Our simulations show that the CIF is well estimated if the imputation already takes place outside the forest on the overall dataset. Especially in settings with a low rate of the event of interest or a high censoring rate, competing events must not be neglected, that is, treated as censoring. When applied to a real-world epidemiological dataset on chronic kidney disease, the imputation approach resulted in highly plausible predictor--response relationships and CIF estimates of renal events.},
  copyright = {{\copyright} 2024 The Author(s). Biometrical Journal published by Wiley-VCH GmbH.},
  langid = {english},
  keywords = {competing events,discrete time-to-event data,imputation,random survival forest,subdistribution hazard},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\WCUPXUF2\\Behning et al. - 2024 - Random Survival Forests With Competing Events A S.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\ILG2MNGE\\bimj.html}
}

@article{bellachWeightedNPMLESubdistribution2019,
  title = {Weighted {{NPMLE}} for the {{Subdistribution}} of a {{Competing Risk}}},
  author = {Bellach, Anna and Kosorok, Michael R. and R{\"u}schendorf, Ludger and Fine, Jason P.},
  year = {2019},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {114},
  number = {525},
  pages = {259--270},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2017.1401540},
  urldate = {2024-03-04},
  abstract = {Direct regression modeling of the subdistribution has become popular for analyzing data with multiple, competing event types. All general approaches so far are based on nonlikelihood-based procedures and target covariate effects on the subdistribution. We introduce a novel weighted likelihood function that allows for a direct extension of the Fine--Gray model to a broad class of semiparametric regression models. The model accommodates time-dependent covariate effects on the subdistribution hazard. To motivate the proposed likelihood method, we derive standard nonparametric estimators and discuss a new interpretation based on pseudo risk sets. We establish consistency and asymptotic normality of the estimators and propose a sandwich estimator of the variance. In comprehensive simulation studies, we demonstrate the solid performance of the weighted nonparametric maximum likelihood estimation in the presence of independent right censoring. We provide an application to a very large bone marrow transplant dataset, thereby illustrating its practical utility. Supplementary materials for this article are available online.},
  pmid = {31073256},
  keywords = {Cumulative incidence function,Fine-Gray model,Nonparametric maximum likelihood estimation,Semiparametric transformation models,Time-varying covariates},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\9TVVDLLH\\Bellach et al. - 2019 - Weighted NPMLE for the Subdistribution of a Compet.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\KH3AVDGU\\01621459.2017.html}
}

@article{bellHandlingMissingData2014,
  title = {Handling Missing Data in {{RCTs}}; a Review of the Top Medical Journals},
  author = {Bell, Melanie L. and Fiero, Mallorie and Horton, Nicholas J. and Hsu, Chiu-Hsieh},
  year = {2014},
  month = nov,
  journal = {BMC Medical Research Methodology},
  volume = {14},
  number = {1},
  pages = {118},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-118},
  urldate = {2022-10-10},
  abstract = {Missing outcome data is a threat to the validity of treatment effect estimates in randomized controlled trials. We aimed to evaluate the extent, handling, and sensitivity analysis of missing data and intention-to-treat (ITT) analysis of randomized controlled trials (RCTs) in top tier medical journals, and compare our findings with previous reviews related to missing data and ITT in RCTs.},
  keywords = {Intention-to-treat,Missing data,Sensitivity analysis},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\LAWEU9DU\\Bell et al. - 2014 - Handling missing data in RCTs\; a review of the top.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\BW34XNYL\\1471-2288-14-118.html}
}

@article{bellucciImmunologicEffectsProphylactic2002,
  title = {Immunologic Effects of Prophylactic Donor Lymphocyte Infusion after Allogeneic Marrow Transplantation for Multiple Myeloma},
  author = {Bellucci, Roberto and Alyea, Edwin P. and Weller, Edie and Chillemi, Antoinette and Hochberg, Ephraim and Wu, Catherine J. and Canning, Christine and Schlossman, Robert and Soiffer, Robert J. and Anderson, Kenneth C. and Ritz, Jerome},
  year = {2002},
  month = jun,
  journal = {Blood},
  volume = {99},
  number = {12},
  pages = {4610--4617},
  issn = {0006-4971},
  doi = {10.1182/blood.V99.12.4610},
  urldate = {2023-11-04},
  abstract = {Reconstitution of T-cell immunity after bone marrow transplantation (BMT) is often delayed, resulting in a prolonged period of immunodeficiency. Donor lymphocyte infusion (DLI) has been used to enhance graft-versus-leukemia activity after BMT, but the effects of DLI on immune reconstitution have not been established. We studied 9 patients with multiple myeloma who received myeloablative therapy and T-cell--depleted allogeneic BMT followed 6 months later by infusion of lymphocytes from the same donor. DLI consisted of 3\,{\texttimes}\,107 CD4+ donor T cells per kilogram obtained after in vitro depletion of CD8+ cells. Cell surface phenotype of peripheral lymphocytes, T-cell receptor (TCR) V{$\beta$} repertoire, TCR rearrangement excision circles (TRECs), and hematopoietic chimerism were studied in the first 6 months after BMT and for 1 year after DLI. These studies were also performed in 7 patients who received similar myeloablative therapy and BMT but without DLI. Phenotypic reconstitution of T and natural killer cells was similar in both groups, but patients who received CD4+ DLI developed increased numbers of CD20+ B cells. TCR V{$\beta$} repertoire complexity was decreased at 3 and 6 months after BMT but improved more rapidly in patients who received DLI (P\,=\,.01). CD4+ DLI was also associated with increased numbers of TRECs in CD3+ T cells (P\,\&lt;\,.001) and with conversion to complete donor hematopoiesis (P\,=\,.05). These results provide evidence that prophylactic infusion of CD4+ donor lymphocytes 6 months after BMT enhances reconstitution of donor T cells and conversion to donor hematopoiesis as well as promoting antitumor immunity.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\N8NWGY9B\\Bellucci et al. - 2002 - Immunologic effects of prophylactic donor lymphocy.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\4GKG5KPT\\Immunologic-effects-of-prophylactic-donor.html}
}

@article{bergerSubdistributionHazardModels2020,
  title = {Subdistribution Hazard Models for Competing Risks in Discrete Time},
  author = {Berger, Moritz and Schmid, Matthias and Welchowski, Thomas and {Schmitz-Valckenberg}, Steffen and Beyersmann, Jan},
  year = {2020},
  month = jul,
  journal = {Biostatistics},
  volume = {21},
  number = {3},
  pages = {449--466},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxy069},
  urldate = {2023-10-07},
  abstract = {A popular modeling approach for competing risks analysis in longitudinal studies is the proportional subdistribution hazards model by Fine and Gray (1999. A proportional hazards model for the subdistribution of a competing risk. Journal of the American Statistical Association94, 496--509). This model is widely used for the analysis of continuous event times in clinical and epidemiological studies. However, it does not apply when event times are measured on a discrete time scale, which is a likely scenario when events occur between pairs of consecutive points in time (e.g., between two follow-up visits of an epidemiological study) and when the exact lengths of the continuous time spans are not known. To adapt the Fine and Gray approach to this situation, we propose a technique for modeling subdistribution hazards in discrete time. Our method, which results in consistent and asymptotically normal estimators of the model parameters, is based on a weighted ML estimation scheme for binary regression. We illustrate the modeling approach by an analysis of nosocomial pneumonia in patients treated in hospitals.},
  file = {C:\Users\efbonneville\Zotero\storage\U5A5BRLC\Berger et al. - 2020 - Subdistribution hazard models for competing risks .pdf}
}

@article{beyersmannCompetingRisksAnalysis2007,
  title = {A Competing Risks Analysis of Bloodstream Infection after Stem-Cell Transplantation Using Subdistribution Hazards and Cause-Specific Hazards},
  author = {Beyersmann, Jan and Dettenkofer, Markus and Bertz, Hartmut and Schumacher, Martin},
  year = {2007},
  journal = {Statistics in Medicine},
  volume = {26},
  number = {30},
  pages = {5360--5369},
  issn = {1097-0258},
  doi = {10.1002/sim.3006},
  urldate = {2021-05-10},
  abstract = {After peripheral blood stem-cell transplantation, patients treated for severe haematologic diseases enter a critical phase (neutropenia). Analysis of bloodstream infection during neutropenia has to account for competing risks. Separate Cox analyses of all cause-specific hazards are the standard technique of choice, but are hard to interpret when the overall effects of covariates on the cumulative incidence function (CIF) are of interest. Proportional subdistribution hazards modelling of the subdistribution of the CIF is establishing itself as an interpretation-friendly alternative. We apply both methods and discuss their relative merits. Copyright {\copyright} 2007 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {baseline hazard,cumulative incidence function,Fine and Gray model,hospital infection},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\IQR2Q9GT\\Beyersmann et al. - 2007 - A competing risks analysis of bloodstream infectio.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\GZ6R9FA4\\sim.html}
}

@book{beyersmannCompetingRisksMultistate2012,
  title = {Competing {{Risks}} and {{Multistate Models}} with {{R}}},
  author = {Beyersmann, Jan and Allignol, Arthur and Schumacher, Martin},
  year = {2012},
  series = {Use {{R}}!},
  publisher = {Springer-Verlag},
  address = {New York},
  doi = {10.1007/978-1-4614-2035-4},
  urldate = {2021-05-18},
  abstract = {This book covers competing risks and multistate models, sometimes summarized as event history analysis. These models generalize the analysis of time to a single event (survival analysis) to analysing the timing of distinct terminal events (competing risks) and possible intermediate events (multistate models). Both R and multistate methods are promoted with a focus on nonparametric methods.},
  isbn = {978-1-4614-2034-7},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\XGY22P5G\9781461420347.html}
}

@article{beyersmannSimulatingCompetingRisks2009,
  title = {Simulating Competing Risks Data in Survival Analysis},
  author = {Beyersmann, Jan and Latouche, Aur{\'e}lien and Buchholz, Anika and Schumacher, Martin},
  year = {2009},
  journal = {Statistics in Medicine},
  volume = {28},
  number = {6},
  pages = {956--971},
  issn = {1097-0258},
  doi = {10.1002/sim.3516},
  urldate = {2020-03-31},
  abstract = {Competing risks analysis considers time-to-first-event (`survival time') and the event type (`cause'), possibly subject to right-censoring. The cause-, i.e. event-specific hazards, completely determine the competing risk process, but simulation studies often fall back on the much criticized latent failure time model. Cause-specific hazard-driven simulation appears to be the exception; if done, usually only constant hazards are considered, which will be unrealistic in many medical situations. We explain simulating competing risks data based on possibly time-dependent cause-specific hazards. The simulation design is as easy as any other, relies on identifiable quantities only and adds to our understanding of the competing risks process. In addition, it immediately generalizes to more complex multistate models. We apply the proposed simulation design to computing the least false parameter of a misspecified proportional subdistribution hazard model, which is a research question of independent interest in competing risks. The simulation specifications have been motivated by data on infectious complications in stem-cell transplanted patients, where results from cause-specific hazards analyses were difficult to interpret in terms of cumulative event probabilities. The simulation illustrates that results from a misspecified proportional subdistribution hazard analysis can be interpreted as a time-averaged effect on the cumulative event probability scale. Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\EMMZKBFS\\Beyersmann et al. - 2009 - Simulating competing risks data in survival analys.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\YPXF9TVM\\sim.html}
}

@article{blakeEstimatingTreatmentEffects2020,
  title = {Estimating Treatment Effects with Partially Observed Covariates Using Outcome Regression with Missing Indicators},
  author = {Blake, Helen A. and Leyrat, Cl{\'e}mence and Mansfield, Kathryn E. and Tomlinson, Laurie A. and Carpenter, James and Williamson, Elizabeth J.},
  year = {2020},
  journal = {Biometrical Journal},
  volume = {62},
  number = {2},
  pages = {428--443},
  issn = {1521-4036},
  doi = {10.1002/bimj.201900041},
  urldate = {2022-10-17},
  abstract = {Missing data is a common issue in research using observational studies to investigate the effect of treatments on health outcomes. When missingness occurs only in the covariates, a simple approach is to use missing indicators to handle the partially observed covariates. The missing indicator approach has been criticized for giving biased results in outcome regression. However, recent papers have suggested that the missing indicator approach can provide unbiased results in propensity score analysis under certain assumptions. We consider assumptions under which the missing indicator approach can provide valid inferences, namely, (1) no unmeasured confounding within missingness patterns; either (2a) covariate values of patients with missing data were conditionally independent of treatment or (2b) these values were conditionally independent of outcome; and (3) the outcome model is correctly specified: specifically, the true outcome model does not include interactions between missing indicators and fully observed covariates. We prove that, under the assumptions above, the missing indicator approach with outcome regression can provide unbiased estimates of the average treatment effect. We use a simulation study to investigate the extent of bias in estimates of the treatment effect when the assumptions are violated and we illustrate our findings using data from electronic health records. In conclusion, the missing indicator approach can provide valid inferences for outcome regression, but the plausibility of its assumptions must first be considered carefully.},
  langid = {english},
  keywords = {average treatment effect,missing confounder data,missing covariate data,missing indicator,outcome regression},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\PNQ5XMFQ\\Blake et al. - 2020 - Estimating treatment effects with partially observ.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\CUGLKSH2\\bimj.html}
}

@article{bojinovDiagnosingMissingAlways2020,
  title = {Diagnosing Missing Always at Random in Multivariate Data},
  author = {Bojinov, Iavor I and Pillai, Natesh S and Rubin, Donald B},
  year = {2020},
  month = mar,
  journal = {Biometrika},
  volume = {107},
  number = {1},
  pages = {246--253},
  issn = {0006-3444},
  doi = {10.1093/biomet/asz061},
  urldate = {2024-10-21},
  abstract = {Models for analysing multivariate datasets with missing values require strong, often unassessable, assumptions. The most common of these is that the mechanism that created the missing data is ignorable, which is a two-fold assumption dependent on the mode of inference. The first part, which is the focus here, under the Bayesian and direct-likelihood paradigms requires that the missing data be missing at random; in contrast, the frequentist-likelihood paradigm demands that the missing data mechanism always produce missing at random data, a condition known as missing always at random. Under certain regularity conditions, assuming missing always at random leads to a condition that can be tested using the observed data alone, namely that the missing data indicators depend only on fully observed variables. In this note we propose three different diagnostic tests that not only indicate when this assumption is incorrect but also suggest which variables are the most likely culprits. Although missing always at random is not a necessary condition to ensure validity under the Bayesian and direct-likelihood paradigms, it is sufficient, and evidence of its violation should encourage the careful statistician to conduct targeted sensitivity analyses.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\YXHST8IB\\Bojinov et al. - 2020 - Diagnosing missing always at random in multivariat.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\B9BITRVG\\5638938.html}
}

@article{bonnevilleHandlingMissingCovariate2023,
  title = {Handling Missing Covariate Data in Clinical Studies in Haematology},
  author = {Bonneville, Edouard F. and Schetelig, Johannes and Putter, Hein and {de Wreede}, Liesbeth C.},
  year = {2023},
  month = jun,
  journal = {Best Practice \& Research Clinical Haematology},
  volume = {36},
  number = {2},
  pages = {101477},
  issn = {1521-6926},
  doi = {10.1016/j.beha.2023.101477},
  urldate = {2023-06-13},
  abstract = {Missing data are frequently encountered across studies in clinical haematology. Failure to handle these missing values in an appropriate manner can complicate the interpretation of a study's findings, as estimates presented may be biased and/or imprecise. In the present work, we first provide an overview of current methods for handling missing covariate data, along with their advantages and disadvantages. Furthermore, a systematic review is presented, exploring both contemporary reporting of missing values in major haematological journals, and the methods used for handling them. A principal finding was that the method of handling missing data was explicitly specified in a minority of articles (in 76 out of 195 articles reporting missing values, 39\%). Among these, complete case analysis and the missing indicator method were the most common approaches to dealing with missing values, with more complex methods such as multiple imputation being extremely rare (in 7 out of 195 articles). An example analysis (with associated code) is also provided using hematopoietic stem cell transplantation data, illustrating the different approaches to handling missing values. We conclude with various recommendations regarding the reporting and handling of missing values for future studies in clinical haematology.},
  langid = {english},
  keywords = {Complete case analysis,Missing covariates,Missing data,Missing indicator method,Multiple imputation},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\JUPPYYA7\\Bonneville et al. - 2023 - Handling missing covariate data in clinical studie.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\2D6UDLPH\\S1521692623000385.html}
}

@article{bonnevilleMultipleImputationCausespecific2022,
  title = {Multiple Imputation for Cause-Specific {{Cox}} Models: {{Assessing}} Methods for Estimation and Prediction},
  shorttitle = {Multiple Imputation for Cause-Specific {{Cox}} Models},
  author = {Bonneville, Edouard F and {Resche-Rigon}, Matthieu and Schetelig, Johannes and Putter, Hein and {de Wreede}, Liesbeth C},
  year = {2022},
  month = oct,
  journal = {Statistical Methods in Medical Research},
  volume = {31},
  number = {10},
  pages = {1860--1880},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/09622802221102623},
  urldate = {2022-10-12},
  abstract = {In studies analyzing competing time-to-event outcomes, interest often lies in both estimating the effects of baseline covariates on the cause-specific hazards and predicting cumulative incidence functions. When missing values occur in these baseline covariates, they may be discarded as part of a complete-case analysis or multiply imputed. In the latter case, the imputations may be performed either compatibly with a substantive model pre-specified as a cause-specific Cox model [substantive model compatible fully conditional specification (SMC-FCS)], or approximately so [multivariate imputation by chained equations (MICE)]. In a large simulation study, we assessed the performance of these three different methods in terms of estimating cause-specific regression coefficients and predicting cumulative incidence functions. Concerning regression coefficients, results provide further support for use of SMC-FCS over MICE, particularly when covariate effects are large and the baseline hazards of the competing events are substantially different. Complete-case analysis also shows adequate performance in settings where missingness is not outcome dependent. With regard to cumulative incidence prediction, SMC-FCS and MICE are performed more similarly, as also evidenced in the illustrative analysis of competing outcomes following a hematopoietic stem cell transplantation. The findings are discussed alongside recommendations for practising statisticians.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\XRDJIELX\Bonneville et al. - 2022 - Multiple imputation for cause-specific Cox models.pdf}
}

@misc{bonnevilleMultipleImputationMissing2024,
  title = {Multiple Imputation of Missing Covariates When Using the {{Fine-Gray}} Model},
  author = {Bonneville, Edouard F and Beyersmann, Jan and Keogh, Ruth H. and Bartlett, Jonathan W. and Morris, Tim P. and Polverelli, Nicola and {de Wreede}, Liesbeth C. and Putter, Hein},
  year = {2024},
  number = {arXiv:2405.16602},
  eprint = {2405.16602},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.16602},
  urldate = {2024-08-28},
  abstract = {The Fine-Gray model for the subdistribution hazard is commonly used for estimating associations between covariates and competing risks outcomes. When there are missing values in the covariates included in a given model, researchers may wish to multiply impute them. Assuming interest lies in estimating the risk of only one of the competing events, this paper develops a substantive-model-compatible multiple imputation approach that exploits the parallels between the Fine-Gray model and the standard (single-event) Cox model. In the presence of right-censoring, this involves first imputing the potential censoring times for those failing from competing events, and thereafter imputing the missing covariates by leveraging methodology previously developed for the Cox model in the setting without competing risks. In a simulation study, we compared the proposed approach to alternative methods, such as imputing compatibly with cause-specific Cox models. The proposed method performed well (in terms of estimation of both subdistribution log hazard ratios and cumulative incidences) when data were generated assuming proportional subdistribution hazards, and performed satisfactorily when this assumption was not satisfied. The gain in efficiency compared to a complete-case analysis was demonstrated in both the simulation study and in an applied data example on competing outcomes following an allogeneic stem cell transplantation. For individual-specific cumulative incidence estimation, assuming proportionality on the correct scale at the analysis phase appears to be more important than correctly specifying the imputation procedure used to impute the missing covariates.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\YLZX6MJV\\Bonneville et al. - 2024 - Multiple imputation of missing covariates when usi.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\5VP6488K\\2405.html}
}

@article{bonnevilleWhyYouShould2024,
  title = {Why You Should Avoid Using Multiple {{Fine}}--{{Gray}} Models: Insights from (Attempts at) Simulating Proportional Subdistribution Hazards Data},
  author = {Bonneville, Edouard F and {de Wreede}, Liesbeth C. and Putter, Hein},
  year = {2024 (in press)},
  journal = {Journal of the Royal Statistical Society Series A: Statistics in Society}
}

@article{bonnevilleWhyYouShould2024a,
  title = {Why You Should Avoid Using Multiple {{Fine}}--{{Gray}} Models: Insights from (Attempts at) Simulating Proportional Subdistribution Hazards Data},
  shorttitle = {Why You Should Avoid Using Multiple {{Fine}}--{{Gray}} Models},
  author = {Bonneville, Edouard F and {de Wreede}, Liesbeth C and Putter, Hein},
  year = {2024},
  month = jun,
  journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
  pages = {qnae056},
  issn = {0964-1998},
  doi = {10.1093/jrsssa/qnae056},
  urldate = {2024-08-28},
  abstract = {Studies considering competing risks will often aim to estimate the cumulative incidence functions conditional on an individual's baseline characteristics. While the Fine--Gray subdistribution hazard model is tailor-made for analysing only one of the competing events, it may still be used in settings where multiple competing events are of scientific interest, where it is specified for each cause in turn. In this work, we provide an overview of data-generating mechanisms where proportional subdistribution hazards hold for at least one cause. We use these to motivate why the use of multiple Fine--Gray models should be avoided in favour of better alternatives such as cause-specific hazard models.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\X7H3GY69\\Bonneville et al. - 2024 - Why you should avoid using multiple Fine–Gray mode.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\DV795E2S\\7700140.html}
}

@article{borganNestedCaseControl2015,
  title = {Nested Case--Control Studies: Should One Break the Matching?},
  shorttitle = {Nested Case--Control Studies},
  author = {Borgan, {\O}rnulf and Keogh, Ruth},
  year = {2015},
  month = oct,
  journal = {Lifetime Data Analysis},
  volume = {21},
  number = {4},
  pages = {517--541},
  issn = {1572-9249},
  doi = {10.1007/s10985-015-9319-y},
  urldate = {2024-12-10},
  abstract = {In a nested case--control study, controls are selected for each case from the individuals who are at risk at the time at which the case occurs. We say that the controls are matched on study time. To adjust for possible confounding, it is common to match on other variables as well. The standard analysis of nested case--control data is based on a partial likelihood which compares the covariates of each case to those of its matched controls. It has been suggested that one may break the matching of nested case--control data and analyse them as case--cohort data using an inverse probability weighted (IPW) pseudo likelihood. Further, when some covariates are available for all individuals in the cohort, multiple imputation (MI) makes it possible to use all available data in the cohort. In the paper we review the standard method and the IPW and MI approaches, and compare their performance using simulations that cover a range of scenarios, including one and two endpoints.},
  langid = {english},
  keywords = {Case-cohort,Competing risks,Cox regression,Inverse probability weighting,Matching,Multiple imputation,Nested case-control},
  file = {C:\Users\efbonneville\Zotero\storage\XZKBGECL\Borgan and Keogh - 2015 - Nested case–control studies should one break the .pdf}
}

@article{boschImmuneReconstitutionAntithymocyte2012a,
  title = {Immune Reconstitution after Anti-Thymocyte Globulin-Conditioned Hematopoietic Cell Transplantation},
  author = {Bosch, Mark and Dhadda, Manveer and {Hoegh-Petersen}, Mette and Liu, Yiping and Hagel, Laura M. and Podgorny, Peter and {Ugarte-Torres}, Alejandra and Khan, Faisal M. and Luider, Joanne and {Auer-Grzesiak}, Iwona and Mansoor, Adnan and Russell, James A. and Daly, Andrew and Stewart, Douglas A. and Maloney, David and Boeckh, Michael and Storek, Jan},
  year = {2012},
  month = sep,
  journal = {Cytotherapy},
  volume = {14},
  number = {10},
  pages = {1258--1275},
  issn = {1465-3249},
  doi = {10.3109/14653249.2012.715243},
  urldate = {2023-11-04},
  abstract = {Background aims Anti-thymocyte globulin (ATG) is being used increasingly to prevent graft-versus-host disease (GvHD); however, its impact on immune reconstitution is relatively unknown. We (i) studied immune reconstitution after ATG-conditioned hematopoietic cell transplantation (HCT), (ii) determined the factors influencing the reconstitution, and (iii) compared it with non-ATG-conditioned HCT. Methods Immune cell subset counts were determined at 1--24 months post-transplant in 125 HCT recipients who received ATG during conditioning. Subset counts were also determined in 46 non-ATG-conditioned patients (similarly treated). Results (i) Reconstitution after ATG-conditioned HCT was fast for innate immune cells, intermediate for B cells and CD8 T cells, and very slow for CD4 T cells and invariant natural killer T (iNKT) (iNKT) cells. (ii) Faster reconstitution after ATG-conditioned HCT was associated with a higher number of cells of the same subset transferred with the graft in the case of memory B cells, naive CD4 T cells, naive CD8 T cells, iNKT cells and myeloid dendritic cells; lower recipient age in the case of naive CD4 T cells and naive CD8 T cells; cytomegalovirus recipient seropositivity in the case of memory/effector T cells; an absence of GvHD in the case of naive B cells; lower ATG serum levels in the case of most T-cell subsets, including iNKT cells; and higher ATG levels in the case of NK cells and B cells. (iii) Compared with non-ATG-conditioned HCT, reconstitution after ATG-conditioned HCT was slower for CD4 T cells, and faster for NK cells and B cells. Conclusions ATG worsens the reconstitution of CD4 T cells but improves the reconstitution of NK and B cells.},
  keywords = {anti-thymocyte,globulin,hematopoietic stem cell transplantation,immune reconstitution,immune system,immunity,lymphocytes},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\H9T98RZA\\Bosch et al. - 2012 - Immune reconstitution after anti-thymocyte globuli.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\Y8D3Z7BD\\Bosch et al. - 2012 - Immune reconstitution after anti-thymocyte globuli.pdf}
}

@article{boulesteixPleaNeutralComparison2013,
  title = {A {{Plea}} for {{Neutral Comparison Studies}} in {{Computational Sciences}}},
  author = {Boulesteix, Anne-Laure and Lauer, Sabine and Eugster, Manuel J. A.},
  year = {2013},
  month = apr,
  journal = {PLOS ONE},
  volume = {8},
  number = {4},
  pages = {e61562},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0061562},
  urldate = {2024-11-20},
  abstract = {In computational science literature including, e.g., bioinformatics, computational statistics or machine learning, most published articles are devoted to the development of ``new methods'', while comparison studies are generally appreciated by readers but surprisingly given poor consideration by many journals. This paper stresses the importance of neutral comparison studies for the objective evaluation of existing methods and the establishment of standards by drawing parallels with clinical research. The goal of the paper is twofold. Firstly, we present a survey of recent computational papers on supervised classification published in seven high-ranking computational science journals. The aim is to provide an up-to-date picture of current scientific practice with respect to the comparison of methods in both articles presenting new methods and articles focusing on the comparison study itself. Secondly, based on the results of our survey we critically discuss the necessity, impact and limitations of neutral comparison studies in computational sciences. We define three reasonable criteria a comparison study has to fulfill in order to be considered as neutral, and explicate general considerations on the individual components of a ``tidy neutral comparison study''. R codes for completely replicating our statistical analyses and figures are available from the companion website http://www.ibe.med.uni-muenchen.de/organisation/mitarbeiter/020\_professuren/boulesteix/plea2013.},
  langid = {english},
  keywords = {Algorithms,Bioinformatics,Computer and information sciences,Drug research and development,Machine learning,Machine learning algorithms,Metaanalysis,Scientific publishing},
  file = {C:\Users\efbonneville\Zotero\storage\GLGNYG8E\Boulesteix et al. - 2013 - A Plea for Neutral Comparison Studies in Computati.pdf}
}

@article{boulesteixReplicationCrisisMethodological2020a,
  title = {A {{Replication Crisis}} in {{Methodological Research}}?},
  author = {Boulesteix, Anne-Laure and Hoffmann, Sabine and Charlton, Alethea and Seibold, Heidi},
  year = {2020},
  month = oct,
  journal = {Significance},
  volume = {17},
  number = {5},
  pages = {18--21},
  issn = {1740-9705},
  doi = {10.1111/1740-9713.01444},
  urldate = {2024-10-22},
  abstract = {Statisticians have been keen to critique statistical aspects of the ``replication crisis'' in other scientific disciplines. But new statistical tools are often published and promoted without any thought to replicability. This needs to change, argue Anne-Laure Boulesteix, Sabine Hoffmann, Alethea Charlton and Heidi Seibold},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\LXF3QHKJ\\Boulesteix et al. - 2020 - A Replication Crisis in Methodological Research.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\TY9VAJG8\\7038554.html}
}

@article{breslow1972,
  title = {Contribution to Discussion of Paper by {{D}}. {{R}}. {{Cox}}},
  author = {Breslow, N. E.},
  year = {1972},
  journal = {Journal of the Royal Statistical Society, Series B},
  volume = {34},
  pages = {216--217}
}

@article{buhlerMultistateModelsFramework2023,
  title = {Multistate Models as a Framework for Estimand Specification in Clinical Trials of Complex Processes},
  author = {B{\"u}hler, Alexandra and Cook, Richard J. and Lawless, Jerald F.},
  year = {2023},
  journal = {Statistics in Medicine},
  volume = {42},
  number = {9},
  pages = {1368--1397},
  issn = {1097-0258},
  doi = {10.1002/sim.9675},
  urldate = {2023-10-06},
  abstract = {Intensity-based multistate models provide a useful framework for characterizing disease processes, the introduction of interventions, loss to followup, and other complications arising in the conduct of randomized trials studying complex life history processes. Within this framework we discuss the issues involved in the specification of estimands and show the limiting values of common estimators of marginal process features based on cumulative incidence function regression models. When intercurrent events arise we stress the need to carefully define the target estimand and the importance of avoiding targets of inference that are not interpretable in the real world. This has implications for analyses, but also the design of clinical trials where protocols may help in the interpretation of estimands based on marginal features.},
  copyright = {{\copyright} 2023 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {estimands,generalized transformation model,intercurrent events,robustness,semi-competing risks},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\7BKR8JJN\\Bühler et al. - 2023 - Multistate models as a framework for estimand spec.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\6AQYDUWT\\sim.html}
}

@article{burtonMissingCovariateData2004,
  title = {Missing Covariate Data within Cancer Prognostic Studies: A Review of Current Reporting and Proposed Guidelines},
  shorttitle = {Missing Covariate Data within Cancer Prognostic Studies},
  author = {Burton, A. and Altman, D. G.},
  year = {2004},
  month = jul,
  journal = {British Journal of Cancer},
  volume = {91},
  number = {1},
  pages = {4--8},
  publisher = {Nature Publishing Group},
  issn = {1532-1827},
  doi = {10.1038/sj.bjc.6601907},
  urldate = {2022-10-16},
  abstract = {Prognostic models play a crucial role in the clinical decision-making process. Unfortunately, missing covariate data impede the construction of valid and reliable models, potentially introducing bias, if handled inappropriately. The extent of missing covariate data within reported cancer prognostic studies, the current handling and the quality of reporting this missing covariate data are unknown. Therefore, a review was conducted of 100 articles reporting multivariate survival analyses to assess potential prognostic factors, published within seven cancer journals in 2002. Missing covariate data is a common occurrence in studies performing multivariate survival analyses, being apparent in 81 of the 100 articles reviewed. The percentage of eligible cases with complete data was obtainable in 39 articles, and was {$<$}90\% in 17 of these articles. The methods used to handle incomplete covariates were obtainable in 32 of the 81 articles with known missing data and the most commonly reported approaches were complete case and available case analysis. This review has highlighted deficiencies in the reporting of missing covariate data. Guidelines for presenting prognostic studies with missing covariate data are proposed, which if followed should clarify and standardise the reporting in future articles.},
  copyright = {2004 The Author(s)},
  langid = {english},
  keywords = {Biomedicine,Cancer Research,Drug Resistance,Epidemiology,general,Molecular Medicine,Oncology},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\ZE7UFMKN\\Burton and Altman - 2004 - Missing covariate data within cancer prognostic st.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\ZRJRLPIG\\6601907.html}
}

@article{buscaInvivoExvivoCell2017,
  title = {In-Vivo or Ex-Vivo {{T}} Cell Depletion or Both to Prevent Graft-versus-Host Disease after Hematopoietic Stem Cell Transplantation},
  author = {Busca, Alessandro and Aversa, Franco},
  year = {2017},
  month = nov,
  journal = {Expert Opinion on Biological Therapy},
  volume = {17},
  number = {11},
  pages = {1401--1415},
  publisher = {Taylor \& Francis},
  issn = {1471-2598},
  doi = {10.1080/14712598.2017.1369949},
  urldate = {2023-11-04},
  abstract = {Introduction: Hematopoietic stem cell transplantation (HSCT) represents a widely accepted therapeutic strategy for the treatment of hematologic disorders which are otherwise considered incurable. Alloreactive T cells infused with the stem cell inoculum may generate graft-versus-host disease (GVHD) representing one the most relevant obstacles to the successful outcome of patients receiving allogeneic HSCT. Areas covered: In this review, the authors provide an overview of the most recent approaches of T-cell depletion (TCD) including ex-vivo {$\alpha\beta$}+ TCD and in-vivo TCD with anti-thymocyte globulin (ATG). Expert opinion: Ex vivo depletion of donor T-cells prevents both acute and chronic GVHD without the need for any additional posttransplant immunological prophylaxis either in haploidentical HSCT and HLA matched transplants. Three prospective trials evaluating the efficacy of ATG in matched unrelated donor transplant recipients demonstrated that ATG reduces the incidence of both acute and chronic GVHD without a significant increase of relapse rate, and similar results have been reported in the setting of blood stem cell grafts from matched sibling donors.},
  pmid = {28846051},
  keywords = {graft-versus-host disease,immune reconstitution,infections,Stem cell transplantation},
  file = {C:\Users\efbonneville\Zotero\storage\CELKYY4R\Busca and Aversa - 2017 - In-vivo or ex-vivo T cell depletion or both to pre.pdf}
}

@book{buurenFlexibleImputationMissing2018,
  title = {Flexible Imputation of Missing Data. {{Second}} Edition},
  author = {{van Buuren}, S.},
  year = {2018},
  publisher = {Chapman \& Hall/CRC Press},
  address = {Boca Raton, FL}
}

@book{carpenter2023MI,
  title = {Multiple Imputation and Its Application.},
  author = {Carpenter, James R. and Bartlett, Jonathan W. and Morris, Tim P. and Wood, Angela M. and Quartagno, {\relax Matteo}. and Kenward, Michael G.},
  year = {2023},
  series = {Statistics in Practice Series},
  edition = {2nd ed.},
  publisher = {John Wiley \& Sons, Incorporated},
  address = {Newark},
  isbn = {978-1-119-75609-5},
  langid = {english},
  keywords = {Medical statistics}
}

@article{carpenterMissingDataStatistical2021,
  title = {Missing Data: {{A}} Statistical Framework for Practice},
  shorttitle = {Missing Data},
  author = {Carpenter, James R. and Smuk, Melanie},
  year = {2021},
  journal = {Biometrical Journal},
  volume = {63},
  number = {5},
  pages = {915--947},
  issn = {1521-4036},
  doi = {10.1002/bimj.202000196},
  urldate = {2023-11-07},
  abstract = {Missing data are ubiquitous in medical research, yet there is still uncertainty over when restricting to the complete records is likely to be acceptable, when more complex methods (e.g. maximum likelihood, multiple imputation and Bayesian methods) should be used, how they relate to each other and the role of sensitivity analysis. This article seeks to address both applied practitioners and researchers interested in a more formal explanation of some of the results. For practitioners, the framework, illustrative examples and code should equip them with a practical approach to address the issues raised by missing data (particularly using multiple imputation), alongside an overview of how the various approaches in the literature relate. In particular, we describe how multiple imputation can be readily used for sensitivity analyses, which are still infrequently performed. For those interested in more formal derivations, we give outline arguments for key results, use simple examples to show how methods relate, and references for full details. The ideas are illustrated with a cohort study, a multi-centre case control study and a randomised clinical trial.},
  copyright = {{\copyright} 2021 The Authors. Biometrical Journal published by Wiley-VCH GmbH},
  langid = {english},
  keywords = {complete records,missing data,multiple imputation,sensitivity analysis},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\KAXYX7GQ\\Carpenter and Smuk - 2021 - Missing data A statistical framework for practice.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\2G9A79ES\\bimj.html}
}

@article{carrollHowAreMissing2020,
  title = {How Are Missing Data in Covariates Handled in Observational Time-to-Event Studies in Oncology? {{A}} Systematic Review},
  shorttitle = {How Are Missing Data in Covariates Handled in Observational Time-to-Event Studies in Oncology?},
  author = {Carroll, Orlagh U. and Morris, Tim P. and Keogh, Ruth H.},
  year = {2020},
  month = may,
  journal = {BMC Medical Research Methodology},
  volume = {20},
  number = {1},
  pages = {134},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-01018-7},
  urldate = {2020-10-30},
  abstract = {Missing data in covariates can result in biased estimates and loss of power to detect associations. It can also lead to other challenges in time-to-event analyses including the handling of time-varying effects of covariates, selection of covariates and their flexible modelling. This review aims to describe how researchers approach time-to-event analyses with missing data.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\CZ357ZCG\\Carroll et al. - 2020 - How are missing data in covariates handled in obse.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\UVZ3PZ2Q\\s12874-020-01018-7.html}
}

@article{cederkvistModelingCumulativeIncidence2019,
  title = {Modeling the Cumulative Incidence Function of Multivariate Competing Risks Data Allowing for Within-Cluster Dependence of Risk and Timing},
  author = {Cederkvist, Luise and Holst, Klaus K and Andersen, Klaus K and Scheike, Thomas H},
  year = {2019},
  month = apr,
  journal = {Biostatistics},
  volume = {20},
  number = {2},
  pages = {199--217},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxx072},
  urldate = {2023-10-08},
  abstract = {We propose to model the cause-specific cumulative incidence function of multivariate competing risks data using a random effects model that allows for within-cluster dependence of both risk and timing. The model contains parameters that makes it possible to assess how the two are connected, e.g. if high-risk is related to early onset. Under the proposed model, the cumulative incidences of all failure causes are modeled and all cause-specific and cross-cause associations specified. Consequently, left-truncation and right-censoring are easily dealt with. The proposed model is assessed using simulation studies and applied in analysis of Danish register-based family data on breast cancer.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\GG8YI7RP\\Cederkvist et al. - 2019 - Modeling the cumulative incidence function of mult.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\BI3795ST\\4788726.html}
}

@inproceedings{chiang1961probability,
  title = {On the Probability of Death from Specific Causes in the Presence of Competing Risks},
  booktitle = {Proceedings of the Fourth {{Berkeley}} Symposium on Mathematical Statistics and Probability},
  author = {Chiang, Chin Long},
  year = {1961},
  volume = {4},
  pages = {169--180},
  publisher = {University of California Press, Berkeley, CA},
  file = {C:\Users\efbonneville\Zotero\storage\XI5CNZXX\Chiang - 1961 - On the probability of death from specific causes i.pdf}
}

@article{chiangStochasticStudyLife1961,
  title = {A {{Stochastic Study}} of the {{Life Table}} and {{Its Applications}}. {{III}}. {{The Follow-Up Study}} with the {{Consideration}} of {{Competing Risks}}},
  author = {Chiang, Chin Long},
  year = {1961},
  journal = {Biometrics},
  volume = {17},
  number = {1},
  eprint = {2527496},
  eprinttype = {jstor},
  pages = {57--78},
  publisher = {[Wiley, International Biometric Society]},
  issn = {0006-341X},
  doi = {10.2307/2527496},
  urldate = {2024-06-19},
  file = {C:\Users\efbonneville\Zotero\storage\JTKH6PQE\Chiang - 1961 - A Stochastic Study of the Life Table and Its Appli.pdf}
}

@article{chiuEffectPrenatalTreatments2020,
  title = {The {{Effect}} of {{Prenatal Treatments}} on {{Offspring Events}} in the {{Presence}} of {{Competing Events}}: {{An Application}} to a {{Randomized Trial}} of {{Fertility Therapies}}},
  shorttitle = {The {{Effect}} of {{Prenatal Treatments}} on {{Offspring Events}} in the {{Presence}} of {{Competing Events}}},
  author = {Chiu, Yu-Han and Stensrud, Mats J. and Dahabreh, Issa J. and Rinaudo, Paolo and Diamond, Michael P. and Hsu, John and {Hern{\'a}ndez-D{\'i}az}, Sonia and Hern{\'a}n, Miguel A.},
  year = {2020},
  month = sep,
  journal = {Epidemiology},
  volume = {31},
  number = {5},
  pages = {636},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000001222},
  urldate = {2024-06-18},
  abstract = {When studying the effect of a prenatal treatment on events in the offspring, failure to produce a live birth is a competing event for events in the offspring. A common approach to handle this competing event is reporting both the treatment-specific probabilities of live births and of the event of interest among live births. However, when the treatment affects the competing event, the latter probability cannot be interpreted as the causal effect among live births. Here we provide guidance for researchers interested in the effects of prenatal treatments on events in the offspring in the presence of the competing event ``no live birth.'' We review the total effect of treatment on a composite event and the total effect of treatment on the event of interest. These causal effects are helpful for decision making but are agnostic about the pathways through which treatment affects the event of interest. Therefore, based on recent work, we also review three causal effects that explicitly consider the pathways through which treatment may affect the event of interest in the presence of competing events: the direct effect of treatment on the event of interest under an intervention to eliminate the competing event, the separable direct and indirect effects of treatment on the event of interest, and the effect of treatment in the principal stratum of those who would have had a live birth irrespective of treatment choice. As an illustrative example, we use a randomized trial of fertility treatments and risk of neonatal complications.},
  langid = {american},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\R97U66SQ\\Chiu et al. - 2020 - The Effect of Prenatal Treatments on Offspring Eve.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\NCZ8ZDWE\\the_effect_of_prenatal_treatments_on_offspring.5.html}
}

@article{choiSemiparametricAcceleratedFailure2018,
  title = {Semiparametric Accelerated Failure Time Cure Rate Mixture Models with Competing Risks},
  author = {Choi, Sangbum and Zhu, Liang and Huang, Xuelin},
  year = {2018},
  journal = {Statistics in Medicine},
  volume = {37},
  number = {1},
  pages = {48--59},
  issn = {1097-0258},
  doi = {10.1002/sim.7508},
  urldate = {2021-09-28},
  abstract = {Modern medical treatments have substantially improved survival rates for many chronic diseases and have generated considerable interest in developing cure fraction models for survival data with a non-ignorable cured proportion. Statistical analysis of such data may be further complicated by competing risks that involve multiple types of endpoints. Regression analysis of competing risks is typically undertaken via a proportional hazards model adapted on cause-specific hazard or subdistribution hazard. In this article, we propose an alternative approach that treats competing events as distinct outcomes in a mixture. We consider semiparametric accelerated failure time models for the cause-conditional survival function that are combined through a multinomial logistic model within the cure-mixture modeling framework. The cure-mixture approach to competing risks provides a means to determine the overall effect of a treatment and insights into how this treatment modifies the components of the mixture in the presence of a cure fraction. The regression and nonparametric parameters are estimated by a nonparametric kernel-based maximum likelihood estimation method. Variance estimation is achieved through resampling methods for the kernel-smoothed likelihood function. Simulation studies show that the procedures work well in practical settings. Application to a sarcoma study demonstrates the use of the proposed method for competing risk data with a cure fraction.},
  langid = {english},
  keywords = {competing risks,cure fraction,kernel smoothing,mixture model,nonparametric likelihood,subdistribution}
}

@phdthesis{clementsMultipleImputationDerived2022,
  title = {Multiple Imputation of a Derived Variable in a Survival Analysis Context},
  author = {Clements, Lily Zara},
  year = {2022},
  urldate = {2024-10-21},
  abstract = {A data set contains variables that are directly measured, and can be expanded by non-trivial transformations of the measured variable; e.g., dichotomising a continuous variable. Additionally, a new variable can be constructed from several measured variables; e.g., body mass index (BMI) is the ratio of weight and height-squared. The transformed or constructed variable is a derived variable, and the measured variable(s) that build the derived variable are constituents. {$<$}br/{$><$}br/{$>$}A complication in a derived variable arises if at least one value in the constituents is not stored, that is, the derived variable is incomplete. Incomplete variables are a common problem when analysing data and can lead to incorrect inferences in the analysis if mishandled. One approach to deal with them is multiple imputation (MI). In MI, each missing value is replaced several times, yielding several complete multiply imputed data sets. Each data set is analysed, with the results subsequently combined. Two approaches to impute an incomplete derived variable are active and passive imputation. In active imputation, the derived variable is directly imputed, so the functional relationship with the constituents is ignored. In passive imputation, the constituents are imputed and the derived variable is later constructed.{$<$}br/{$>$}Previous literature finds that the performance of active and passive MI can depend on the model fitted to the multiply imputed data. One gap in the literature is in the performance of active and passive MI in a survival analysis context.{$<$}br/{$><$}br/{$>$}In this thesis, a simulation study is run to investigate the performance of active and passive imputation for three functional forms in a survival analysis context: ratio, additive, and index. {$<$}br/{$><$}br/{$>$}In an additive form, the derived variable is a weighted sum of the constituents. In an index form, a numerical variable is categorised as a factor.{$<$}br/{$>$}Conditions investigated include how the missingness is imposed, and the number of predictors to impute the missing values. A special case of passive imputation outperforms active imputation for a ratio and additive functional form. Active imputation outperforms passive imputation for an index functional form.},
  collaborator = {Clements, Lily Zara and Bohning, Dankmar and Kimber, Alan and Biedermann, Stefanie},
  copyright = {uos\_thesis},
  langid = {english},
  school = {University of Southampton},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\LKCMXVYD\\Clements - 2022 - Multiple imputation of a derived variable in a sur.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\AKS2E4I5\\469605.html}
}

@article{cliftLivingRiskPrediction2020,
  title = {Living Risk Prediction Algorithm ({{QCOVID}}) for Risk of Hospital Admission and Mortality from Coronavirus 19 in Adults: National Derivation and Validation Cohort Study},
  shorttitle = {Living Risk Prediction Algorithm ({{QCOVID}}) for Risk of Hospital Admission and Mortality from Coronavirus 19 in Adults},
  author = {Clift, Ash K. and Coupland, Carol A. C. and Keogh, Ruth H. and {Diaz-Ordaz}, Karla and Williamson, Elizabeth and Harrison, Ewen M. and Hayward, Andrew and Hemingway, Harry and Horby, Peter and Mehta, Nisha and Benger, Jonathan and Khunti, Kamlesh and Spiegelhalter, David and Sheikh, Aziz and Valabhji, Jonathan and Lyons, Ronan A. and Robson, John and Semple, Malcolm G. and Kee, Frank and Johnson, Peter and Jebb, Susan and Williams, Tony and {Hippisley-Cox}, Julia},
  year = {2020},
  month = oct,
  journal = {BMJ},
  volume = {371},
  pages = {m3731},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj.m3731},
  urldate = {2023-11-06},
  abstract = {Objective To derive and validate a risk prediction algorithm to estimate hospital admission and mortality outcomes from coronavirus disease 2019 (covid-19) in adults. Design Population based cohort study. Setting and participants QResearch database, comprising 1205 general practices in England with linkage to covid-19 test results, Hospital Episode Statistics, and death registry data. 6.08 million adults aged 19-100 years were included in the derivation dataset and 2.17 million in the validation dataset. The derivation and first validation cohort period was 24 January 2020 to 30 April 2020. The second temporal validation cohort covered the period 1 May 2020 to 30 June 2020. Main outcome measures The primary outcome was time to death from covid-19, defined as death due to confirmed or suspected covid-19 as per the death certification or death occurring in a person with confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection in the period 24 January to 30 April 2020. The secondary outcome was time to hospital admission with confirmed SARS-CoV-2 infection. Models were fitted in the derivation cohort to derive risk equations using a range of predictor variables. Performance, including measures of discrimination and calibration, was evaluated in each validation time period. Results 4384 deaths from covid-19 occurred in the derivation cohort during follow-up and 1722 in the first validation cohort period and 621 in the second validation cohort period. The final risk algorithms included age, ethnicity, deprivation, body mass index, and a range of comorbidities. The algorithm had good calibration in the first validation cohort. For deaths from covid-19 in men, it explained 73.1\% (95\% confidence interval 71.9\% to 74.3\%) of the variation in time to death (R2); the D statistic was 3.37 (95\% confidence interval 3.27 to 3.47), and Harrell's C was 0.928 (0.919 to 0.938). Similar results were obtained for women, for both outcomes, and in both time periods. In the top 5\% of patients with the highest predicted risks of death, the sensitivity for identifying deaths within 97 days was 75.7\%. People in the top 20\% of predicted risk of death accounted for 94\% of all deaths from covid-19. Conclusion The QCOVID population based risk algorithm performed well, showing very high levels of discrimination for deaths and hospital admissions due to covid-19. The absolute risks presented, however, will change over time in line with the prevailing SARS-C0V-2 infection rate and the extent of social distancing measures in place, so they should be interpreted with caution. The model can be recalibrated for different time periods, however, and has the potential to be dynamically updated as the pandemic evolves.},
  chapter = {Research},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2019. Re-use permitted under CC                 BY. No commercial re-use. See rights and permissions. Published by                 BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  pmid = {33082154},
  file = {C:\Users\efbonneville\Zotero\storage\4H7I8JKJ\Clift et al. - 2020 - Living risk prediction algorithm (QCOVID) for risk.pdf}
}

@article{cornfieldEstimationProbabilityDeveloping1957,
  title = {Estimation of the {{Probability}} of {{Developing}} a {{Disease}} in the {{Presence}} of {{Competing Risks}}},
  author = {Cornfield, Jerome},
  year = {1957},
  month = may,
  journal = {American Journal of Public Health and the Nations Health},
  volume = {47},
  number = {5},
  pages = {601--607},
  issn = {0002-9572},
  urldate = {2024-06-19},
  pmcid = {PMC1551242},
  pmid = {13424810},
  file = {C:\Users\efbonneville\Zotero\storage\PDPUZY7B\Cornfield - 1957 - Estimation of the Probability of Developing a Dise.pdf}
}

@article{coxPartialLikelihood1975,
  title = {Partial Likelihood},
  author = {Cox, D. R.},
  year = {1975},
  month = aug,
  journal = {Biometrika},
  volume = {62},
  number = {2},
  pages = {269--276},
  publisher = {Oxford Academic},
  issn = {0006-3444},
  doi = {10.1093/biomet/62.2.269},
  urldate = {2020-11-09},
  abstract = {Abstract.  A definition is given of partial likelihood generalizing the ideas of conditional and marginal likelihood. Applications include life tables and infer},
  langid = {english}
}

@article{coxRegressionModelsLifeTables1972,
  title = {Regression {{Models}} and {{Life-Tables}}},
  author = {Cox, D. R.},
  year = {1972},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {34},
  number = {2},
  pages = {187--202},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1972.tb00899.x},
  urldate = {2024-08-09},
  abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
  copyright = {{\copyright} 1972 The Authors},
  langid = {english},
  keywords = {accelerated life tests,age-specific failure rate,asymptotic theory,censored data,conditional inference,hazard function,life table,medical applications,product limit estimate,regression,reliability theory,two-sample rank tests},
  file = {C:\Users\efbonneville\Zotero\storage\ZFEG9YID\j.2517-6161.1972.tb00899.html}
}

@article{damicoClinicalStatesCirrhosis2018,
  title = {Clinical States of Cirrhosis and Competing Risks},
  author = {D'Amico, Gennaro and Morabito, Alberto and D'Amico, Mario and Pasta, Linda and Malizia, Giuseppe and Rebora, Paola and Valsecchi, Maria Grazia},
  year = {2018},
  month = mar,
  journal = {Journal of Hepatology},
  volume = {68},
  number = {3},
  pages = {563--576},
  issn = {0168-8278},
  doi = {10.1016/j.jhep.2017.10.020},
  urldate = {2024-03-28},
  abstract = {The clinical course of cirrhosis is mostly determined by the progressive increase of portal hypertension, hyperdynamic circulation, bacterial translocation and activation of systemic inflammation. Different disease states, encompassing compensated and decompensated cirrhosis and a late decompensated state, are related to the progression of these mechanisms and may be recognised by haemodynamic or clinical characteristics. While these disease states do not follow a predictable sequence, they correspond to varying mortality risk. Acute-on-chronic liver failure may occur either in decompensated or in compensated cirrhosis and is always associated with a high short-term mortality. The increasing severity of these disease states prompted the concept of clinical states of cirrhosis. A multistate approach has been considered to describe the clinical course of the disease. Such an approach requires the assessment of the probabilities of different outcomes in each state, which compete with each other to occur first and mark the transition towards a different state. This requires the use of competing risks analysis, since the traditional Kaplan-Meier analysis should only be used in two-state settings. Accounting for competing risks also has implications for prognosis and treatment efficacy research. The aim of this review is to summarise relevant clinical states and to show examples of competing risks analysis in multistate models of cirrhosis.},
  keywords = {Cirrhosis,Clinical course of cirrhosis,Clinical states of cirrhosis,Competing risks,Cumulative incidence function,Multistate models for cirrhosis,Portal hypertension},
  file = {C:\Users\efbonneville\Zotero\storage\CH22NBB8\S0168827817323991.html}
}

@article{dekkerReconstitutionCellSubsets2020a,
  title = {Reconstitution of {{T Cell Subsets Following Allogeneic Hematopoietic Cell Transplantation}}},
  author = {Dekker, Linde and {de Koning}, Coco and Lindemans, Caroline and Nierkens, Stefan},
  year = {2020},
  month = jul,
  journal = {Cancers},
  volume = {12},
  number = {7},
  pages = {1974},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-6694},
  doi = {10.3390/cancers12071974},
  urldate = {2023-11-04},
  abstract = {Allogeneic (allo) hematopoietic cell transplantation (HCT) is the only curative treatment option for patients suffering from chemotherapy-refractory or relapsed hematological malignancies. The occurrence of morbidity and mortality after allo-HCT is still high. This is partly correlated with the immunological recovery of the T cell subsets, of which the dynamics and relations to complications are still poorly understood. Detailed information on T cell subset recovery is crucial to provide tools for better prediction and modulation of adverse events. Here, we review the current knowledge regarding CD4+ and CD8+ T cells, {$\gamma\delta$} T cells, iNKT cells, Treg cells, MAIT cells and naive and memory T cell reconstitution, as well as their relations to outcome, considering different cell sources and immunosuppressive therapies. We conclude that the T cell subsets reconstitute in different ways and are associated with distinct adverse and beneficial events; however, adequate reconstitution of all the subsets is associated with better overall survival. Although the exact mechanisms involved in the reconstitution of each T cell subset and their associations with allo-HCT outcome need to be further elucidated, the data and suggestions presented here point towards the development of individualized approaches to improve their reconstitution. This includes the modulation of immunotherapeutic interventions based on more detailed immune monitoring, aiming to improve overall survival changes.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {allogeneic hematopoietic cell transplantation,biomarkers,conditioning,hematological malignancies,immune reconstitution,immunosuppressive therapies,serotherapy,T cell subsets},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\PD5BDFHH\\Dekker et al. - 2020 - Reconstitution of T Cell Subsets Following Allogen.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\WKCSU77F\\Dekker et al. - 2020 - Reconstitution of T Cell Subsets Following Allogen.pdf}
}

@article{delgadoSurvivalAnalysisHematologic2014,
  title = {Survival Analysis in Hematologic Malignancies: Recommendations for Clinicians},
  shorttitle = {Survival Analysis in Hematologic Malignancies},
  author = {Delgado, Julio and Pereira, Arturo and Villamor, Neus and {L{\'o}pez-Guillermo}, Armando and Rozman, Ciril},
  year = {2014},
  month = sep,
  journal = {Haematologica},
  volume = {99},
  number = {9},
  pages = {1410--1420},
  issn = {1592-8721},
  doi = {10.3324/haematol.2013.100784},
  urldate = {2022-10-10},
  abstract = {The widespread availability of statistical packages has undoubtedly helped hematologists worldwide in the analysis of their data, but has also led to the inappropriate use of statistical methods. In this article, we review some basic concepts of survival analysis and also make recommendations about how and when to perform each particular test using SPSS, Stata and R. In particular, we describe a simple way of defining cut-off points for continuous variables and the appropriate and inappropriate uses of the Kaplan-Meier method and Cox proportional hazard regression models. We also provide practical advice on how to check the proportional hazards assumption and briefly review the role of relative survival and multiple imputation.},
  copyright = {Copyright (c)},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\4SMP38DM\Delgado et al. - 2014 - Survival analysis in hematologic malignancies rec.pdf}
}

@article{delordMultipleImputationCompeting2016,
  title = {Multiple Imputation for Competing Risks Regression with Interval-Censored Data},
  author = {Delord, Marc and G{\'e}nin, Emmanuelle},
  year = {2016},
  month = jul,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {86},
  number = {11},
  pages = {2217--2228},
  publisher = {Taylor \& Francis},
  issn = {0094-9655},
  doi = {10.1080/00949655.2015.1106543},
  urldate = {2021-03-16},
  abstract = {We present here an extension of Pan's multiple imputation approach to Cox regression in the setting of interval-censored competing risks data. The idea is to convert interval-censored data into multiple sets of complete or right-censored data and to use partial likelihood methods to analyse them. The process is iterated, and at each step, the coefficient of interest, its variance--covariance matrix, and the baseline cumulative incidence function are updated from multiple posterior estimates derived from the Fine and Gray sub-distribution hazards regression given augmented data. Through simulation of patients at risks of failure from two causes, and following a prescheduled programme allowing for informative interval-censoring mechanisms, we show that the proposed method results in more accurate coefficient estimates as compared to the simple imputation approach. We have implemented the method in the MIICD R package, available on the CRAN website.},
  keywords = {62N01,62N02,62N03,baseline cumulative incidence function,Competing risks,informative interval censoring,interval-censored data,multiple imputation,proportional sub-distribution hazards regression}
}

@article{demirtasComputingPointbiserialCorrelation2016,
  title = {Computing the {{Point-biserial Correlation}} under {{Any Underlying Continuous Distribution}}},
  author = {Demirtas, Hakan and Hedeker, Donald},
  year = {2016},
  month = sep,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {45},
  number = {8},
  pages = {2744--2751},
  publisher = {Taylor \& Francis},
  issn = {0361-0918},
  doi = {10.1080/03610918.2014.920883},
  urldate = {2020-12-06},
  abstract = {The connection between the point-biserial and biserial correlations is well-established when the underlying distribution is bivariate normal. For many other bivariate distributions, the formula that links these two quantities is not straightforward to derive or does not have a closed form. We propose a simple technique that enables researchers to compute one of these correlations when the other is specified. For this, we take advantage of the constancy of their ratio, which can be easily approximated for any distribution. We illustrate the proposed method using several examples and discuss its extension to the ordinal case. We believe that this approach is potentially useful in stochastic simulation..},
  keywords = {Dichotomization,Primary 62F40,Secondary 62P10,Simulation,Sorting}
}

@article{dewreedeMstatePackageAnalysis2011,
  title = {Mstate: {{An R Package}} for the {{Analysis}} of {{Competing Risks}} and {{Multi-State Models}}},
  shorttitle = {Mstate},
  author = {{de Wreede}, Liesbeth C. and Fiocco, Marta and Putter, Hein},
  year = {2011},
  month = jan,
  journal = {Journal of Statistical Software},
  volume = {38},
  number = {1},
  pages = {1--30},
  issn = {1548-7660},
  doi = {10.18637/jss.v038.i07},
  urldate = {2020-10-23},
  copyright = {Copyright (c) 2010 Liesbeth C. de Wreede, Marta Fiocco, Hein Putter},
  langid = {english}
}

@article{dewreedeMstatePackageEstimation2010,
  title = {The Mstate Package for Estimation and Prediction in Non- and Semi-Parametric Multi-State and Competing Risks Models},
  author = {{de Wreede}, Liesbeth C. and Fiocco, Marta and Putter, Hein},
  year = {2010},
  month = sep,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {99},
  number = {3},
  pages = {261--274},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2010.01.001},
  urldate = {2020-04-01},
  abstract = {In recent years, multi-state models have been studied widely in survival analysis. Despite their clear advantages, their use in biomedical and other applications has been rather limited so far. An important reason for this is the lack of flexible and user-friendly software for multi-state models. This paper introduces a package in R, called `mstate', for each of the steps of the analysis of multi-state models. It can be applied to non- and semi-parametric models. The package contains functions to facilitate data preparation and flexible estimation of different types of covariate effects in the context of Cox regression models, functions to estimate patient-specific transition intensities, dynamic prediction probabilities and their associated standard errors (both Greenwood and Aalen-type). Competing risks models can also be analyzed by means of mstate, as they are a special type of multi-state models. The package is available from the R homepage http://cran.r-project.org. We give a self-contained account of the underlying mathematical theory, including a new asymptotic result for the cumulative hazard function and new recursive formulas for the calculation of the estimated standard errors of the estimated transition probabilities, and we illustrate the use of the key functions of the mstate package by the analysis of a reversible multi-state model describing survival of liver cirrhosis patients.},
  langid = {english},
  keywords = {Competing risks models,Cox models,Markov models,Multi-state models,Software,Survival analysis},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\BKJDL83D\\de Wreede et al. - 2010 - The mstate package for estimation and prediction i.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\PYK42FD8\\de Wreede et al. - 2010 - The mstate package for estimation and prediction i.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\YEUDRYXJ\\S0169260710000027.html}
}

@article{diaz-ordazAreMissingData2014,
  title = {Are Missing Data Adequately Handled in Cluster Randomised Trials? {{A}} Systematic Review and Guidelines},
  shorttitle = {Are Missing Data Adequately Handled in Cluster Randomised Trials?},
  author = {{D{\'i}az-Ordaz}, Karla and Kenward, Michael G and Cohen, Abie and Coleman, Claire L and Eldridge, Sandra},
  year = {2014},
  month = oct,
  journal = {Clinical Trials},
  volume = {11},
  number = {5},
  pages = {590--600},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/1740774514537136},
  urldate = {2022-10-10},
  abstract = {BackgroundMissing data are a potential source of bias, and their handling in the statistical analysis can have an important impact on both the likelihood and degree of such bias. Inadequate handling of the missing data may also result in invalid variance estimation. The handling of missing values is more complex in cluster randomised trials, but there are no reviews of practice in this field.ObjectivesA systematic review of published trials was conducted to examine how missing data are reported and handled in cluster randomised trials.MethodsWe systematically identified cluster randomised trials, published in English in 2011, using the National Library of Medicine (MEDLINE) via PubMed. Non-randomised and pilot/feasibility trials were excluded, as were reports of secondary analyses, interim analyses, and economic evaluations and those where no data were at the individual level. We extracted information on missing data and the statistical methods used to deal with them from a random sample of the identified studies.ResultsWe included 132 trials. There was evidence of missing data in 95 (72\%). Only 32 trials reported handling missing data, 22 of them using a variety of single imputation techniques, 8 using multiple imputation without accommodating the clustering and 2 stating that their likelihood-based complete case analysis accounted for missing values because the data were assumed Missing-at-Random.LimitationsThe results presented in this study are based on a large random sample of cluster randomised trials published in 2011, identified in electronic searches and therefore possibly missing some trials, most likely of poorer quality. Also, our results are based on information in the main publication for each trial. These reports may omit some important information on the presence of, and reasons for, missing data and on the statistical methods used to handle them. Our extraction methods, based on published reports, could not distinguish between missing data in outcomes and missing data in covariates. This distinction may be important in determining the assumptions about the missing data mechanism necessary for complete case analyses to be valid.ConclusionsMissing data are present in the majority of cluster randomised trials. However, they are poorly reported, and most authors give little consideration to the assumptions under which their analysis will be valid. The majority of the methods currently used are valid under very strong assumptions about the missing data, whose plausibility is rarely discussed in the corresponding reports. This may have important consequences for the validity of inferences in some trials. Methods which result in valid inferences under general Missing-at-Random assumptions are available and should be made more accessible.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\ZJAC73WK\Díaz-Ordaz et al. - 2014 - Are missing data adequately handled in cluster ran.pdf}
}

@article{diggleInformativeDropOutLongitudinal1994,
  title = {Informative {{Drop-Out}} in {{Longitudinal Data Analysis}}},
  author = {Diggle, P. and Kenward, M. G.},
  year = {1994},
  journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {43},
  number = {1},
  eprint = {2986113},
  eprinttype = {jstor},
  pages = {49--93},
  publisher = {[Wiley, Royal Statistical Society]},
  issn = {0035-9254},
  doi = {10.2307/2986113},
  urldate = {2024-08-23},
  abstract = {A model is proposed for continuous longitudinal data with non-ignorable or informative drop-out (ID). The model combines a multivariate linear model for the underlying response with a logistic regression model for the drop-out process. The latter incorporates dependence of the probability of drop-out on unobserved, or missing, observations. Parameters in the model are estimated by using maximum likelihood (ML) and inferences drawn through conventional likelihood procedures. In particular, likelihood ratio tests can be used to assess the informativeness of the drop-out process through comparison of the full model with reduced models corresponding to random drop-out (RD) and completely random processes. A simulation study is used to assess the procedure in two settings: the comparison of time trends under a linear regression model with autocorrelated errors and the estimation of period means and treatment differences from a four-period four-treatment crossover trial. It is seen in both settings that, when data are generated under an ID process, the ML estimators from the ID model do not suffer from the bias that is present in the ordinary least squares and RD ML estimators. The approach is then applied to three examples. These derive from a milk protein trial involving three groups of cows, milk yield data from a study of mastitis in dairy cattle and data from a multicentre clinical trial on the study of depression. All three examples provide evidence of an underlying ID process, two with some strength. It is seen that the assumption of an ID rather than an RD process has practical implications for the interpretation of the data.},
  file = {C:\Users\efbonneville\Zotero\storage\3UQSKLZY\Diggle and Kenward - 1994 - Informative Drop-Out in Longitudinal Data Analysis.pdf}
}

@article{dondersReviewGentleIntroduction2006,
  title = {Review: {{A}} Gentle Introduction to Imputation of Missing Values},
  shorttitle = {Review},
  author = {Donders, A. Rogier T. and {van der Heijden}, Geert J. M. G. and Stijnen, Theo and Moons, Karel G. M.},
  year = {2006},
  month = oct,
  journal = {Journal of Clinical Epidemiology},
  volume = {59},
  number = {10},
  pages = {1087--1091},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2006.01.014},
  urldate = {2022-10-17},
  abstract = {In most situations, simple techniques for handling missing data (such as complete case analysis, overall mean imputation, and the missing-indicator method) produce biased results, whereas imputation techniques yield valid results without complicating the analysis once the imputations are carried out. Imputation techniques are based on the idea that any subject in a study sample can be replaced by a new randomly chosen subject from the same source population. Imputation of missing data on a variable is replacing that missing by a value that is drawn from an estimate of the distribution of this variable. In single imputation, only one estimate is used. In multiple imputation, various estimates are used, reflecting the uncertainty in the estimation of this distribution. Under the general conditions of so-called missing at random and missing completely at random, both single and multiple imputations result in unbiased estimates of study associations. But single imputation results in too small estimated standard errors, whereas multiple imputation results in correctly estimated standard errors and confidence intervals. In this article we explain why all this is the case, and use a simple simulation study to demonstrate our explanations. We also explain and illustrate why two frequently used methods to handle missing data, i.e., overall mean imputation and the missing-indicator method, almost always result in biased estimates.},
  langid = {english},
  keywords = {Bias,Indicator method,Missing data,Multiple imputation,Precision,Single imputation},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\GLZ3N6J4\\Donders et al. - 2006 - Review A gentle introduction to imputation of mis.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\HR786MTV\\S0895435606001971.html}
}

@article{donoghoeImportanceCensoringCompeting2017,
  title = {The Importance of Censoring in Competing Risks Analysis of the Subdistribution Hazard},
  author = {Donoghoe, Mark W. and Gebski, Val},
  year = {2017},
  month = apr,
  journal = {BMC Medical Research Methodology},
  volume = {17},
  number = {1},
  pages = {52},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0327-3},
  urldate = {2022-08-08},
  abstract = {The analysis of time-to-event data can be complicated by competing risks, which are events that alter the probability of, or completely preclude the occurrence of an event of interest. This is distinct from censoring, which merely prevents us from observing the time at which the event of interest occurs. However, the censoring distribution plays a vital role in the proportional subdistribution hazards model, a commonly used method for regression analysis of time-to-event data in the presence of competing risks.},
  keywords = {Censoring,Competing risks,Proportional subdistribution hazards model},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\AHSJ9P95\\Donoghoe and Gebski - 2017 - The importance of censoring in competing risks ana.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\Y9B2BKSW\\s12874-017-0327-3.html}
}

@article{duCompatibilityImputationSpecification2022,
  title = {Compatibility in Imputation Specification},
  author = {Du, Han and Alacam, Egamaria and Mena, Stefany and Keller, Brian T.},
  year = {2022},
  month = feb,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01749-5},
  urldate = {2022-08-18},
  abstract = {Missing data such as data missing at random (MAR) are unavoidable in real data and have the potential to undermine the validity of research results. Multiple imputation is one of the most widely used MAR-based methods in education and behavioral science applications. Arbitrarily specifying imputation models can lead to incompatibility and cause biased estimation. Building on the recent developments of model-based imputation and Arnold's compatibility work, this paper systematically summarizes when the traditional fully conditional specification (FCS) is applicable and how to specify a model-based imputation model if needed. We summarize two Compatibility Requirements to help researchers check compatibility more easily and a decision tree to check whether the traditional FCS is applicable in a given scenario. Additionally, we present a clear overview of two types of model-based imputation: the sequential and separate specifications. We illustrate how to specify model-based imputation with examples. Additionally, we provide example code of a free software program, Blimp, for implementing model-based imputation.},
  langid = {english},
  keywords = {Compatibility,Imputation,Missing data},
  file = {C:\Users\efbonneville\Zotero\storage\9XPJQFCV\Du et al. - 2022 - Compatibility in imputation specification.pdf}
}

@article{dunbarRelationshipCirculatingNatural2008,
  title = {The Relationship between Circulating Natural Killer Cells after Reduced Intensity Conditioning Hematopoietic Stem Cell Transplantation and Relapse-Free Survival and Graft-versus-Host Disease},
  author = {Dunbar, Erin M. and Buzzeo, Mathew P. and Levine, Jeff B. and Schold, Jesse D. and {Meier-Kriesche}, Herwig-Ulf and Reddy, Vijay},
  year = {2008},
  month = dec,
  journal = {Haematologica},
  volume = {93},
  number = {12},
  pages = {1852--1858},
  issn = {1592-8721},
  doi = {10.3324/haematol.13033},
  urldate = {2023-11-04},
  abstract = {Background Natural killer cells are known to have anti-tumor activity in haploidentical hematopoietic stem cell transplantation. We hypothesized that reconstituted circulating natural killer cells may be associated with improved relapse-free survival after HLA-matched hematopoietic stem cell transplantation.Design and Methods Serial peripheral blood absolute natural killer cell counts were prospectively measured by flow cytometry of lymphocytes expressing CD56 and CD16 in 167 patients. Cluster analysis was used at engraftment and 60 days post-transplant to distinguish patients with high and low absolute natural killer cell counts. At engraftment 80 patients had high counts ({$>$} 22.2/mm3) and 43 had low counts. At 60 days post-transplant 84 patients had high counts ({$>$} 18.2/mm3) and 38 had low counts. The primary study end-points were death, relapse and acute graft-versus-host disease. The median follow-up was 373 days (range, 67--1767).Results Among patients given reduced intensity conditioning, a low absolute natural killer cell count at 60 days post-transplant was independently associated with relapse [adjusted hazard ratio (AHR) = 28.4, 95\% confidence interval (CI) 4.3--186.4] and death (AHR = 17.5, 95\% CI 4.3--71.3). Furthermore, patients given reduced intensity conditioning who had a high absolute natural killer cell count at 60 days had a significantly better 1-year survival than those with a low count by Kaplan-Meier analysis (83\% vs. 11\%, p},
  copyright = {Copyright (c)},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\GDE4F9UH\Dunbar et al. - 2008 - The relationship between circulating natural kille.pdf}
}

@article{eeftingMultistateAnalysisIllustrates2016,
  title = {Multi-State Analysis Illustrates Treatment Success after Stem Cell Transplantation for Acute Myeloid Leukemia Followed by Donor Lymphocyte Infusion},
  author = {Eefting, Matthias and {de Wreede}, Liesbeth C. and Halkes, Constantijn J.M. and {von dem Borne}, Peter A. and Kersting, Sabina and Marijt, Erik W.A. and Veelken, Hendrik and Putter, Hein and Schetelig, Johannes and Falkenburg, J.H. Frederik},
  year = {2016},
  month = apr,
  journal = {Haematologica},
  volume = {101},
  number = {4},
  pages = {506--514},
  issn = {0390-6078},
  doi = {10.3324/haematol.2015.136846},
  urldate = {2023-11-04},
  abstract = {In the field of hematopoietic stem cell transplantation, the common approach is to focus outcome analyses on time to relapse and death, without assessing the impact of post-transplant interventions. We investigated whether a multi-state model would give insight into the events after transplantation in a cohort of patients who were transplanted using a strategy including scheduled donor lymphocyte infusions. Seventy-eight consecutive patients who underwent myeloablative T-cell depleted allogeneic stem cell transplantation for acute myeloid leukemia or myelodysplastic syndrome were studied. We constructed a multi-state model to analyze the impact of donor lymphocyte infusion and graft-versus-host disease on the probabilities of relapse and non-relapse mortality over time. Based on this model we introduced a new measure for outcome after transplantation which we called `treatment success': being alive without relapse and immunosuppression for graft-versus-host disease. All relevant clinical events were implemented into the multi-state model and were denoted treatment success or failure (either transient or permanent). Both relapse and non-relapse mortality were causes of failure of comparable magnitude. Whereas relapse was the dominant cause of failure from the transplantation state, its rate was reduced after graft-versus-host disease, and especially after donor lymphocyte infusion. The long-term probability of treatment success was approximately 40\%. This probability was increased after donor lymphocyte infusion. Our multi-state model helps to interpret the impact of post-transplantation interventions and clinical events on failure and treatment success, thus extracting more information from observational data.},
  pmcid = {PMC5004390},
  pmid = {26802054},
  file = {C:\Users\efbonneville\Zotero\storage\EX7ZR5NC\Eefting et al. - 2016 - Multi-state analysis illustrates treatment success.pdf}
}

@article{eeftingMyeloablativeCelldepletedAlloSCT2014,
  title = {Myeloablative {{T}} Cell-Depleted {{alloSCT}} with Early Sequential Prophylactic Donor Lymphocyte Infusion Is an Efficient and Safe Post-Remission Treatment for Adult {{ALL}}},
  author = {Eefting, M. and Halkes, C. J. M. and {de Wreede}, L. C. and {van Pelt}, C. M. and Kersting, S. and Marijt, E. W. A. and {von dem Borne}, P. A. and Willemze, R. and Veelken, H. and Falkenburg, J. H. F.},
  year = {2014},
  month = feb,
  journal = {Bone Marrow Transplantation},
  volume = {49},
  number = {2},
  pages = {287--291},
  publisher = {Nature Publishing Group},
  issn = {1476-5365},
  doi = {10.1038/bmt.2013.111},
  urldate = {2023-11-04},
  abstract = {The prognosis of adult patients with ALL remains unsatisfactory. AlloSCT is associated with a beneficial GVL response mediated by donor T cells. However, GVHD results in substantial mortality and long-term morbidity. T-cell depletion (TCD) of the graft reduces the severity of GVHD, but is associated with an increased relapse rate after alloSCT. Therefore, early sequential donor lymphocyte infusion (DLI) is likely to be necessary for a successful GVL reaction. Twenty-five adult ALL patients (10 Ph+ALL) were eligible for early DLI after initial disease control with myeloablative TCD-alloSCT in first CR (CR1), if active GVHD was absent at 3--6\,months after alloSCT. Patients with a sibling donor or an unrelated donor were scheduled for 3.0 {\texttimes} 106 CD3+ cells/kg or 1.5 {\texttimes} 106 CD3+ cells/kg, respectively, at 6\,months after alloSCT. Three patients died before evaluation (one early relapse). Five patients had active GVHD. Fourteen of the remaining seventeen patients received DLI (median time-to-DLI: 185\,days). Overall, only 17\% required long-term systemic immunosuppression for GVHD. With a median follow-up after TCD-alloSCT of 50\,months, 2-year survival probability was 68\% (95\% confidence interval (CI) 49--87\%). In conclusion, myeloablative TCD-alloSCT with early sequential DLI is an efficient and safe post-remission treatment for adult ALL patients in CR1.},
  copyright = {2014 Macmillan Publishers Limited},
  langid = {english},
  keywords = {Acute lymphocytic leukaemia,Graft-versus-host disease,T cells,Transplant immunology},
  file = {C:\Users\efbonneville\Zotero\storage\IJETERIY\Eefting et al. - 2014 - Myeloablative T cell-depleted alloSCT with early s.pdf}
}

@article{elbadisyMultimetricComparisonMachine2024,
  title = {Multi-Metric Comparison of Machine Learning Imputation Methods with Application to Breast Cancer Survival},
  author = {El Badisy, Imad and Graffeo, Nathalie and Khalis, Mohamed and Giorgi, Roch},
  year = {2024},
  month = aug,
  journal = {BMC Medical Research Methodology},
  volume = {24},
  number = {1},
  pages = {191},
  issn = {1471-2288},
  doi = {10.1186/s12874-024-02305-3},
  urldate = {2024-12-10},
  abstract = {Handling missing data in clinical prognostic studies is an essential yet challenging task. This study aimed to provide a comprehensive assessment of the effectiveness and reliability of different machine learning (ML) imputation methods across various analytical perspectives. Specifically, it focused on three distinct classes of performance metrics used to evaluate ML imputation methods: post-imputation bias of regression estimates, post-imputation predictive accuracy, and substantive model-free metrics. As an illustration, we applied data from a real-world breast cancer survival study. This comprehensive approach aimed to provide a thorough assessment of the effectiveness and reliability of ML imputation methods across various analytical perspectives. A simulated dataset with 30\% Missing At Random (MAR) values was used. A number of single imputation (SI) methods - specifically KNN, missMDA, CART, missForest, missRanger, missCforest - and multiple imputation (MI) methods - specifically miceCART and miceRF - were evaluated. The performance metrics used were Gower's distance, estimation bias, empirical standard error, coverage rate, length of confidence interval, predictive accuracy, proportion of falsely classified (PFC), normalized root mean squared error (NRMSE), AUC, and C-index scores. The analysis revealed that in terms of Gower's distance, CART and missForest were the most accurate, while missMDA and CART excelled for binary covariates; missForest and miceCART were superior for continuous covariates. When assessing bias and accuracy in regression estimates, miceCART and miceRF exhibited the least bias. Overall, the various imputation methods demonstrated greater efficiency than complete-case analysis (CCA), with MICE methods providing optimal confidence interval coverage. In terms of predictive accuracy for Cox models, missMDA and missForest had superior AUC and C-index scores. Despite offering better predictive accuracy, the study found that SI methods introduced more bias into the regression coefficients compared to MI methods. This study underlines the importance of selecting appropriate imputation methods based on study goals and data types in time-to-event research. The varying effectiveness of methods across the different performance metrics studied highlights the value of using advanced machine learning algorithms within a multiple imputation framework to enhance research integrity and the robustness of findings.},
  langid = {english},
  keywords = {Breast cancer survival,Imputation methods,Machine learning,Performance metrics,Single and multiple imputation,Survival analysis},
  file = {C:\Users\efbonneville\Zotero\storage\WJCAUL58\El Badisy et al. - 2024 - Multi-metric comparison of machine learning imputa.pdf}
}

@article{elfekyImmuneReconstitutionFollowing2019a,
  title = {Immune Reconstitution Following Hematopoietic Stem Cell Transplantation Using Different Stem Cell Sources},
  author = {Elfeky, Reem and Lazareva, Arina and Qasim, Waseem and Veys, Paul},
  year = {2019},
  month = jul,
  journal = {Expert Review of Clinical Immunology},
  volume = {15},
  number = {7},
  pages = {735--751},
  publisher = {Taylor \& Francis},
  issn = {1744-666X},
  doi = {10.1080/1744666X.2019.1612746},
  urldate = {2023-11-04},
  abstract = {Introduction: Adequate immune reconstitution post-HSCT is crucial for the success of transplantation, and can be affected by both patient- and transplant-related factors. Areas covered: A systematic literature search in PubMed, Scopus, and abstracts of international congresses is performed to investigate immune recovery posttransplant. In this review, we discuss the pattern of immune recovery in the post-transplant period focusing on the impact of stem cell source (bone marrow, peripheral blood stem cells, and cord blood) on immune recovery and HSCT outcome. We examine the impact of serotherapy on immune reconstitution and the need to tailor dosing of serotherapy agents when using different stem cell sources. We discuss new techniques being used particularly with cord blood and haploidentical grafts to improve immune recovery in each scenario. Expert opinion: Cord blood T cells provide a unique CD4+ biased immune reconstitution. Initial studies using targeted serotherapy with cord grafts showed improved immune recovery with limited alloreactivity. Two competing haploidentical approaches have developed in recent years including TCR{$\alpha\beta$}/CD19 depleted grafts and post-cyclophosphamide haplo-HSCT. Both approaches have comparable survival rates with limited alloreactivity. However, delayed immune reconstitution is still an ongoing problem and could be improved by modified donor lymphocyte infusions from the same haploidentical donor.},
  pmid = {31070946},
  keywords = {cord blood,haploidentical,Immune recovery,rATG,serotherapy,stem cell source,TCR/CD19 depleted grafts},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\754P8FBI\\Elfeky et al. - 2019 - Immune reconstitution following hematopoietic stem.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\7WPLPJ4E\\Elfeky et al. - 2019 - Immune reconstitution following hematopoietic stem.pdf}
}

@book{endersAppliedMissingData2022,
  title = {Applied Missing Data Analysis, 2nd Ed.},
  author = {Enders, Craig K.},
  year = {2022},
  series = {Applied Missing Data Analysis, 2nd Ed.},
  pages = {ix, 546},
  publisher = {The Guilford Press},
  address = {New York,  NY,  US},
  abstract = {The field of missing data has advanced probably more than any other quantitative topic area. This second edition of the book Applied Missing Data Analysis translates the missing data literature into a comprehensive, accessible resource that serves both substantive researchers who use sophisticated quantitative methods in their work as well as quantitative specialists. The author's approachable treatise provides a comprehensive treatment of the causes of missing data and how best to address them. He clarifies the principles by which various mechanisms of missing data can be recovered, and provides expert guidance on which method to implement, how to execute it, and what to report about the modern approach one has chosen. The structure of the second edition mimics that of the first, albeit with different beta weights attached to each topic. Maximum likelihood, Bayesian estimation, and multiple imputation are again the main analytic pillars. The maximum likelihood chapters cover the classic estimators described in the first edition, as well as newer methods based on factored regressions. Whereas the first edition primarily described the Bayesian framework as an estimation method co-opted for multivariate multiple imputation, this edition takes the much broader view that Bayesian analyses are an alternative to maximum likelihood estimation---and one that is arguably more capable and mature at this point in history. The multiple imputation chapter includes expanded coverage of two classic approaches---joint model imputation and fully conditional specification---as well as sections devoted to newer model-based (or substantive model-compatible) imputation strategies deriving from factored regressions. The penultimate chapter consists of a series of real data analysis examples that illustrate a broad range of specialized topics and practical issues. The book concludes with a wrap-up chapter that provides reporting guidelines and a brief tour of the current software landscape. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  isbn = {9781462549863 (Hardcover)},
  keywords = {*Bayesian Analysis,*Maximum Likelihood,*Multivariate Analysis,*Statistical Analysis,*Statistical Correlation,Quantitative Methods}
}

@article{endersMissingDataUpdate2023,
  title = {Missing Data: {{An}} Update on the State of the Art},
  shorttitle = {Missing Data},
  author = {Enders, Craig K.},
  year = {2023},
  journal = {Psychological Methods},
  pages = {No Pagination Specified-No Pagination Specified},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/met0000563},
  abstract = {The year 2022 is the 20th anniversary of Joseph Schafer and John Graham's paper titled ``Missing data: Our view of the state of the art,'' currently the most highly cited paper in the history of Psychological Methods. Much has changed since 2002, as missing data methodologies have continually evolved and improved; the range of applications that are possible with modern missing data techniques has increased dramatically, and software options are light years ahead of where they were. This article provides an update on the state of the art that catalogs important innovations from the past two decades of missing data research. The paper addresses topics described in the original paper, including developments related to missing data theory, full information maximum likelihood, Bayesian estimation, multiple imputation, and models for missing not at random processes. The paper also describes newer factored regression specifications and missing data handling for multilevel models, both of which have been a focus of recent research. The paper concludes with a summary of the current software landscape and a discussion of several practical issues. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
  keywords = {Maximum Likelihood,Models,Statistical Data,Statistical Estimation,Statistical Probability},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\XHVTPTN6\\Enders - 2023 - Missing data An update on the state of the art.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\VPPGR7MM\\2023-54424-001.html}
}

@article{erikssonLargeSampleResults2020,
  title = {Large Sample Results for Frequentist Multiple Imputation for {{Cox}} Regression with Missing Covariate Data},
  author = {Eriksson, Frank and Martinussen, Torben and Nielsen, S{\o}ren Feodor},
  year = {2020},
  month = aug,
  journal = {Annals of the Institute of Statistical Mathematics},
  volume = {72},
  number = {4},
  pages = {969--996},
  issn = {1572-9052},
  doi = {10.1007/s10463-019-00716-4},
  urldate = {2024-12-12},
  abstract = {Incomplete information on explanatory variables is commonly encountered in studies of possibly censored event times. A popular approach to deal with partially observed covariates is multiple imputation, where a number of completed data sets, that can be analyzed by standard complete data methods, are obtained by imputing missing values from an appropriate distribution. We show how the combination of multiple imputations from a compatible model with suitably estimated parameters and the usual Cox regression estimators leads to consistent and asymptotically Gaussian estimators of both the finite-dimensional regression parameter and the infinite-dimensional cumulative baseline hazard parameter. We also derive a consistent estimator of the covariance operator. Simulation studies and an application to a study on survival after treatment for liver cirrhosis show that the estimators perform well with moderate sample sizes and indicate that iterating the multiple-imputation estimator increases the precision.},
  langid = {english},
  keywords = {Asymptotic distribution,Coarsened data,Semiparametric,Survival,Variance estimator},
  file = {C:\Users\efbonneville\Zotero\storage\WBWPWLRA\Eriksson et al. - 2020 - Large sample results for frequentist multiple impu.pdf}
}

@article{erlerDealingMissingCovariates2016,
  title = {Dealing with Missing Covariates in Epidemiologic Studies: A Comparison between Multiple Imputation and a Full {{Bayesian}} Approach},
  shorttitle = {Dealing with Missing Covariates in Epidemiologic Studies},
  author = {Erler, Nicole S. and Rizopoulos, Dimitris and van Rosmalen, Joost and Jaddoe, Vincent W. V. and Franco, Oscar H. and Lesaffre, Emmanuel M. E. H.},
  year = {2016},
  journal = {Statistics in Medicine},
  volume = {35},
  number = {17},
  pages = {2955--2974},
  issn = {1097-0258},
  doi = {10.1002/sim.6944},
  urldate = {2020-03-31},
  abstract = {Incomplete data are generally a challenge to the analysis of most large studies. The current gold standard to account for missing data is multiple imputation, and more specifically multiple imputation with chained equations (MICE). Numerous studies have been conducted to illustrate the performance of MICE for missing covariate data. The results show that the method works well in various situations. However, less is known about its performance in more complex models, specifically when the outcome is multivariate as in longitudinal studies. In current practice, the multivariate nature of the longitudinal outcome is often neglected in the imputation procedure, or only the baseline outcome is used to impute missing covariates. In this work, we evaluate the performance of MICE using different strategies to include a longitudinal outcome into the imputation models and compare it with a fully Bayesian approach that jointly imputes missing values and estimates the parameters of the longitudinal model. Results from simulation and a real data example show that MICE requires the analyst to correctly specify which components of the longitudinal process need to be included in the imputation models in order to obtain unbiased results. The full Bayesian approach, on the other hand, does not require the analyst to explicitly specify how the longitudinal outcome enters the imputation models. It performed well under different scenarios. Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\T8ALCJ9Y\\Erler et al. - 2016 - Dealing with missing covariates in epidemiologic s.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\V4NP32LD\\sim.html}
}

@article{erlerJointAIJointAnalysis2021,
  title = {{{JointAI}}: {{Joint Analysis}} and {{Imputation}} of {{Incomplete Data}} in {{R}}},
  shorttitle = {{{JointAI}}},
  author = {Erler, Nicole S. and Rizopoulos, Dimitris and Lesaffre, Emmanuel M. E. H.},
  year = {2021},
  month = nov,
  journal = {Journal of Statistical Software},
  volume = {100},
  pages = {1--56},
  issn = {1548-7660},
  doi = {10.18637/jss.v100.i20},
  urldate = {2022-10-17},
  abstract = {Missing data occur in many types of studies and typically complicate the analysis. Multiple imputation, either using joint modeling or the more flexible fully conditional specification approach, are popular and work well in standard settings. In settings involving nonlinear associations or interactions, however, incompatibility of the imputation model with the analysis model is an issue often resulting in bias. Similarly, complex outcomes such as longitudinal or survival outcomes cannot be adequately handled by standard implementations. In this paper, we introduce the R package JointAI, which utilizes the Bayesian framework to perform simultaneous analysis and imputation in regression models with incomplete covariates. Using a fully Bayesian joint modeling approach it overcomes the issue of uncongeniality while retaining the attractive flexibility of fully conditional specification multiple imputation by specifying the joint distribution of analysis and imputation models as a sequence of univariate models that can be adapted to the type of variable. JointAI provides functions for Bayesian inference with generalized linear and generalized linear mixed models and extensions thereof as well as survival models and joint models for longitudinal and survival data, that take arguments analogous to the corresponding well known functions for the analysis of complete data from base R and other packages. Usage and features of JointAI are described and illustrated using various examples and the theoretical background is outlined.},
  copyright = {Copyright (c) 2021 Nicole S. Erler, Dimitris Rizopoulos, Emmanuel M. E. H. Lesaffre},
  langid = {english},
  keywords = {JAGS,Statistics - Computation,Statistics - Methodology},
  file = {C:\Users\efbonneville\Zotero\storage\4BVEV634\Erler et al. - 2021 - JointAI Joint Analysis and Imputation of Incomple.pdf}
}

@article{falcaroEstimatingExcessHazard2015,
  title = {Estimating {{Excess Hazard Ratios}} and {{Net Survival When Covariate Data Are Missing}}: {{Strategies}} for {{Multiple Imputation}}},
  shorttitle = {Estimating {{Excess Hazard Ratios}} and {{Net Survival When Covariate Data Are Missing}}},
  author = {Falcaro, Milena and Nur, Ula and Rachet, Bernard and Carpenter, James R.},
  year = {2015},
  month = may,
  journal = {Epidemiology},
  volume = {26},
  number = {3},
  pages = {421--428},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000000283},
  urldate = {2020-11-09},
  abstract = {Background:~         Net survival is the survival probability we would observe if the disease under study were the only cause of death. When estimated from routinely collected population-based cancer registry data, this indicator is a key metric for cancer control. Unfortunately, such data typically contain a non-negligible proportion of missing values on important prognostic factors (eg, tumor stage).         Methods:~         We carried out an empirical study to compare the performance of complete records analysis and several multiple imputation strategies when net survival is estimated via a flexible parametric proportional hazards model that includes stage, a partially observed categorical covariate. Starting from fully observed cancer registry data, we induced missingness on stage under three scenarios. For each of these scenarios, we simulated 100 incomplete datasets and evaluated the performance of the different strategies.         Results:~         Ordinal logistic models are not suitable for the imputation of tumor stage. Complete records analysis may lead to grossly misleading estimates of net survival, even when the missing data mechanism is conditionally independent of survival time given the covariates and the bias on the excess hazard ratios estimates is negligible.         Conclusions:~         As key covariates are unlikely missing completely at random, studies estimating net survival should not use complete records. When the missingness can be inferred from available data, appropriate multiple imputation should be performed. In the context of flexible parametric proportional hazards models with a partially observed stage covariate, a multinomial logistic imputation model for stage should be used and should include the Nelson-Aalen cumulative hazard estimate and the event indicator.},
  langid = {american}
}

@article{falkenburgGraftTumorEffects2017,
  title = {Graft versus Tumor Effects and Why People Relapse},
  author = {Falkenburg, J. H. Frederik and Jedema, Inge},
  year = {2017},
  month = dec,
  journal = {Hematology},
  volume = {2017},
  number = {1},
  pages = {693--698},
  issn = {1520-4391},
  doi = {10.1182/asheducation-2017.1.693},
  urldate = {2023-11-04},
  abstract = {Graft-versus-tumor (GVT) reactivity mediated by donor T cells in the context of allogeneic stem cell transplantation (alloSCT) is one of the most potent forms of cellular immunotherapy. The antitumor effect against hematologic malignancies is mediated by a polyclonal T-cell response targeting polymorphic antigens expressed on hematopoietic tissues of the recipient, leaving donor hematopoiesis in the patient after transplantation unharmed. Fortunately, hematopoietic tissues (including malignant hematopoietic cell populations) are relatively susceptible to T-cell recognition. If, however, nonhematopoietic tissues of the recipient are targeted as well, graft-versus-host disease (GVHD) will occur. The balance between GVT and GVHD is influenced by the genetic disparity between donor and recipient, the number and origin of professional antigen-presenting cells provoking the immune response, the target antigen specificity, magnitude and diversity of the response, and the in vivo inflammatory environment, whereas inhibitory factors may silence the immune response. Manipulation of each of these factors will determine the balance between GVT and GVHD.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\8PSU7TJG\\Falkenburg and Jedema - 2017 - Graft versus tumor effects and why people relapse.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\WQLWP2K4\\Graft-versus-tumor-effects-and-why-people-relapse.html}
}

@article{fineProportionalHazardsModel1999,
  title = {A {{Proportional Hazards Model}} for the {{Subdistribution}} of a {{Competing Risk}}},
  author = {Fine, Jason P. and Gray, Robert J.},
  year = {1999},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {94},
  number = {446},
  pages = {496--509},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.1999.10474144},
  urldate = {2020-11-09},
  abstract = {With explanatory covariates, the standard analysis for competing risks data involves modeling the cause-specific hazard functions via a proportional hazards assumption. Unfortunately, the cause-specific hazard function does not have a direct interpretation in terms of survival probabilities for the particular failure type. In recent years many clinicians have begun using the cumulative incidence function, the marginal failure probabilities for a particular cause, which is intuitively appealing and more easily explained to the nonstatistician. The cumulative incidence is especially relevant in cost-effectiveness analyses in which the survival probabilities are needed to determine treatment utility. Previously, authors have considered methods for combining estimates of the cause-specific hazard functions under the proportional hazards formulation. However, these methods do not allow the analyst to directly assess the effect of a covariate on the marginal probability function. In this article we propose a novel semiparametric proportional hazards model for the subdistribution. Using the partial likelihood principle and weighting techniques, we derive estimation and inference procedures for the finite-dimensional regression parameter under a variety of censoring scenarios. We give a uniformly consistent estimator for the predicted cumulative incidence for an individual with certain covariates; confidence intervals and bands can be obtained analytically or with an easy-to-implement simulation technique. To contrast the two approaches, we analyze a dataset from a breast cancer clinical trial under both models.},
  keywords = {Hazard of subdistribution,Martingale,Partial likelihood,Transformation model},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\USQDNA8R\\Fine and Gray - 1999 - A Proportional Hazards Model for the Subdistributi.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\MMGICLU8\\01621459.1999.html}
}

@book{fitzmaurice2008longitudinal,
  title = {Longitudinal Data Analysis},
  author = {Fitzmaurice, G. and Davidian, M. and Verbeke, G. and Molenberghs, G.},
  year = {2008},
  series = {{{ISSN}}},
  publisher = {CRC Press},
  isbn = {978-1-4200-1157-9},
  lccn = {2008020681}
}

@article{fleischhauerEffectTcellepitopeMatching2012,
  title = {Effect of {{T-cell-epitope}} Matching at {{HLA-DPB1}} in Recipients of Unrelated-Donor Haemopoietic-Cell Transplantation: A Retrospective Study},
  shorttitle = {Effect of {{T-cell-epitope}} Matching at {{HLA-DPB1}} in Recipients of Unrelated-Donor Haemopoietic-Cell Transplantation},
  author = {Fleischhauer, Katharina and Shaw, Bronwen E. and Gooley, Theodore and Malkki, Mari and Bardy, Peter and Bignon, Jean-Denis and Dubois, Val{\'e}rie and Horowitz, Mary M. and Madrigal, J. Alejandro and Morishima, Yasuo and Oudshoorn, Machteld and Ringden, Olle and Spellman, Stephen and Velardi, Andrea and Zino, Elisabetta and Petersdorf, Effie W.},
  year = {2012},
  month = apr,
  journal = {The Lancet Oncology},
  volume = {13},
  number = {4},
  pages = {366--374},
  publisher = {Elsevier},
  issn = {1470-2045, 1474-5488},
  doi = {10.1016/S1470-2045(12)70004-9},
  urldate = {2023-11-04},
  langid = {english},
  pmid = {22340965},
  file = {C:\Users\efbonneville\Zotero\storage\LRAZ8R26\Fleischhauer et al. - 2012 - Effect of T-cell-epitope matching at HLA-DPB1 in r.pdf}
}

@article{frederik2019delayed,
  title = {Delayed Transfer of Immune Cells or the Art of Donor Lymphocyte Infusion},
  author = {Frederik Falkenburg, {\relax JH} and Schmid, Christoph and Kolb, Hans Joachim and Locatelli, Franco and Kuball, J{\"u}rgen},
  year = {2019},
  journal = {The EBMT Handbook: Hematopoietic Stem Cell Transplantation and Cellular Therapies},
  pages = {443--448},
  publisher = {Springer International Publishing}
}

@article{gaspariniRsimsumSummariseResults2018,
  title = {Rsimsum: {{Summarise}} Results from {{Monte Carlo}} Simulation Studies},
  author = {Gasparini, Alessandro},
  year = {2018},
  journal = {Journal of Open Source Software},
  volume = {3},
  number = {26},
  pages = {739},
  doi = {10.21105/joss.00739}
}

@article{geBayesianInferenceFully2012,
  title = {Bayesian Inference of the Fully Specified Subdistribution Model for Survival Data with Competing Risks},
  author = {Ge, Miaomiao and Chen, Ming-Hui},
  year = {2012},
  month = jul,
  journal = {Lifetime Data Analysis},
  volume = {18},
  number = {3},
  pages = {339--363},
  issn = {1572-9249},
  doi = {10.1007/s10985-012-9221-9},
  urldate = {2021-07-05},
  abstract = {Competing risks data are routinely encountered in various medical applications due to the fact that patients may die from different causes. Recently, several models have been proposed for fitting such survival data. In this paper, we develop a fully specified subdistribution model for survival data in the presence of competing risks via a subdistribution model for the primary cause of death and conditional distributions for other causes of death. Various properties of this fully specified subdistribution model have been examined. An efficient Gibbs sampling algorithm via latent variables is developed to carry out posterior computations. Deviance information criterion (DIC) and logarithm of the pseudomarginal likelihood (LPML) are used for model comparison. An extensive simulation study is carried out to examine the performance of DIC and LPML in comparing the cause-specific hazards model, the mixture model, and the fully specified subdistribution model. The proposed methodology is applied to analyze a real dataset from a prostate cancer study in detail.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\DTTP7MU6\Ge and Chen - 2012 - Bayesian inference of the fully specified subdistr.pdf}
}

@article{gerdsAbsoluteRiskRegression2012a,
  title = {Absolute Risk Regression for Competing Risks: Interpretation, Link Functions, and Prediction},
  shorttitle = {Absolute Risk Regression for Competing Risks},
  author = {Gerds, Thomas A. and Scheike, Thomas H. and Andersen, Per K.},
  year = {2012},
  journal = {Statistics in Medicine},
  volume = {31},
  number = {29},
  pages = {3921--3930},
  issn = {1097-0258},
  doi = {10.1002/sim.5459},
  urldate = {2023-10-06},
  abstract = {In survival analysis with competing risks, the transformation model allows different functions between the outcome and explanatory variables. However, the model's prediction accuracy and the interpretation of parameters may be sensitive to the choice of link function. We review the practical implications of different link functions for regression of the absolute risk (or cumulative incidence) of an event. Specifically, we consider models in which the regression coefficients {$\beta$} have the following interpretation: The probability of dying from cause D during the next t years changes with a factor exp({$\beta$}) for a one unit change of the corresponding predictor variable, given fixed values for the other predictor variables. The models have a direct interpretation for the predictive ability of the risk factors. We propose some tools to justify the models in comparison with traditional approaches that combine a series of cause-specific Cox regression models or use the Fine--Gray model. We illustrate the methods with the use of bone marrow transplant data. Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {absolute risk,competing risk,cumulative incidence,prediction model,regression model},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\CRTCFR7R\\Gerds et al. - 2012 - Absolute risk regression for competing risks inte.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\ZIRBJ68L\\sim.html}
}

@article{gerdsCalibrationPlotsRisk2014a,
  title = {Calibration Plots for Risk Prediction Models in the Presence of Competing Risks},
  author = {Gerds, Thomas A. and Andersen, Per K. and Kattan, Michael W.},
  year = {2014},
  journal = {Statistics in Medicine},
  volume = {33},
  number = {18},
  pages = {3191--3203},
  issn = {1097-0258},
  doi = {10.1002/sim.6152},
  urldate = {2023-10-06},
  abstract = {AbstractA predicted risk of 17\% can be called reliable if it can be expected that the event will occur to about 17 of 100 patients who all received a predicted risk of 17\%. Statistical models can predict the absolute risk of an event such as cardiovascular death in the presence of competing risks such as death due to other causes. For personalized medicine and patient counseling, it is necessary to check that the model is calibrated in the sense that it provides reliable predictions for all subjects. There are three often encountered practical problems when the aim is to display or test if a risk prediction model is well calibrated. The first is lack of independent validation data, the second is right censoring, and the third is that when the risk scale is continuous, the estimation problem is as difficult as density estimation. To deal with these problems, we propose to estimate calibration curves for competing risks models based on jackknife pseudo-values that are combined with a nearest neighborhood smoother and a cross-validation approach to deal with all three problems. Copyright {\copyright} 2014 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2014 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {calibration plots,competing risks,kernel smoothing,pseudo-values,risk models},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\3BTUG5KR\\Gerds et al. - 2014 - Calibration plots for risk prediction models in th.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\A5BKUG8D\\sim.html}
}

@book{geskus2015data,
  title = {Data Analysis with Competing Risks and Intermediate States},
  author = {Geskus, Ronald B},
  year = {2015},
  volume = {82},
  publisher = {CRC Press}
}

@article{geskusCauseSpecificCumulativeIncidence2011,
  title = {Cause-{{Specific Cumulative Incidence Estimation}} and the {{Fine}} and {{Gray Model Under Both Left Truncation}} and {{Right Censoring}}},
  author = {Geskus, Ronald B.},
  year = {2011},
  journal = {Biometrics},
  volume = {67},
  number = {1},
  pages = {39--49},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2010.01420.x},
  urldate = {2021-05-10},
  abstract = {The standard estimator for the cause-specific cumulative incidence function in a competing risks setting with left truncated and/or right censored data can be written in two alternative forms. One is a weighted empirical cumulative distribution function and the other a product-limit estimator. This equivalence suggests an alternative view of the analysis of time-to-event data with left truncation and right censoring: individuals who are still at risk or experienced an earlier competing event receive weights from the censoring and truncation mechanisms. As a consequence, inference on the cumulative scale can be performed using weighted versions of standard procedures. This holds for estimation of the cause-specific cumulative incidence function as well as for estimation of the regression parameters in the Fine and Gray proportional subdistribution hazards model. We show that, with the appropriate filtration, a martingale property holds that allows deriving asymptotic results for the proportional subdistribution hazards model in the same way as for the standard Cox proportional hazards model. Estimation of the cause-specific cumulative incidence function and regression on the subdistribution hazard can be performed using standard software for survival analysis if the software allows for inclusion of time-dependent weights. We show the implementation in the R statistical package. The proportional subdistribution hazards model is used to investigate the effect of calendar period as a deterministic external time varying covariate, which can be seen as a special case of left truncation, on AIDS related and non-AIDS related cumulative mortality.},
  copyright = {{\copyright} 2010, The International Biometric Society},
  langid = {english},
  keywords = {Competing risks,Inverse probability weight,Subdistribution hazard,Survival analysis}
}

@article{geskusCompetingRisksConcepts2024,
  title = {Competing {{Risks}}: {{Concepts}}, {{Methods}}, and {{Software}}},
  shorttitle = {Competing {{Risks}}},
  author = {Geskus, Ronald B.},
  year = {2024},
  month = apr,
  journal = {Annual Review of Statistics and Its Application},
  volume = {11},
  number = {Volume 11, 2024},
  pages = {227--254},
  publisher = {Annual Reviews},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-040522-094556},
  urldate = {2024-10-31},
  abstract = {The role of competing risks in the analysis of time-to-event data is increasingly acknowledged. Software is readily available. However, confusion remains regarding the proper analysis: When and how do I need to take the presence of competing risks into account? Which quantities are relevant for my research question? How can they be estimated and what assumptions do I need to make? The main quantities in a competing risks analysis are the cause-specific cumulative incidence, the cause-specific hazard, and the subdistribution hazard. We describe their nonparametric estimation, give an overview of regression models for each of these quantities, and explain their difference in interpretation. We discuss the proper analysis in relation to the type of study question, and we suggest software in R and Stata. Our focus is on competing risks analysis in medical research, but methods can equally be applied in other fields like social science, engineering, and economics.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\YI38KQK5\annurev-statistics-040522-094556.html}
}

@article{ghoshComparisonRegressionApproaches2021,
  title = {Comparison of {{Regression Approaches}} for {{Analyzing Survival Data}} in the {{Presence}} of {{Competing Risks}}:},
  shorttitle = {Comparison of {{Regression Approaches}} for {{Analyzing Survival Data}} in the {{Presence}} of {{Competing Risks}}},
  author = {Ghosh, Sarada and Samanta, G. P. and Mubayi, Anuj},
  year = {2021},
  month = apr,
  journal = {Letters in Biomathematics},
  volume = {8},
  number = {1},
  pages = {29--47-29--47},
  issn = {2373-7867},
  urldate = {2021-05-18},
  abstract = {Emerging infectious diseases have impacted human race regularly with the past few decades alone has been rife with outbreaks such as H7N9 Avian-influenza (2013), Ebola (2014), MERS-CoV (2012), SARS-CoV1 (2003), and Zika (2015). COVID-19 coronavirus variants are emerging across the globe causing ongoing pandemic. Older age, male sex, number of comorbidities, and access to timely health care are identified as some of the risk factors associated with COVID-19 mortality. The regression approaches for capturing the competing risks are applied to COVID-19 in this work. The most commonly used approaches are the cause-specific and sub-distribution hazards regression which are applied on the COVID-19 incidence-data from USA. Additionally, the pseudo-observation approach, which allows for analysis of survival data, is applied on the same data. The simulations are carried out to compare approaches under different scenarios and also illustrate the relative effect of COVID-19 infected people based on their gender and age.},
  copyright = {Copyright (c) 2021 Letters in Biomathematics},
  langid = {english},
  keywords = {Cause-specific hazard regression,Competing risk,Cumulative incidence function,Pseudo approach,Sub-distribution hazard regression}
}

@article{gooptuEffectSirolimusImmune2019,
  title = {Effect of {{Sirolimus}} on {{Immune Reconstitution Following Myeloablative Allogeneic Stem Cell Transplantation}}: {{An Ancillary Analysis}} of a {{Randomized Controlled Trial Comparing Tacrolimus}}/{{Sirolimus}} and {{Tacrolimus}}/{{Methotrexate}} ({{Blood}} and {{Marrow Transplant Clinical Trials Network}}/{{BMT CTN}} 0402)},
  shorttitle = {Effect of {{Sirolimus}} on {{Immune Reconstitution Following Myeloablative Allogeneic Stem Cell Transplantation}}},
  author = {Gooptu, Mahasweta and Kim, Haesook T. and Howard, Alan and Choi, Sung W. and Soiffer, Robert J. and Antin, Joseph H. and Ritz, Jerome and Cutler, Corey S.},
  year = {2019},
  month = nov,
  journal = {Biology of Blood and Marrow Transplantation},
  volume = {25},
  number = {11},
  pages = {2143--2151},
  issn = {1083-8791},
  doi = {10.1016/j.bbmt.2019.06.029},
  urldate = {2023-11-04},
  abstract = {Although allogeneic hematopoietic cell transplantation (HCT) is a potentially curative therapy for hematologic neoplasms, one of its limiting toxicities continues to be graft-versus-host disease, both acute (aGVHD) and chronic (cGVHD). Sirolimus is a mammalian target of rapamycin inhibitor that has proven effective in GVHD prophylaxis in combination with a calcineurin inhibitor, such as tacrolimus. The impact of sirolimus on immune reconstitution has not been comprehensively investigated in vivo thus far, however. Here we present an ancillary analysis of the randomized study BMT-CTN 0402 that examined the effect of sirolimus on immune subsets post-transplantation. We further examine the association between different lymphocyte subsets and outcomes post-transplantation in each arm. BMT-CTN 0402 was a randomized trial (n\,=\,304) comparing 2 GVHD prophylaxis regimens, tacrolimus/sirolimus (Tac/Sir) and tacrolimus/methotrexate (Tac/MTX), in patients with acute myelogenous leukemia, acute lymphoblastic leukemia, or myelodysplastic syndrome undergoing myeloablative HLA-matched HCT. There were no differences in 114-day GVHD-free survival (primary endpoint), aGVHD, cGVHD, relapse, or overall survival (OS) between the 2 arms. Of the 304 patients, 264 had available samples for the current immune reconstitution analysis. Blood samples were collected at 1, 3, 6, 12, and 24 months post-HCT. Multiparameter flow cytometry was performed at the project laboratory (Esoterix Clinical Trials Services) in a blinded fashion, and results for the 2 arms were compared. Multivariable Cox regression models, treating each phenotypic parameter as a time-dependent variable, were constructed to study the impact of reconstitution on clinical outcomes. There were no significant differences in patient and transplantation characteristics between the Tac/Sir and Tac/MTX arms in this analysis. Absolute lymphocyte count and CD3+ cell, CD4+ cell, and conventional T cell (Tcon) counts were significantly decreased in the Tac/Sir arm for up to 3 months post-HCT, whereas CD8+ cells recovered even more slowly (up to 6 months) in this arm. Interestingly, there was no clear difference in the absolute number of regulatory T cells (Tregs, defined as CD4+CD25+ cells) between the 2 arms at any point post-HCT; however, the Treg:Tcon ratio was significantly greater in the Tac/Sir arm in the first 3 months after HCT. B lymphocyte recovery was significantly compromised in the Tac/Sir arm from 1 month to 6 months after HCT, whereas natural killer cell reconstitution was not affected in the Tac/Sir arm. In the outcomes analysis, higher numbers of CD3+ cells, CD4+ cells, CD8+ cells, and Tregs were associated with better OS. Neither Treg numbers nor the Treg:Tcon ratio was correlated with GVHD. Our findings indicate that Tac/Sir has a more profound T cell suppressive effect than the combination of Tac/MTX in the early post-transplantation period, and particularly compromises the recovery of CD8+ T cells, which have been implicated in aGVHD. Sirolimus used in vivo with tacrolimus does not appear to result in increased absolute numbers of Tregs, but might have a beneficial effect on the Treg:Tcon balance in the first 3 months after transplantation. Nonetheless, no differences in aGVHD or cGVHD between the 2 arms were observed in the parent randomized trial. Calcineurin-inhibitor free, sirolimus-containing GVHD prophylaxis strategies, incorporating other novel agents, should be investigated further to maximize the potential favorable effect of sirolimus on Treg:Tcon balance in the post-transplantation immune repertoire. Sirolimus significantly compromises B cell recovery in the first 6 months post-HCT, with potential complex effects on cGVHD that merit further study.},
  keywords = {B lymphocytes,Immune reconstitution,Myeloablative,Sirolimus,Tregs},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\DMQB8WXZ\\Gooptu et al. - 2019 - Effect of Sirolimus on Immune Reconstitution Follo.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\TGIWC48I\\S1083879119304136.html}
}

@article{gorgizadehImputationApproachUsing2022a,
  title = {An Imputation Approach Using Subdistribution Weights for Deep Survival Analysis with Competing Events},
  author = {Gorgi Zadeh, Shekoufeh and Behning, Charlotte and Schmid, Matthias},
  year = {2022},
  month = mar,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {3815},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-07828-7},
  urldate = {2024-05-02},
  abstract = {With the popularity of deep neural networks (DNNs) in recent years, many researchers have proposed DNNs for the analysis of survival data (time-to-event data). These networks learn the distribution of survival times directly from the predictor variables without making strong assumptions on the underlying stochastic process. In survival analysis, it is common to observe several types of events, also called competing events. The occurrences of these competing events are usually not independent of one another and have to be incorporated in the modeling process in addition to censoring. In classical survival analysis, a popular method to incorporate competing events is the subdistribution hazard model, which is usually fitted using weighted Cox regression. In the DNN framework, only few architectures have been proposed to model the distribution of time to a specific event in a competing events situation. These architectures are characterized by a separate subnetwork/pathway per event, leading to large networks with huge amounts of parameters that may become difficult to train. In this work, we propose a novel imputation strategy for data preprocessing that incorporates weights derived from a time-discrete version of the classical subdistribution hazard model. With this, it is no longer necessary to add multiple subnetworks to the DNN to handle competing events. Our experiments on synthetic and real-world datasets show that DNNs with multiple subnetworks per event can simply be replaced by a DNN designed for a single-event analysis without loss in accuracy.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Machine learning,Statistical methods},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\9B2KIIN4\\Gorgi Zadeh et al. - 2022 - An imputation approach using subdistribution weigh.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\IMYQ2XZ3\\s41598-022-07828-7.html}
}

@article{grambauerProportionalSubdistributionHazards2010,
  title = {Proportional Subdistribution Hazards Modeling Offers a Summary Analysis, Even If Misspecified},
  author = {Grambauer, Nadine and Schumacher, Martin and Beyersmann, Jan},
  year = {2010},
  journal = {Statistics in Medicine},
  volume = {29},
  number = {7-8},
  pages = {875--884},
  issn = {1097-0258},
  doi = {10.1002/sim.3786},
  urldate = {2020-11-15},
  abstract = {Competing risks model time-to-first-event and the event type. Our motivating data example is the ONKO-KISS study on the occurrence of infections in neutropenic patients after stem-cell transplantation with first-event-types `infection' and `end of neutropenia'. The standard approach to study the effects of covariates in competing risks is to assume each event-specific hazard (ESH) to follow a proportional hazards model. However, a summarizing probability interpretation of the different event-specific effects of one covariate can be challenging. This difficulty has led to the development of the proportional subdistribution hazards model of a competing event of interest. However, one model specification usually precludes the other. Assuming proportional ESHs, we find that the subdistribution log-hazard ratio may show a pronounced time-dependency, even changing sign. Still, the subdistribution analysis is useful by estimating the least false parameter (LFP), a time-averaged effect on the cumulative event probabilities. In examples, we find that the LFP offers a robust summary of the effects on the ESHs for different observation periods, ranging from heavy censoring to no censoring at all. In particular, if there is no effect on the competing ESH, the subdistribution log-hazard ratio is close to the event-specific log-hazard ratio of interest. We reanalyze an interpretationally challenging example from the ONKO-KISS study and conduct a simulation study, where we find that the LFP is reliably estimated by the subdistribution analysis even for moderate sample sizes. Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {cause-specific hazard,competing risks,Cox model,Fine and Gray model,model misspecification},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\8MPULHBN\\Grambauer et al. - 2010 - Proportional subdistribution hazards modeling offe.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\MLQIV34I\\sim.html}
}

@article{grayClassKSampleTests1988,
  title = {A {{Class}} of {{K-Sample Tests}} for {{Comparing}} the {{Cumulative Incidence}} of a {{Competing Risk}}},
  author = {Gray, Robert J.},
  year = {1988},
  journal = {The Annals of Statistics},
  volume = {16},
  number = {3},
  eprint = {2241622},
  eprinttype = {jstor},
  pages = {1141--1154},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364},
  urldate = {2021-05-11},
  abstract = {In this paper, for right censored competing risks data, a class of tests developed for comparing the cumulative incidence of a particular type of failure among different groups. The tests are based on comparing weighted averages of the hazards of the subdistribution for the failure type of interest. Asymptotic results are derived by expressing the statistics in terms of counting processes and using martingale central limit theory. It is proposed that weight functions very similar to those for the Gp tests from ordinary survival analysis be used. Simulation results indicate that the asymptotic distributions provide adequate approximations in moderate sized samples.}
}

@article{groenwoldMissingCovariateData2012,
  title = {Missing Covariate Data in Clinical Research: When and When Not to Use the Missing-Indicator Method for Analysis},
  shorttitle = {Missing Covariate Data in Clinical Research},
  author = {Groenwold, Rolf H. H. and White, Ian R. and Donders, A. Rogier T. and Carpenter, James R. and Altman, Douglas G. and Moons, Karel G. M.},
  year = {2012},
  month = aug,
  journal = {CMAJ},
  volume = {184},
  number = {11},
  pages = {1265--1269},
  publisher = {CMAJ},
  issn = {0820-3946, 1488-2329},
  doi = {10.1503/cmaj.110977},
  urldate = {2023-01-27},
  abstract = {Missing data are a frequently encountered problem in epidemiologic and clinical research.[1][1],[2][2] One approach is to include in the analysis only those participants without missing observations (complete or available case analysis).[1][1]--[4][3] However, in addition to reducing statistical},
  chapter = {Analysis},
  copyright = {{\copyright} 2012 Canadian Medical Association or its licensors},
  langid = {english},
  pmid = {22371511},
  file = {C:\Users\efbonneville\Zotero\storage\4XGUBPJ3\Groenwold et al. - 2012 - Missing covariate data in clinical research when .pdf}
}

@article{guillaumeEscalatedLymphodepletionFollowed2012,
  title = {Escalated Lymphodepletion Followed by Donor Lymphocyte Infusion Can Induce a Graft-versus-Host Response without Overwhelming Toxicity},
  author = {Guillaume, T. and Gaugler, B. and Chevallier, P. and Delaunay, J. and Ayari, S. and Clavert, A. and Rialland, F. and Le Gouill, S. and Blin, N. and Gastinne, T. and Mah{\'e}, B. and Dubruille, V. and Moreau, P. and Mohty, M.},
  year = {2012},
  month = aug,
  journal = {Bone Marrow Transplantation},
  volume = {47},
  number = {8},
  pages = {1112--1117},
  publisher = {Nature Publishing Group},
  issn = {1476-5365},
  doi = {10.1038/bmt.2011.231},
  urldate = {2023-11-04},
  abstract = {Treatment of relapse of hematological malignancies following allogeneic hematopoietic SCT (allo-HSCT) remains very challenging and relies usually on the readministration of chemotherapy combined with donor lymphocyte infusion (DLI). To enhance DLI effectiveness, lymphodepletion (LD) with fludarabine (Flu) and/or CY before the injection of lymphocytes is an attractive modality to modify the immune environment, leading possibly to suppression of regulatory T cells (Treg) and exposing the patient to cytokine activation. However, LD before DLI may lead to induction of deleterious GVHD. To avoid inducing overwhelming toxicity, we proceeded by escalating doses of both LD and DLI. Eighteen patients with various non-CML hematological malignancies who relapsed following allo-HSCT were treated with chemotherapy and LD-DLI or LD-DLI upfront. T-cell subpopulation and DC levels as well as cytokine plasma levels (IL-7, IL-15) were measured before and following LD-DLI. Cumulative incidence of acute grade II--IV GVHD was 29.4\% similar to that reported in patients receiving DLI without LD. In addition, Flu alone with low dose of DLI was not associated with severe GHVD. CY/Flu at the respective doses of 600\,mg/m2 on day 1 and Flu 25\,mg/m2/day on days 1--3 did not result in a marked decrease of Treg cells, nor in endogenous IL-7 and IL-15 production. However, a peripheral expansion of DCs was observed. These findings suggest that the escalated dose procedure appears safe and prevent overwhelming toxicity. A dose-limiting toxicity has not yet been reached.},
  copyright = {2012 Macmillan Publishers Limited},
  langid = {english},
  keywords = {Cell transplantation,Graft-versus-host disease,Haematological cancer},
  file = {C:\Users\efbonneville\Zotero\storage\QYLSV5KP\Guillaume et al. - 2012 - Escalated lymphodepletion followed by donor lympho.pdf}
}

@article{haenschMultipleImputationPartially2022,
  title = {Multiple Imputation of Partially Observed Covariates in Discrete-Time Survival Analysis},
  author = {Haensch, Anna-Carolina and Bartlett, Jonathan and Wei{\ss}, Bernd},
  year = {2022},
  month = dec,
  journal = {Sociological Methods \& Research},
  pages = {00491241221140147},
  publisher = {SAGE Publications Inc},
  issn = {0049-1241},
  doi = {10.1177/00491241221140147},
  urldate = {2023-05-14},
  abstract = {Discrete-time survival analysis (DTSA) models are a popular way of modeling events in the social sciences. However, the analysis of discrete-time survival data is challenged by missing data in one or more covariates. Negative consequences of missing covariate data include efficiency losses and possible bias. A popular approach to circumventing these consequences is multiple imputation (MI). In MI, it is crucial to include outcome information in the imputation models. As there is little guidance on how to incorporate the observed outcome information into the imputation model of missing covariates in DTSA, we explore different existing approaches using fully conditional specification (FCS) MI and substantive-model compatible (SMC)-FCS MI. We extend SMC-FCS for DTSA and provide an implementation in the smcfcs R package. We compare the approaches using Monte Carlo simulations and demonstrate a good performance of the new approach compared to existing approaches.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\NCLTDYL8\Haensch et al. - 2022 - Multiple imputation of partially observed covariat.pdf}
}

@article{haile3parameterGompertzDistribution2016,
  title = {A 3-Parameter {{Gompertz}} Distribution for Survival Data with Competing Risks, with an Application to Breast Cancer Data},
  author = {Haile, S. R. and Jeong, J.-H. and Chen, X. and Cheng, Y.},
  year = {2016},
  month = sep,
  journal = {Journal of Applied Statistics},
  volume = {43},
  number = {12},
  pages = {2239--2253},
  publisher = {Taylor \& Francis},
  issn = {0266-4763},
  doi = {10.1080/02664763.2015.1134450},
  urldate = {2023-10-08},
  abstract = {The cumulative incidence function is of great importance in the analysis of survival data when competing risks are present. Parametric modeling of such functions, which are by nature improper, suggests the use of improper distributions. One frequently used improper distribution is that of Gompertz, which captures only monotone hazard shapes. In some applications, however, subdistribution hazard estimates have been observed with unimodal shapes. An extension to the Gompertz distribution is presented which can capture unimodal as well as monotone hazard shapes. Important properties of the proposed distribution are discussed, and the proposed distribution is used to analyze survival data from a breast cancer clinical trial.},
  keywords = {62N01,Competing risks,cumulative incidence function,improper distribution,parametric modeling,subdistribution hazards,survival analysis},
  file = {C:\Users\efbonneville\Zotero\storage\GN6TA277\Haile et al. - 2016 - A 3-parameter Gompertz distribution for survival d.pdf}
}

@phdthesis{hallerAnalysisCompetingRisks2014,
  type = {{Text.PhDThesis}},
  title = {{The analysis of competing risks data with a focus on estimation of cause-specific and subdistribution hazard ratios from a mixture model}},
  author = {Haller, Bernhard},
  year = {2014},
  month = may,
  issn = {1917-0319},
  urldate = {2021-05-07},
  abstract = {Treatment efficacy in clinical trials is often assessed by time from treatment initiation to occurrence of a certain critical or beneficial event. In most cases the event of interest cannot be observed for all patients, as patients are only followed for a limited time or contact to patients is lost during their follow-up time. Therefore, certain methods were developed in the framework of the so called time-to-event or survival analysis, in order to obtain valid and consistent estimates in the presence of these "censored observations", using all available information. In classical event time analysis only one endpoint exists, as the death of a patient. As patients can die from different causes, in some clinical trials time to one out of two or more mutually exclusive types of event may be of interest. In many oncological studies, for example, time to cancer-specific death is considered as primary endpoint with deaths from other causes acting as so called competing risks. Different methods for data analysis in the competing risks framework were developed in recent years, which either focus on modelling the cause-specific or the subdistribution hazard rate or split the joint distribution of event times and event types into quantities, that can be estimated from observable data. In this work the analysis of event time data in the presence of competing risks is described, including the presentation and discussion of different regression approaches. A major topic of this work is the estimation of cause-specific and subdistribution hazard rates from a mixture model and a new approach using penalized B-splines (P-splines) for estimation of conditional hazard rates in a mixture model is proposed. In order to evaluate the behaviour of the new approach, a simulation study was conducted, using simulation techniques for competing risks data, which are described in detail in this work. The presented regression models were applied to data from a clinical cohort study investigating a risk stratification for cardiac mortality in patients, that survived a myocardial infarction. Finally, the use of the presented methods for event time analysis in the presence of competing risks and results obtained from the simulation study and the data analysis are discussed.},
  langid = {ngerman},
  school = {Ludwig-Maximilians-Universit{\"a}t M{\"u}nchen},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\H6V2B6NT\\Haller - 2014 - The analysis of competing risks data with a focus .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\P9XJN2N7\\17031.html}
}

@article{hallerFlexibleSimulationCompeting2014,
  title = {Flexible Simulation of Competing Risks Data Following Prespecified Subdistribution Hazards},
  author = {Haller, Bernhard and Ulm, Kurt},
  year = {2014},
  month = dec,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {84},
  number = {12},
  pages = {2557--2576},
  publisher = {Taylor \& Francis},
  issn = {0094-9655},
  doi = {10.1080/00949655.2013.793345},
  urldate = {2021-05-07},
  abstract = {In recent years different approaches for the analysis of time-to-event data in the presence of competing risks, i.e. when subjects can fail from one of two or more mutually exclusive types of event, were introduced. Different approaches for the analysis of competing risks data, focusing either on cause-specific or subdistribution hazard rates, were presented in statistical literature. Many new approaches use complicated weighting techniques or resampling methods, not allowing an analytical evaluation of these methods. Simulation studies often replace analytical comparisons, since they can be performed more easily and allow investigation of non-standard scenarios. For adequate simulation studies the generation of appropriate random numbers is essential. We present an approach to generate competing risks data following flexible prespecified subdistribution hazards. Event times and types are simulated using possibly time-dependent cause-specific hazards, chosen in a way that the generated data will follow the desired subdistribution hazards or hazard ratios, respectively.},
  keywords = {binomial algorithm,competing risks,event-time analysis,simulation,subdistribution hazard}
}

@article{hanMultipleImputationCompeting2018,
  title = {Multiple Imputation for Competing Risks Survival Data via Pseudo-Observations},
  author = {Han, Seungbong and Andrei, Adin-Cristian and Tsui, Kam-Wah},
  year = {2018},
  month = jul,
  journal = {Communications for Statistical Applications and Methods},
  volume = {25},
  number = {4},
  pages = {385--396},
  publisher = {Korean Statistical Society},
  issn = {2287-7843},
  doi = {10.29220/CSAM.2018.25.4.385},
  urldate = {2023-03-01},
  abstract = {Seungbong Han, Adin-Cristian Andrei, and Kam-Wah Tsui. CSAM 2018;25:385-96. https://doi.org/10.29220/CSAM.2018.25.4.385},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\JLLR4HKS\Han et al. - 2018 - Multiple imputation for competing risks survival d.pdf}
}

@article{hanSecondaryCytogeneticAbnormalities2021,
  title = {Secondary Cytogenetic Abnormalities in Core-Binding Factor {{AML}} Harboring Inv(16) vs t(8;21)},
  author = {Han, S. Y. and {Mrozek K.} and {Voutsinas J.} and {Wu Q.} and {Morgan E.A.} and {Vestergaard H.} and {Ohgami R.} and {Kluin P.M.} and {Kristensen T.K.} and {Pullarkat S.} and {Moller M.B.} and {Schiefer A.-I.} and {Baughn L.B.} and {Kim Y.} and {Czuchlewski D.} and {Hilberink J.R.} and {Horny H.-P.} and {George T.I.} and {Dolan M.} and {Ku N.K.} and {Yi C.A.} and {Pullarkat V.} and {Kohlschmidt J.} and {Salhotra A.} and {Soma L.} and {Bloomfield C.D.} and {Chen D.} and {Sperr W.R.} and {Marcucci G.} and {Cho C.} and {Akin C.} and {Gotlib J.} and {Broesby-Olsen S.} and {Larson M.} and {Linden M.A.} and {Deeg H.J.} and {Hoermann G.} and {Perales M.-A.} and {Hornick J.L.} and {Litzow M.R.} and {Nakamura R.} and {Weisdorf D.} and {Borthakur G.} and {Huls G.} and {Valent P.} and {Ustun C.} and {Yeung C.C.S.}},
  year = {2021},
  journal = {Blood Advances},
  volume = {5},
  number = {10},
  pages = {2481--2489},
  publisher = {American Society of Hematology},
  address = {United States},
  issn = {2473-9529},
  doi = {10.1182/BLOODADVANCES.2020003605},
  abstract = {Patients with core-binding factor (CBF) acute myeloid leukemia (AML), caused by either t(8; 21)(q22;q22) or inv(16)(p13q22)/t(16;16)(p13;q22), have higher complete remission rates and longer survival than patients with other subtypes of AML. However,;40\% of patients relapse, and the literature suggests that patients with inv(16) fare differently from those with t(8;21). We retrospectively analyzed 537 patients with CBF-AML, focusing on additional cytogenetic aberrations to examine their impact on clinical outcomes. Trisomies of chromosomes 8, 21, or 22 were significantly more common in patients with inv(16)/t(16;16): 16\% vs 7\%, 6\% vs 0\%, and 17\% vs 0\%, respectively. In contrast, del(9q) and loss of a sex chromosome were more frequent in patients with t(8;21): 15\% vs 0.4\% for del(9q), 37\% vs 0\% for loss of X in females, and 44\% vs 5\% for loss of Y in males. Hyperdiploidy was more frequent in patients with inv(16) (25\% vs 9\%, whereas hypodiploidy was more frequent in patients with t(8;21) (37\% vs 3\%. In multivariable analyses (adjusted for age, white blood counts at diagnosis, and KIT mutation status), trisomy 8 was associated with improved overall survival (OS) in inv(16), whereas the presence of other chromosomal abnormalities (not trisomy 8) was associated with decreased OS. In patients with t(8;21), hypodiploidy was associated with improved disease-free survival; hyperdiploidy and del(9q) were associated with improved OS. KIT mutation (either positive or not tested, compared with negative) conferred poor prognoses in univariate analysis only in patients with t(8;21).Copyright {\copyright} 2021 by The American Society of Hematology},
  langid = {english},
  keywords = {*acute myeloid leukemia/dt [Drug Therapy],*acute myeloid leukemia/th [Therapy],*chromosome aberration,*core binding factor/ec [Endogenous Compound],adolescent,adult,age,aged,allogeneic hematopoietic stem cell transplantation,antineoplastic agent/dt [Drug Therapy],article,autologous hematopoietic stem cell transplantation,CD135 antigen/ec [Endogenous Compound],child,controlled study,diploidy,disease free survival,female,flt3 gene,gene,gene mutation,human,hyperdiploidy,hypodiploidy,induction chemotherapy,KIT gene,leukocyte count,major clinical study,male,middle aged,NPM1 gene,nucleophosmin/ec [Endogenous Compound],overall survival,pseudodiploidy,remission,retrospective study,survival rate,trisomy,trisomy 13,trisomy 21,trisomy 22,trisomy 4,trisomy 8},
  file = {C:\Users\efbonneville\Zotero\storage\SS83ZYVA\Han S.Y. et al. - 2021 - Secondary cytogenetic abnormalities in core-bindin.pdf}
}

@article{hansenELN2017Genetic2021,
  title = {{{ELN}} 2017 {{Genetic Risk Stratification Predicts Survival}} of {{Acute Myeloid Leukemia Patients Receiving Allogeneic Hematopoietic Stem Cell Transplantation}}},
  author = {Hansen, D.K. and {Kim J.} and {Thompson Z.} and {Hussaini M.} and {Nishihori T.} and {Ahmad A.} and {Elmariah H.} and {Faramand R.} and {Mishra A.} and {Davila M.L.} and {Khimani F.} and {Lazaryan A.} and {Sallman D.} and {Liu H.} and {Perez L.E.} and {Fernandez H.} and {Nieder M.L.} and {Lancet J.E.} and {Pidala J.A.} and {Anasetti C.} and {Bejanyan N.}},
  year = {2021},
  journal = {Transplantation and Cellular Therapy},
  volume = {27},
  number = {3},
  pages = {256.e1-256.e7},
  publisher = {Elsevier B.V.},
  address = {Netherlands},
  issn = {2666-6367},
  doi = {10.1016/j.jtct.2020.12.021},
  abstract = {European LeukemiaNet (ELN) 2017 risk stratification by genetics is prognostic of outcomes in patients with acute myeloid leukemia (AML). However, the prognostic impact of the 2017 ELN genetic risk stratification after allogeneic hematopoietic cell transplantation (alloHCT) is not well established. We examined the effect of 2017 ELN genetic risk stratification on alloHCT outcomes of AML. We included 500 adult ({$>$}=18 years) AML patients in first (n = 370) or second (n = 130) complete remission receiving alloHCT from 2005 to 2016. Patients were classified into favorable (12\%), intermediate (57\%), and adverse (32\%) 2017 ELN risk groups. The Cox proportional hazard model was used to conduct the multivariable analyses of leukemia-free survival (LFS) and overall survival (OS). Relapse and nonrelapse mortality were analyzed by the Fine-Gray regression model. OS at 2 years was 72\% in the favorable versus 60\% in the intermediate versus 45\% in the adverse risk groups (P {$<$}.001). In multivariable analyses, the 2017 ELN classifier was an independent predictor of OS after alloHCT with significantly higher overall mortality in the intermediate (hazard ratio [HR] = 1.68, 95\% confidence interval [CI], 1.06-2.68; P =.03) and adverse (HR = 2.50, 95\% CI, 1.54-4.06; P {$<$}.001) risk groups compared to the favorable risk group. Similarly, LFS was worse in the intermediate (HR = 1.63, 95\%, CI 1.06-2.53; P =.03) and adverse (HR 2.23, 95\% CI, 1.41-3.54; P {$<$}.001) risk groups while relapse was higher in the adverse risk group (HR = 2.36, 95\% CI, 1.28-4.35; P =.006) as compared to the favorable risk group. These data highlight the prognostic impact of the 2017 ELN genetic risk stratification on the survival of AML patients after alloHCT. Patients in the adverse risk group had the highest risk of relapse and worst survival. Thus the 2017 ELN prognostic system can help identify AML patients who may benefit from clinical trials offering relapse mitigation strategies to improve transplant outcomes.Copyright {\copyright} 2020 The American Society for Transplantation and Cellular Therapy},
  langid = {english},
  keywords = {*acute myeloid leukemia/dt [Drug Therapy],*acute myeloid leukemia/rt [Radiotherapy],*acute myeloid leukemia/th [Therapy],*allogeneic hematopoietic stem cell transplantation,*cancer prognosis,*genetic risk,adult,aged,article,busulfan/dt [Drug Therapy],busulfan/iv [Intravenous Drug Administration],cancer radiotherapy,cancer specific survival,clinical outcome,cohort analysis,controlled study,cyclophosphamide/dt [Drug Therapy],female,fludarabine/dt [Drug Therapy],fludarabine/iv [Intravenous Drug Administration],graft versus host reaction/dt [Drug Therapy],graft versus host reaction/pc [Prevention],high risk population,human,leukemia relapse,leukemia remission,major clinical study,male,melphalan/dt [Drug Therapy],minimal residual disease,mortality,myeloablative conditioning,overall survival,rapamycin/cb [Drug Combination],rapamycin/dt [Drug Therapy],single drug dose,tacrolimus/cb [Drug Combination],tacrolimus/dt [Drug Therapy],thymocyte antibody,whole body radiation},
  file = {C:\Users\efbonneville\Zotero\storage\SMQEY68J\Hansen D.K. et al. - 2021 - ELN 2017 Genetic Risk Stratification Predicts Surv.pdf}
}

@article{hartleyAnalysisIncompleteData1971,
  title = {The {{Analysis}} of {{Incomplete Data}}},
  author = {Hartley, H. O. and Hocking, R. R.},
  year = {1971},
  journal = {Biometrics},
  volume = {27},
  number = {4},
  eprint = {2528820},
  eprinttype = {jstor},
  pages = {783--823},
  publisher = {[Wiley, International Biometric Society]},
  issn = {0006-341X},
  doi = {10.2307/2528820},
  urldate = {2024-06-19},
  abstract = {In this paper, we attempt to provide a simple taxonomy for incomplete-data problems and at the same time develop unified methods of analysis. The emphasis is on techniques which are natural extensions of the complete-data analysis and which will handle rather general classes of incomplete-data problems as opposed to custom-made techniques for special problems. The principle of estimation is either maximum likelihood or is at least based on maximum likelihood.},
  file = {C:\Users\efbonneville\Zotero\storage\QLV94S64\Hartley and Hocking - 1971 - The Analysis of Incomplete Data.pdf}
}

@article{hassanCMVReactivationInitiates2022,
  title = {{{CMV}} Reactivation Initiates Long-Term Expansion and Differentiation of the {{NK}} Cell Repertoire},
  author = {Hassan, Norfarazieda and Eldershaw, Suzy and Stephens, Christine and Kinsella, Francesca and Craddock, Charles and Malladi, Ram and Zuo, Jianmin and Moss, Paul},
  year = {2022},
  journal = {Frontiers in Immunology},
  volume = {13},
  issn = {1664-3224},
  urldate = {2023-11-04},
  abstract = {IntroductionNK cells play an important role in suppression of viral replication and are critical for effective control of persistent infections such as herpesviruses. Cytomegalovirus infection is associated with expansion of `adaptive-memory' NK cells with a characteristic CD56dimCD16bright NKG2C+ phenotype but the mechanisms by which this population is maintained remain uncertain.MethodsWe studied NK cell reconstitution in patients undergoing haemopoietic stem cell transplantation and related this to CMV reactivation.ResultsNK cells expanded in the early post-transplant period but then remained stable in the absence of viral reactivation. However, CMV reactivation led to a rapid and sustained 10-fold increase in NK cell number. The proportion of NKG2C-expressing cells increases on all NK subsets although the kinetics of expansion peaked at 6 months on immature CD56bright cells whilst continuing to rise on the mature CD56dim pool. Phenotypic maturation was observed by acquisition of CD57 expression. Effective control of viral reactivation was seen when the peripheral NK cell count reached 20,000/ml.DiscussionThese data show that short term CMV reactivation acts to reprogramme hemopoiesis to drive a sustained modulation and expansion of the NK cell pool and reveal further insight into long term regulation of the innate immune repertoire by infectious challenge.},
  file = {C:\Users\efbonneville\Zotero\storage\IL338GUR\Hassan et al. - 2022 - CMV reactivation initiates long-term expansion and.pdf}
}

@article{hayatirezvanRiseMultipleImputation2015,
  title = {The Rise of Multiple Imputation: A Review of the Reporting and Implementation of the Method in Medical Research},
  shorttitle = {The Rise of Multiple Imputation},
  author = {Hayati Rezvan, Panteha and Lee, Katherine J. and Simpson, Julie A.},
  year = {2015},
  month = apr,
  journal = {BMC Medical Research Methodology},
  volume = {15},
  number = {1},
  pages = {30},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0022-1},
  urldate = {2022-10-10},
  abstract = {Missing data are common in medical research, which can lead to a loss in statistical power and potentially biased results if not handled appropriately. Multiple imputation (MI) is a statistical method, widely adopted in practice, for dealing with missing data. Many academic journals now emphasise the importance of reporting information regarding missing data and proposed guidelines for documenting the application of MI have been published. This review evaluated the reporting of missing data, the application of MI including the details provided regarding the imputation model, and the frequency of sensitivity analyses within the MI framework in medical research articles.},
  keywords = {Missing data,Multiple imputation,Reporting,Sensitivity analysis},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\REBZEX7D\\Hayati Rezvan et al. - 2015 - The rise of multiple imputation a review of the r.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\PUJ8TAYX\\s12874-015-0022-1.html}
}

@article{heitjanDistinguishingMissingRandom1996,
  title = {Distinguishing "{{Missing}} at {{Random}}" and "{{Missing Completely}} at {{Random}}"},
  author = {Heitjan, Daniel F. and Basu, Srabashi},
  year = {1996},
  journal = {The American Statistician},
  volume = {50},
  number = {3},
  eprint = {2684656},
  eprinttype = {jstor},
  pages = {207--213},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0003-1305},
  doi = {10.2307/2684656},
  urldate = {2024-12-11},
  abstract = {Missing at random (MAR) and missing completely at random (CAR) are ignobility conditions-when they hold, they guarantee that certain kinds of inferences may be made without recourse to complicated missing-data modeling. In this article we review the definitions of MAR, CAR, and their recent generalizations. We apply the definitions in three common incomplete-data examples, demonstrating by simulation the consequences of departures from ignorability. We argue that practitioners who face potentially nonignorable incomplete data must consider both the mode of inference and the nature of the conditioning when deciding which ignorability condition to invoke.},
  file = {C:\Users\efbonneville\Zotero\storage\ZSJ3FW45\Heitjan and Basu - 1996 - Distinguishing Missing at Random and Missing Co.pdf}
}

@article{heitjanIgnorabilityCoarseData1991,
  title = {Ignorability and {{Coarse Data}}},
  author = {Heitjan, Daniel F. and Rubin, Donald B.},
  year = {1991},
  month = dec,
  journal = {The Annals of Statistics},
  volume = {19},
  number = {4},
  pages = {2244--2253},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176348396},
  urldate = {2024-08-22},
  abstract = {We present a general statistical model for data coarsening, which includes as special cases rounded, heaped, censored, partially categorized and missing data. Formally, with coarse data, observations are made not in the sample space of the random variable of interest, but rather in its power set. Grouping is a special case in which the degree of coarsening is known and nonstochastic. We establish simple conditions under which the possible stochastic nature of the coarsening mechanism can be ignored when drawing Bayesian and likelihood inferences and thus the data can be validly treated as grouped data. The conditions are that the data be coarsened at random, a generalization of the condition missing at random, and that the parameters of the data and the coarsening process be distinct. Applications of the general model and the ignorability condition are illustrated in a numerical example and described briefly in a variety of special cases.},
  keywords = {62A10,62A15,62F99,Censord data,coarsened at random,grouped data,heaped data,interval-censored data,missing at random,missing data,rounded data},
  file = {C:\Users\efbonneville\Zotero\storage\DRXYQLWH\Heitjan and Rubin - 1991 - Ignorability and Coarse Data.pdf}
}

@article{heitjanIgnorabilityCoarseData1993,
  title = {Ignorability and {{Coarse Data}}: {{Some Biomedical Examples}}},
  shorttitle = {Ignorability and {{Coarse Data}}},
  author = {Heitjan, Daniel F.},
  year = {1993},
  journal = {Biometrics},
  volume = {49},
  number = {4},
  eprint = {2532251},
  eprinttype = {jstor},
  pages = {1099--1109},
  publisher = {International Biometric Society},
  issn = {0006-341X},
  doi = {10.2307/2532251},
  urldate = {2024-12-10},
  abstract = {Heitjan and Rubin (1991, Annals of Statistics 19, 2244-2253) define data to be "coarse" when one observes not the exact value of the data but only some set (a subset of the sample space) that contains the exact value. This definition covers a number of incomplete-data problems arising in biomedicine, including rounded, heaped, censored, and missing data. In analyzing coarse data, it is common to proceed as though the degree of coarseness is fixed in advance-in a word, to ignore the randomness in the coarsening mechanism. When coarsening is actually stochastic, however, inferences that ignore this randomness may be seriously misleading. Heitjan and Rubin (1991) have proposed a general model of data coarsening and established conditions under which it is appropriate to ignore the stochastic nature of the coarsening. The conditions are that the data be coarsened at random [a generalization of missing at random (Rubin, 1976, Biometrika 63, 581-592)] and that the parameters of the data and the coarsening process be distinct. This article presents detailed applications of the general model and the ignorability conditions to a variety of coarse-data problems arising in biomedical statistics. A reanalysis of the Stanford Heart Transplant Data (Crowley and Hu, 1977, Journal of the American Statistical Association 72, 27-36) reveals significant evidence that censoring of pretransplant survival times by transplantation was nonignorable, suggesting a greater benefit from cardiac transplantation than previous analyses had found.},
  file = {C:\Users\efbonneville\Zotero\storage\KACK6KHH\Heitjan - 1993 - Ignorability and Coarse Data Some Biomedical Exam.pdf}
}

@article{hickeyComparisonJointModels2018a,
  title = {A {{Comparison}} of {{Joint Models}} for {{Longitudinal}} and {{Competing Risks Data}}, with {{Application}} to an {{Epilepsy Drug Randomized Controlled Trial}}},
  author = {Hickey, Graeme L. and Philipson, Pete and Jorgensen, Andrea and {Kolamunnage-Dona}, Ruwanthi},
  year = {2018},
  month = oct,
  journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
  volume = {181},
  number = {4},
  pages = {1105--1123},
  issn = {0964-1998},
  doi = {10.1111/rssa.12348},
  urldate = {2024-06-25},
  abstract = {Joint modelling of longitudinal data and competing risks has grown over the past decade. Despite the recent methodological developments, there are still limited options for fitting these models in standard statistical software programs, which prohibits their adoption by applied biostatisticians. We summarize four published models, each of which has software available for model estimation. Each model features a different hazard function, latent association structure between the submodels, estimation approach and software implementation. Of the four models considered here, the model specifications and association structures are substantially different, thus complicating model-to-model comparison. The models are applied to the `Standard and new anti-epileptic drugs' trial of anti-epileptic drugs to investigate the effect of drug titration on the treatment effects of lamotrigine and carbamazepine on the mode of treatment failure. Notwithstanding the vastly different association structures, we show that the inference from each model is consistent, namely, that there is a beneficial effect of lamotrigine on unacceptable adverse events over carbamazepine and a non-significant effect on the hazard of inadequate seizure control. The association between anti-epileptic drug titration and treatment failure was significant in most models. To allow for the routine adoption of joint modelling of competing risks and longitudinal data in the analysis of clinical data sets, further work is required on the development of model diagnostics to aid model choice.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\7DTHCDRR\\Hickey et al. - 2018 - A Comparison of Joint Models for Longitudinal and .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\UDNSRNJB\\7072031.html}
}

@article{hickeyJointModellingTimetoevent2016,
  title = {Joint Modelling of Time-to-Event and Multivariate Longitudinal Outcomes: Recent Developments and Issues},
  shorttitle = {Joint Modelling of Time-to-Event and Multivariate Longitudinal Outcomes},
  author = {Hickey, Graeme L. and Philipson, Pete and Jorgensen, Andrea and {Kolamunnage-Dona}, Ruwanthi},
  year = {2016},
  month = sep,
  journal = {BMC Medical Research Methodology},
  volume = {16},
  number = {1},
  pages = {117},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0212-5},
  urldate = {2024-08-28},
  abstract = {Available methods for the joint modelling of longitudinal and time-to-event outcomes have typically only allowed for a single longitudinal outcome and a solitary event time. In practice, clinical studies are likely to record multiple longitudinal outcomes. Incorporating all sources of data will improve the predictive capability of any model and lead to more informative inferences for the purpose of medical decision-making.},
  langid = {english},
  keywords = {Joint models,Longitudinal data,Multivariate data,Software,Time-to-event data},
  file = {C:\Users\efbonneville\Zotero\storage\9N7GNHQ2\Hickey et al. - 2016 - Joint modelling of time-to-event and multivariate .pdf}
}

@article{hinchliffeFlexibleParametricModelling2013,
  title = {Flexible Parametric Modelling of Cause-Specific Hazards to Estimate Cumulative Incidence Functions},
  author = {Hinchliffe, Sally R. and Lambert, Paul C.},
  year = {2013},
  month = feb,
  journal = {BMC Medical Research Methodology},
  volume = {13},
  number = {1},
  pages = {13},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-13-13},
  urldate = {2024-03-04},
  abstract = {Competing risks are a common occurrence in survival analysis. They arise when a patient is at risk of more than one mutually exclusive event, such as death from different causes, and the occurrence of one of these may prevent any other event from ever happening.},
  langid = {english},
  keywords = {Cause-specific hazards,Competing risks,Flexible parametric model},
  file = {C:\Users\efbonneville\Zotero\storage\38ATLR7A\Hinchliffe and Lambert - 2013 - Flexible parametric modelling of cause-specific ha.pdf}
}

@article{hongAccuracyRandomforestbasedImputation2020,
  title = {Accuracy of Random-Forest-Based Imputation of Missing Data in the Presence of Non-Normality, Non-Linearity, and Interaction},
  author = {Hong, Shangzhi and Lynn, Henry S.},
  year = {2020},
  month = jul,
  journal = {BMC Medical Research Methodology},
  volume = {20},
  number = {1},
  pages = {199},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-01080-1},
  urldate = {2024-12-10},
  abstract = {Missing data are common in statistical analyses, and imputation methods based on random forests (RF) are becoming popular for handling missing data especially in biomedical research. Unlike standard imputation approaches, RF-based imputation methods do not assume normality or require specification of parametric models. However, it is still inconclusive how they perform for non-normally distributed data or when there are non-linear relationships or interactions.},
  langid = {english},
  keywords = {Imputation accuracy,Missing data imputation,Random forest},
  file = {C:\Users\efbonneville\Zotero\storage\SGIB9CQA\Hong and Lynn - 2020 - Accuracy of random-forest-based imputation of miss.pdf}
}

@article{horowitzGraftversusleukemiaReactionsBone1990,
  title = {Graft-versus-Leukemia Reactions after Bone Marrow Transplantation},
  author = {Horowitz, {\relax MM} and Gale, {\relax RP} and Sondel, {\relax PM} and Goldman, {\relax JM} and Kersey, J and Kolb, {\relax HJ} and Rimm, {\relax AA} and Ringden, O and Rozman, C and Speck, B},
  year = {1990},
  month = feb,
  journal = {Blood},
  volume = {75},
  number = {3},
  pages = {555--562},
  issn = {0006-4971},
  doi = {10.1182/blood.V75.3.555.555},
  urldate = {2023-11-04},
  abstract = {To determine whether graft-versus-leukemia (GVL) reactions are important in preventing leukemia recurrence after bone marrow transplantation, we studied 2,254 persons receiving HLA-identical sibling bone marrow transplants for acute myelogenous leukemia (AML) in first remission, acute lymphoblastic leukemia (ALL) in first remission, and chronic myelogenous leukemia (CML) in first chronic phase. Four groups were investigated in detail: recipients of non--T-cell depleted allografts without graft-versus-host disease (GVHD), recipients of non-- T-cell depleted allografts with GVHD, recipients of T-cell depleted allografts, and recipients of genetically identical twin transplants. Decreased relapse was observed in recipients of non--T-cell depleted allografts with acute (relative risk 0.68, P = .03), chronic (relative risk 0.43, P = .01), and both acute and chronic GVDH (relative risk 0.33, P = .0001) as compared with recipients of non--T-cell depleted allografts without GVHD. These data support an antileukemia effect of GVHD. AML patients who received identical twin transplants had an increased probability of relapse (relative risk 2.58, P = .008) compared with allograft recipients without GVHD. These data support an antileukemia effect of allogeneic grafts independent of GVHD. CML patients who received T-cell depleted transplants with or without GVHD had higher probabilities of relapse (relative risks 4.45 and 6.91, respectively, P = .0001) than recipients of non--T-cell depleted allografts without GVHD. These data support an antileukemia effect independent of GVHD that is altered by T-cell depletion. These results explain the efficacy of allogeneic bone marrow transplantation in eradicating leukemia, provide evidence for a role of the immune system in controlling human cancers, and suggest future directions to improve leukemia therapy.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\2LLM4Z93\\Horowitz et al. - 1990 - Graft-versus-leukemia reactions after bone marrow .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\75LW5XXP\\Graft-versus-leukemia-reactions-after-bone-marrow.html}
}

@article{horowitzRoleRegistriesFacilitating2008,
  title = {The Role of Registries in Facilitating Clinical Research in {{BMT}}: Examples from the {{Center}} for {{International Blood}} and {{Marrow Transplant Research}}},
  shorttitle = {The Role of Registries in Facilitating Clinical Research in {{BMT}}},
  author = {Horowitz, M. M.},
  year = {2008},
  month = aug,
  journal = {Bone Marrow Transplantation},
  volume = {42},
  number = {1},
  pages = {S1-S2},
  publisher = {Nature Publishing Group},
  issn = {1476-5365},
  doi = {10.1038/bmt.2008.101},
  urldate = {2024-07-01},
  abstract = {Observational databases, such as those maintained by the Center for International Blood and Marrow Transplant Research (CIBMTR) and the European Blood and Marrow Transplant Group (EBMT), play an important role in facilitating research into hematopoietic SCT (HCT) outcomes. The CIBMTR maintains a large database of the outcome of BMTs performed in 450 centers in 47 countries, including information on 240\,000 transplant recipients and adding information on about 14\,000 new transplants per year. The database has data for 9000 survivors followed for 10 or more years. The database may facilitate the understanding of outcomes by addressing questions difficult to answer through clinical trials. Clinical databases may also aid the development of optimal designs for prospective clinical trials. Use of the CIBMTR database for trial design and monitoring is an integral part of the US Blood and Marrow Transplant Clinical Trials Network (BMT CTN). The establishment of international outcomes registries was an important component of advances in HCT over the past three decades. Future progress will be further enhanced by inter-registry collaboration through the Worldwide Blood and Marrow Transplant Group (WBMT).},
  copyright = {2008 Macmillan Publishers Limited},
  langid = {english},
  keywords = {Cell Biology,general,Hematology,Internal Medicine,Medicine/Public Health,Public Health,Stem Cells},
  file = {C:\Users\efbonneville\Zotero\storage\CSUFFQL6\Horowitz - 2008 - The role of registries in facilitating clinical re.pdf}
}

@article{huangDynamicPredictionRelapse2021,
  title = {Dynamic Prediction of Relapse in Patients with Acute Leukemias after Allogeneic Transplantation: {{Joint}} Model for Minimal Residual Disease},
  shorttitle = {Dynamic Prediction of Relapse in Patients with Acute Leukemias after Allogeneic Transplantation},
  author = {Huang, Aijie and Chen, Qi and Fei, Yang and Wang, Ziwei and Ni, Xiong and Gao, Lei and Chen, Li and Chen, Jie and Zhang, Weiping and Yang, Jianmin and Wang, Jianmin and Hu, Xiaoxia},
  year = {2021},
  journal = {International Journal of Laboratory Hematology},
  volume = {43},
  number = {1},
  pages = {84--92},
  issn = {1751-553X},
  doi = {10.1111/ijlh.13328},
  urldate = {2024-10-04},
  abstract = {Introduction Relapse remains the leading cause of treatment failure after allogeneic hematopoietic stem cell transplantation (alloHSCT) in leukemia. Numerous investigations have demonstrated that minimal residual disease (MRD) before or after alloHSCT is prognostic of relapse risk. These MRD data were collected at specific checkpoints and could not dynamically predict the relapse risk after alloHSCT, which needs serial monitoring. Methods In the present study, we retrospectively analyzed MRD measured with multi-parameter flow cytometry in 207 acute myeloid leukemia (AML) patients (acute promyelocytic leukemia excluded), and 124 acute B lymphoblastic leukemia (ALL) patients. A three-step method based on joint model was used to build a relapse risk prediction model. Results The 3-year overall survival and relapse-free survival rates of the entire cohort were 67.1\% {\textpm} 2.8\% and 61.6\% {\textpm} 2.8\%, respectively. The model included disease status before alloHSCT, acute and chronic graft-versus-host disease, and serial MRD data. The time-dependent receiver operating characteristics was used to evaluate the ability of the model. It fitted well with actual incidence of relapse. The serial MRD data collected after alloHSCT had better discrimination capabilities for recurrence prediction with the area under the curve from 0.67 to 0.91 (AML: 0.66-0.89; ALL: 0.70-0.96). Conclusion The joint model was able to dynamically predict relapse-free probability after alloHSCT, which would be a useful tool to provide important information to guide decision-making in the clinic and facilitate the individualized therapy.},
  copyright = {{\copyright} 2020 John Wiley \& Sons Ltd},
  langid = {english},
  keywords = {allogeneic hematopoietic stem cell transplantation,flow cytometry,joint model,minimal residual disease,relapse},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\7BCSAWBZ\\Huang et al. - 2021 - Dynamic prediction of relapse in patients with acu.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\Q54J6IVQ\\ijlh.html}
}

@article{hughesAccountingMissingData2019,
  title = {Accounting for Missing Data in Statistical Analyses: Multiple Imputation Is Not Always the Answer},
  shorttitle = {Accounting for Missing Data in Statistical Analyses},
  author = {Hughes, Rachael A and Heron, Jon and Sterne, Jonathan A C and Tilling, Kate},
  year = {2019},
  month = aug,
  journal = {International Journal of Epidemiology},
  volume = {48},
  number = {4},
  pages = {1294--1304},
  issn = {0300-5771},
  doi = {10.1093/ije/dyz032},
  urldate = {2022-10-10},
  abstract = {Missing data are unavoidable in epidemiological research, potentially leading to bias and loss of precision. Multiple imputation (MI) is widely advocated as an improvement over complete case analysis (CCA). However, contrary to widespread belief, CCA is preferable to MI in some situations.We provide guidance on choice of analysis when data are incomplete. Using causal diagrams to depict missingness mechanisms, we describe when CCA will not be biased by missing data and compare MI and CCA, with respect to bias and efficiency, in a range of missing data situations. We illustrate selection of an appropriate method in practice.For most regression models, CCA gives unbiased results when the chance of being a complete case does not depend on the outcome after taking the covariates into consideration, which includes situations where data are missing not at random. Consequently, there are situations in which CCA analyses are unbiased while MI analyses, assuming missing at random (MAR), are biased. By contrast MI, unlike CCA, is valid for all MAR situations and has the potential to use information contained in the incomplete cases and auxiliary variables to reduce bias and/or improve precision. For this reason, MI was preferred over CCA in our real data example.Choice of method for dealing with missing data is crucial for validity of conclusions, and should be based on careful consideration of the reasons for the missing data, missing data patterns and the availability of auxiliary information.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\TQTGNG94\\Hughes et al. - 2019 - Accounting for missing data in statistical analyse.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\J9GZ8QUP\\5382162.html}
}

@article{inoueImpactConditioningIntensity2021,
  title = {Impact of Conditioning Intensity and Regimen on Transplant Outcomes in Patients with Adult {{T-cell}} Leukemia-Lymphoma},
  author = {Inoue, Y. and {Nakano N.} and {Fuji S.} and {Eto T.} and {Kawakita T.} and {Suehiro Y.} and {Miyamoto T.} and {Sawayama Y.} and {Uchida N.} and {Kondo T.} and {Kanda J.} and {Atsuta Y.} and {Fukuda T.} and {Yoshimitsu M.} and {Kato K.}},
  year = {2021},
  journal = {Bone Marrow Transplantation},
  volume = {56},
  number = {12},
  pages = {2964--2974},
  publisher = {Springer Nature},
  address = {United Kingdom},
  issn = {0268-3369},
  doi = {10.1038/s41409-021-01445-0},
  abstract = {In allogeneic hematopoietic cell transplantation (allo-HCT) for adult T-cell leukemia-lymphoma (ATL), the optimal conditioning regimens have not yet been determined. We conducted a Japanese nationwide, retrospective study to investigate this issue. This study included 914 ATL patients who underwent allo-HCT between 1995 and 2015. In patients aged 55 years or younger, there was no statistically significant difference between reduced-intensity conditioning (RIC) regimens and myeloablative conditioning (MAC) regimens regarding risk of relapse (vs. RIC group: MAC group, hazard ratio (HR) 0.76, P = 0.071), non-relapse mortality (vs. RIC group: MAC group, HR 1.38, P = 0.115), or overall mortality (vs. RIC group: MAC group, HR 1.17, P = 0.255). Among RIC regimens, fludarabine plus melphalan-based (Flu/Mel) regimens were associated with a lower risk of relapse (Flu/Mel140 group, HR 0.59, P {$<$} 0.001; Flu/Mel80 group, HR 0.79, P = 0.021) than the Flu plus busulfan-based regimen (Flu/Bu2 group). Meanwhile, Flu/Mel140 group had a significantly higher risk of non-relapse mortality (vs. Flu/Bu2 group: HR 1.53, P = 0.025). In conclusion, it is acceptable to select a RIC regimen for younger patients. Moreover, it might be beneficial to select a Flu/Mel-based regimen for patients at high risk of relapse.Copyright {\copyright} 2021, The Author(s), under exclusive licence to Springer Nature Limited.},
  langid = {english},
  keywords = {*adult T cell leukemia/th [Therapy],*allogeneic hematopoietic stem cell transplantation,*cancer patient,*relapse,*T cell leukemia,*transplantation conditioning,*treatment outcome,adult,aged,all cause mortality,allogeneic hematopoietic stem cell transplantation,article,busulfan,busulfan/iv [Intravenous Drug Administration],busulfan/po [Oral Drug Administration],cancer mortality,cancer recurrence,cancer risk,controlled study,cyclophosphamide,drug combination,drug therapy,female,fludarabine,groups by age,high risk population,human,influenza,Japan,Japanese (people),low risk population,major clinical study,male,melphalan,middle aged,multicenter study,myeloablative conditioning,reduced intensity conditioning,retrospective study,risk assessment,surgery},
  file = {C:\Users\efbonneville\Zotero\storage\UWSFRWVS\Inoue Y. et al. - 2021 - Impact of conditioning intensity and regimen on tr.pdf}
}

@article{itoImpactLowdoseAntithymocyte2020,
  title = {Impact of Low-Dose Anti-Thymocyte Globulin on Immune Reconstitution after Allogeneic Hematopoietic Cell Transplantation},
  author = {Ito, Ayumu and Kitano, Shigehisa and Tajima, Kinuko and Kim, Youngji and Tanaka, Takashi and Inamoto, Yoshihiro and Kim, Sung-Won and Yamamoto, Noboru and Fukuda, Takahiro and Okamoto, Shinichiro},
  year = {2020},
  month = jan,
  journal = {International Journal of Hematology},
  volume = {111},
  number = {1},
  pages = {120--130},
  issn = {1865-3774},
  doi = {10.1007/s12185-019-02756-1},
  urldate = {2023-11-04},
  abstract = {How low-dose anti-thymocyte globulin (ATG) for prophylaxis of graft-versus-host disease (GVHD) influences immune reconstitution after allogeneic hematopoietic stem cell transplantation (allo-HCT) remains incompletely understood. We prospectively enrolled 41 consecutive adult patients and conducted cytometry-based immunophenotyping for 12~months after allo-HCT. Rabbit ATG (Thymoglobulin) was administered at a median total dose of 1.75~mg/kg in 16 of the 41 patients. Compared with patients who did not receive ATG, those who did had a significantly smaller number of na{\"i}ve T cells (especially CD4+\,) within three months after allo-HCT. No significant difference was observed between the two groups in the reconstitution of other T cells (effector, memory, Th1, Th2, Th17, Treg, and Tfh), B cells (transitional, na{\"i}ve, memory, and plasmablast), NK cells (regulatory and cytolytic), or dendritic cells (myeloid and plasmacytoid). Patients with fewer CD4+  na{\"i}ve T cells than the median count (7.60 cells/{\textmu}L) at two months after allo-HCT developed chronic GVHD less frequently than those with CD4+ na{\"i}ve T cells above the median count (2-year cumulative incidences were 0.31 and 0.53, respectively; p\,=\,0.133). This pilot study suggests low-dose Thymoglobulin suppresses the recovery of na{\"i}ve T cells after allo-HCT, which may contribute to a lower incidence of chronic GVHD.},
  langid = {english},
  keywords = {Allogeneic hematopoietic stem cell transplantation (allo-HCT),Anti-thymocyte globulin (ATG),Graft-versus-host disease (GVHD),Immune reconstitution},
  file = {C:\Users\efbonneville\Zotero\storage\FIJ6WBF5\Ito et al. - 2020 - Impact of low-dose anti-thymocyte globulin on immu.pdf}
}

@article{jakobsenWhenHowShould2017,
  title = {When and How Should Multiple Imputation Be Used for Handling Missing Data in Randomised Clinical Trials -- a Practical Guide with Flowcharts},
  author = {Jakobsen, Janus Christian and Gluud, Christian and Wetterslev, J{\o}rn and Winkel, Per},
  year = {2017},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {17},
  number = {1},
  pages = {162},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0442-1},
  urldate = {2022-10-10},
  abstract = {Missing data may seriously compromise inferences from randomised clinical trials, especially if missing data are not handled appropriately. The potential bias due to missing data depends on the mechanism causing the data to be missing, and the analytical methods applied to amend the missingness. Therefore, the analysis of trial data with missing values requires careful planning and attention.},
  keywords = {Missing data,Multiple imputation,Randomised clinical trials},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\FVLSK9IY\\Jakobsen et al. - 2017 - When and how should multiple imputation be used fo.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\59WMVMXR\\s12874-017-0442-1.html}
}

@article{janvinCausalInferenceRecurrent2024,
  title = {Causal Inference with Recurrent and Competing Events},
  author = {Janvin, Matias and Young, Jessica G. and Ryalen, P{\aa}l C. and Stensrud, Mats J.},
  year = {2024},
  month = jan,
  journal = {Lifetime Data Analysis},
  volume = {30},
  number = {1},
  pages = {59--118},
  issn = {1572-9249},
  doi = {10.1007/s10985-023-09594-8},
  urldate = {2024-06-18},
  abstract = {Many research questions concern treatment effects on outcomes that can recur several times in the same individual. For example, medical researchers are interested in treatment effects on hospitalizations in heart failure patients and sports injuries in athletes. Competing events, such as death, complicate causal inference in studies of recurrent events because once a competing event occurs, an individual cannot have more recurrent events. Several statistical estimands have been studied in recurrent event settings, with and without competing events. However, the causal interpretations of these estimands, and the conditions that are required to identify these estimands from observed data, have yet to be formalized. Here we use a formal framework for causal inference to formulate several causal estimands in recurrent event settings, with and without competing events. When competing events exist, we clarify when commonly used classical statistical estimands can be interpreted as causal quantities from the causal mediation literature, such as (controlled) direct effects and total effects. Furthermore, we show that recent results on interventionist mediation estimands allow us to define new causal estimands with recurrent and competing events that may be of particular clinical relevance in many subject matter settings. We use causal directed acyclic graphs and single world intervention graphs to illustrate how to reason about identification conditions for the various causal estimands based on subject matter knowledge. Furthermore, using results on counting processes, we show that our causal estimands and their identification conditions, which are articulated in discrete time, converge to classical continuous time counterparts in the limit of fine discretizations of time. We propose estimators and establish their consistency for the various identifying functionals. Finally, we use the proposed estimators to compute the effect of blood pressure lowering treatment on the recurrence of acute kidney injury using data from the Systolic Blood Pressure Intervention Trial.},
  langid = {english},
  keywords = {Causal inference,Competing events,Event history analysis,Recurrent events,Separable effects},
  file = {C:\Users\efbonneville\Zotero\storage\8WTHA7UV\Janvin et al. - 2024 - Causal inference with recurrent and competing even.pdf}
}

@article{jeongDirectParametricInference2006,
  title = {Direct Parametric Inference for the Cumulative Incidence Function},
  author = {Jeong, Jong-Hyeon and Fine, Jason},
  year = {2006},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {55},
  number = {2},
  pages = {187--200},
  issn = {1467-9876},
  doi = {10.1111/j.1467-9876.2006.00532.x},
  urldate = {2021-05-10},
  abstract = {Summary. In survival data that are collected from phase III clinical trials on breast cancer, a patient may experience more than one event, including recurrence of the original cancer, new primary cancer and death. Radiation oncologists are often interested in comparing patterns of local or regional recurrences alone as first events to identify a subgroup of patients who need to be treated by radiation therapy after surgery. The cumulative incidence function provides estimates of the cumulative probability of locoregional recurrences in the presence of other competing events. A simple version of the Gompertz distribution is proposed to parameterize the cumulative incidence function directly. The model interpretation for the cumulative incidence function is more natural than it is with the usual cause-specific hazard parameterization. Maximum likelihood analysis is used to estimate simultaneously parametric models for cumulative incidence functions of all causes. The parametric cumulative incidence approach is applied to a data set from the National Surgical Adjuvant Breast and Bowel Project and compared with analyses that are based on parametric cause-specific hazard models and nonparametric cumulative incidence estimation.},
  langid = {english},
  keywords = {Breast cancer,Clinical trial,Competing risks,Cumulative incidence,Cure model,Improper distribution},
  file = {C:\Users\efbonneville\Zotero\storage\DBBGG3DV\Jeong and Fine - 2006 - Direct parametric inference for the cumulative inc.pdf}
}

@article{jeongParametricRegressionCumulative2007,
  title = {Parametric Regression on Cumulative Incidence Function},
  author = {Jeong, Jong-Hyeon and Fine, Jason P.},
  year = {2007},
  month = apr,
  journal = {Biostatistics},
  volume = {8},
  number = {2},
  pages = {184--196},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxj040},
  urldate = {2021-05-10},
  abstract = {We propose parametric regression analysis of cumulative incidence function with competing risks data. A simple form of Gompertz distribution is used for the improper baseline subdistribution of the event of interest. Maximum likelihood inferences on regression parameters and associated cumulative incidence function are developed for parametric models, including a flexible generalized odds rate model. Estimation of the long-term proportion of patients with cause-specific events is straightforward in the parametric setting. Simple goodness-of-fit tests are discussed for evaluating a fixed odds rate assumption. The parametric regression methods are compared with an existing semiparametric regression analysis on a breast cancer data set where the cumulative incidence of recurrence is of interest. The results demonstrate that the likelihood-based parametric analyses for the cumulative incidence function are a practically useful alternative to the semiparametric analyses.},
  file = {C:\Users\efbonneville\Zotero\storage\QH6CENH6\Jeong and Fine - 2007 - Parametric regression on cumulative incidence func.pdf}
}

@book{kalbfleisch2011statistical,
  title = {The Statistical Analysis of Failure Time Data},
  author = {Kalbfleisch, J.D. and Prentice, R.L.},
  year = {2011},
  series = {Wiley Series in Probability and Statistics},
  publisher = {Wiley},
  isbn = {978-1-118-03123-0},
  lccn = {2002068965}
}

@article{kantidakisStatisticalModelsMachine2023,
  title = {Statistical Models versus Machine Learning for Competing Risks: Development and Validation of Prognostic Models},
  shorttitle = {Statistical Models versus Machine Learning for Competing Risks},
  author = {Kantidakis, Georgios and Putter, Hein and Liti{\`e}re, Saskia and Fiocco, Marta},
  year = {2023},
  month = feb,
  journal = {BMC Medical Research Methodology},
  volume = {23},
  number = {1},
  pages = {51},
  issn = {1471-2288},
  doi = {10.1186/s12874-023-01866-z},
  urldate = {2023-10-06},
  abstract = {In health research, several chronic diseases are susceptible to competing risks (CRs). Initially, statistical models (SM) were developed to estimate the cumulative incidence of an event in the presence of CRs. As recently there is a growing interest in applying machine learning (ML) for clinical prediction, these techniques have also been extended to model CRs but literature is limited. Here, our aim is to investigate the potential role of ML versus SM for CRs within non-complex data (small/medium sample size, low dimensional setting).},
  langid = {english},
  keywords = {Artificial neural networks,Competing risks,Predictive performance,Random survival forests,Regression models,Supervised machine learning,Survival analysis},
  file = {C:\Users\efbonneville\Zotero\storage\4VIB8TGI\Kantidakis et al. - 2023 - Statistical models versus machine learning for com.pdf}
}

@article{kaplanNonparametricEstimationIncomplete1958,
  title = {Nonparametric {{Estimation}} from {{Incomplete Observations}}},
  author = {Kaplan, E. L. and Meier, Paul},
  year = {1958},
  journal = {Journal of the American Statistical Association},
  volume = {53},
  number = {282},
  eprint = {2281868},
  eprinttype = {jstor},
  pages = {457--481},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  doi = {10.2307/2281868},
  urldate = {2024-07-16},
  abstract = {In lifetesting, medical follow-up, and other fields the observation of the time of occurrence of the event of interest (called a death) may be prevented for some of the items of the sample by the previous occurrence of some other event (called a loss). Losses may be either accidental or controlled, the latter resulting from a decision to terminate certain observations. In either case it is usually assumed in this paper that the lifetime (age at death) is independent of the potential loss time; in practice this assumption deserves careful scrutiny. Despite the resulting incompleteness of the data, it is desired to estimate the proportion P(t) of items in the population whose lifetimes would exceed t (in the absence of such losses), without making any assumption about the form of the function P(t). The observation for each item of a suitable initial event, marking the beginning of its lifetime, is presupposed. For random samples of size N the product-limit (PL) estimate can be defined as follows: List and label the N observed lifetimes (whether to death or loss) in order of increasing magnitude, so that one has 0 {$\leq$} t\textsubscript{1}' {$\leq$} t\textsubscript{2}' {$\leq$} {$\cdots$} {$\leq$} t\textsubscript{N}'. Then \${\textbackslash}hat\{P\}(t) = {\textbackslash}prod\_r {\textbackslash}lbrack(N - r)/(N - r + 1){\textbackslash}rbrack\$, where r assumes those values for which t\textsubscript{r}' {$\leq$} t and for which t\textsubscript{r}' measures the time to death. This estimate is the distribution, unrestricted as to form, which maximizes the likelihood of the observations. Other estimates that are discussed are the actuarial estimates (which are also products, but with the number of factors usually reduced by grouping); and reduced-sample (RS) estimates, which require that losses not be accidental, so that the limits of observation (potential loss times) are known even for those items whose deaths are observed. When no losses occur at ages less than t, the estimate of P(t) in all cases reduces to the usual binomial estimate, namely, the observed proportion of survivors.},
  file = {C:\Users\efbonneville\Zotero\storage\4SRETTLB\Kaplan and Meier - 1958 - Nonparametric Estimation from Incomplete Observati.pdf}
}

@article{keoghMultipleImputationCox2018,
  title = {Multiple Imputation in {{Cox}} Regression When There Are Time-Varying Effects of Covariates},
  author = {Keogh, Ruth H. and Morris, Tim P.},
  year = {2018},
  journal = {Statistics in Medicine},
  volume = {37},
  number = {25},
  pages = {3661--3678},
  issn = {1097-0258},
  doi = {10.1002/sim.7842},
  urldate = {2021-05-03},
  abstract = {In Cox regression, it is important to test the proportional hazards assumption and sometimes of interest in itself to study time-varying effects (TVEs) of covariates. TVEs can be investigated with log hazard ratios modelled as a function of time. Missing data on covariates are common and multiple imputation is a popular approach to handling this to avoid the potential bias and efficiency loss resulting from a ``complete-case'' analysis. Two multiple imputation methods have been proposed for when the substantive model is a Cox proportional hazards regression: an approximate method (Imputing missing covariate values for the Cox model in Statistics in Medicine (2009) by White and Royston) and a substantive-model-compatible method (Multiple imputation of covariates by fully conditional specification: accommodating the substantive model in Statistical Methods in Medical Research (2015) by Bartlett et al). At present, neither accommodates TVEs of covariates. We extend them to do so for a general form for the TVEs and give specific details for TVEs modelled using restricted cubic splines. Simulation studies assess the performance of the methods under several underlying shapes for TVEs. Our proposed methods give approximately unbiased TVE estimates for binary covariates with missing data, but for continuous covariates, the substantive-model-compatible method performs better. The methods also give approximately correct type I errors in the test for proportional hazards when there is no TVE and gain power to detect TVEs relative to complete-case analysis. Ignoring TVEs at the imputation stage results in biased TVE estimates, incorrect type I errors, and substantial loss of power in detecting TVEs. We also propose a multivariable TVE model selection algorithm. The methods are illustrated using data from the Rotterdam Breast Cancer Study. R code is provided.},
  copyright = {{\copyright} 2018 The Authors. Statistics~in~Medicine Published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {Cox regression,missing data,multiple imputation,restricted cubic spline,time-varying effect},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\3MKX2VTY\\Keogh and Morris - 2018 - Multiple imputation in Cox regression when there a.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\YVFGGWB6\\sim.html}
}

@article{keoghMultipleImputationMissing2018,
  title = {Multiple {{Imputation}} of {{Missing Data}} in {{Nested Case-Control}} and {{Case-Cohort Studies}}},
  author = {Keogh, Ruth H. and Seaman, Shaun R. and Bartlett, Jonathan W. and Wood, Angela M.},
  year = {2018},
  month = dec,
  journal = {Biometrics},
  volume = {74},
  number = {4},
  pages = {1438--1449},
  issn = {0006-341X},
  doi = {10.1111/biom.12910},
  urldate = {2024-12-10},
  abstract = {The nested case-control and case-cohort designs are two main approaches for carrying out a substudy within a prospective cohort. This article adapts multiple imputation (MI) methods for handling missing covariates in full-cohort studies for nested case-control and case-cohort studies. We consider data missing by design and data missing by chance. MI analyses that make use of full-cohort data and MI analyses based on substudy data only are described, alongside an intermediate approach in which the imputation uses full-cohort data but the analysis uses only the substudy. We describe adaptations to two imputation methods: the approximate method (MI-approx) of White and Royston (2009) and the ``substantive model compatible'' (MI-SMC) method of Bartlett et al. (2015). We also apply the ``MI matched set'' approach of Seaman and Keogh (2015) to nested case-control studies, which does not require any full-cohort information. The methods are investigated using simulation studies and all perform well when their assumptions hold. Substantial gains in efficiency can be made by imputing data missing by design using the full-cohort approach or by imputing data missing by chance in analyses using the substudy only. The intermediate approach brings greater gains in efficiency relative to the substudy approach and is more robust to imputation model misspecification than the full-cohort approach. The methods are illustrated using the ARIC Study cohort. Supplementary Materials provide R and Stata code.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\FBFGH5K8\\Keogh et al. - 2018 - Multiple Imputation of Missing Data in Nested Case.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\GRP2TC9N\\7537543.html}
}

@article{keoghUsingFullcohortData2013,
  title = {Using Full-Cohort Data in Nested Case--Control and Case--Cohort Studies by Multiple Imputation},
  author = {Keogh, Ruth H. and White, Ian R.},
  year = {2013},
  journal = {Statistics in Medicine},
  volume = {32},
  number = {23},
  pages = {4021--4043},
  issn = {1097-0258},
  doi = {10.1002/sim.5818},
  urldate = {2024-12-10},
  abstract = {In many large prospective cohorts, expensive exposure measurements cannot be obtained for all individuals. Exposure--disease association studies are therefore often based on nested case--control or case--cohort studies in which complete information is obtained only for sampled individuals. However, in the full cohort, there may be a large amount of information on cheaply available covariates and possibly a surrogate of the main exposure(s), which typically goes unused. We view the nested case--control or case--cohort study plus the remainder of the cohort as a full-cohort study with missing data. Hence, we propose using multiple imputation (MI) to utilise information in the full cohort when data from the sub-studies are analysed. We use the fully observed data to fit the imputation models. We consider using approximate imputation models and also using rejection sampling to draw imputed values from the true distribution of the missing values given the observed data. Simulation studies show that using MI to utilise full-cohort information in the analysis of nested case--control and case--cohort studies can result in important gains in efficiency, particularly when a surrogate of the main exposure is available in the full cohort. In simulations, this method outperforms counter-matching in nested case--control studies and a weighted analysis for case--cohort studies, both of which use some full-cohort information. Approximate imputation models perform well except when there are interactions or non-linear terms in the outcome model, where imputation using rejection sampling works well. Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {case-cohort study,multiple imputation,nested case-control study,rejection sampling},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\2L89ZXJP\\Keogh and White - 2013 - Using full-cohort data in nested case–control and .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\TEQUULIJ\\sim.html}
}

@article{kimNovelCompositeEndpoints2021,
  title = {Novel {{Composite Endpoints}} after {{Allogeneic Hematopoietic Cell Transplantation}}},
  author = {Kim, Haesook T. and Logan, Brent and Weisdorf, Daniel J.},
  year = {2021},
  month = aug,
  journal = {Transplantation and Cellular Therapy},
  volume = {27},
  number = {8},
  pages = {650--657},
  issn = {2666-6367},
  doi = {10.1016/j.jtct.2021.05.005},
  urldate = {2023-10-06},
  abstract = {With the recent development of transplant-specific composite endpoints for evaluation of allogeneic hematopoietic cell transplantation (alloHCT) outcomes, the use of these novel endpoints is growing rapidly. Combining multiple endpoints into a single endpoint, these composite endpoints appear simple and can be used as a summary measure for overall effectiveness of an intervention. However, all component endpoints may not have equal clinical significance, and an intervention may not work proportionally in the same direction for all components of a composite endpoint. This may complicate the interpretation of results, particularly if there are opposing effects of differing component endpoints. We assess the benefits and limitations of various composite endpoints used in alloHCT studies recently and propose guidelines for their use and interpretation. {\copyright} 2021 American Society for Blood and Marrow Transplantation. Published by Elsevier Inc. All rights reserved.},
  keywords = {Allogeneic hematopoietic cell transplantation,Composite Endpoint,GRFS},
  file = {C:\Users\efbonneville\Zotero\storage\FLZMAPHR\Kim et al. - 2021 - Novel Composite Endpoints after Allogeneic Hematop.pdf}
}

@article{kipourouEstimationAdjustedCausespecific2019,
  title = {Estimation of the Adjusted Cause-Specific Cumulative Probability Using Flexible Regression Models for the Cause-Specific Hazards},
  author = {Kipourou, Dimitra-Kleio and Charvat, Hadrien and Rachet, Bernard and Belot, Aur{\'e}lien},
  year = {2019},
  journal = {Statistics in Medicine},
  volume = {38},
  number = {20},
  pages = {3896--3910},
  issn = {1097-0258},
  doi = {10.1002/sim.8209},
  urldate = {2024-03-04},
  abstract = {In competing risks setting, we account for death according to a specific cause and the quantities of interest are usually the cause-specific hazards (CSHs) and the cause-specific cumulative probabilities. A cause-specific cumulative probability can be obtained with a combination of the CSHs or via the subdistribution hazard. Here, we modeled the CSH with flexible hazard-based regression models using B-splines for the baseline hazard and time-dependent (TD) effects. We derived the variance of the cause-specific cumulative probabilities at the population level using the multivariate delta method and showed how we could easily quantify the impact of a covariate on the cumulative probability scale using covariate-adjusted cause-specific cumulative probabilities and their difference. We conducted a simulation study to evaluate the performance of this approach in its ability to estimate the cumulative probabilities using different functions for the cause-specific log baseline hazard and with or without a TD effect. In the scenario with TD effect, we tested both well-specified and misspecified models. We showed that the flexible regression models perform nearly as well as the nonparametric method, if we allow enough flexibility for the baseline hazards. Moreover, neglecting the TD effect hardly affects the cumulative probabilities estimates of the whole population but impacts them in the various subgroups. We illustrated our approach using data from people diagnosed with monoclonal gammopathy of undetermined significance and provided the R-code to derive those quantities, as an extension of the R-package mexhaz.},
  copyright = {{\copyright} 2019 The Authors Statistics in Medicine Published by John Wiley \& Sons Ltd},
  langid = {english},
  keywords = {cause-specific hazards,competing risks,cumulative incidence function,cumulative probability of death,flexible parametric models},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\8HPN895J\\Kipourou et al. - 2019 - Estimation of the adjusted cause-specific cumulati.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\T97S5GU5\\sim.html}
}

@book{kleinHandbookSurvivalAnalysis2016,
  title = {Handbook of {{Survival Analysis}}},
  author = {Klein, John P. and van Houwelingen, Hans C. and Ibrahim, Joseph G. and Scheike, Thomas H.},
  year = {2016},
  month = apr,
  publisher = {CRC Press},
  abstract = {Handbook of Survival Analysis presents modern techniques and research problems in lifetime data analysis. This area of statistics deals with time-to-event data that is complicated by censoring and the dynamic nature of events occurring in time. With chapters written by leading researchers in the field, the handbook focuses on advances in survival analysis techniques, covering classical and Bayesian approaches. It gives a complete overview of the current status of survival analysis and should inspire further research in the field. Accessible to a wide range of readers, the book provides:An introduction to various areas in survival analysis for graduate students and novicesA reference to modern investigations into survival analysis for more established researchers A text or supplement for a second or advanced course in survival analysisA useful guide to statistical methods for analyzing survival data experiments for practicing statisticians},
  googlebooks = {t1vOBQAAQBAJ},
  isbn = {978-1-4665-5567-9},
  langid = {english},
  keywords = {Mathematics / Probability & Statistics / General,Medical / Epidemiology}
}

@article{kleinkeMultipleImputationViolated2017,
  title = {Multiple {{Imputation Under Violated Distributional Assumptions}}: {{A Systematic Evaluation}} of the {{Assumed Robustness}} of {{Predictive Mean Matching}}},
  shorttitle = {Multiple {{Imputation Under Violated Distributional Assumptions}}},
  author = {Kleinke, Kristian},
  year = {2017},
  month = aug,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {42},
  number = {4},
  pages = {371--404},
  publisher = {American Educational Research Association},
  issn = {1076-9986},
  doi = {10.3102/1076998616687084},
  urldate = {2024-10-22},
  abstract = {Predictive mean matching (PMM) is a standard technique for the imputation of incomplete continuous data. PMM imputes an actual observed value, whose predicted value is among a set of k {$\geq$} 1 values (the so-called donor pool), which are closest to the one predicted for the missing case. PMM is usually better able to preserve the original distribution of the empirical data than fully parametric multiple imputation (MI) approaches, when empirical data deviate from their distributional assumptions. Use of PMM is therefore especially worthwhile in situations where model assumptions of fully parametric MI procedures are violated and where fully parametric procedures would yield highly implausible estimates. Unfortunately, today there are only a handful of studies that systematically tested the robustness of PMM and it is still widely unknown where exactly the limits of this procedure lie. I examined the performance of PMM in situations where data were skewed to varying degrees, under different sample sizes, missing data percentages, and using different settings of the PMM approach. It was found that small donor pools overall yielded better results than large donor pools and that PMM generally worked well, unless data were highly skewed and more than about 20\% to 30\% of the data had to be imputed. Also, PMM generally performed better when sample size was sufficiently large.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\4F7ZIA9P\Kleinke - 2017 - Multiple Imputation Under Violated Distributional .pdf}
}

@article{kleinRegressionModelingCompeting2005a,
  title = {Regression {{Modeling}} of {{Competing Risks Data Based}} on {{Pseudovalues}} of the {{Cumulative Incidence Function}}},
  author = {Klein, John P. and Andersen, Per Kragh},
  year = {2005},
  journal = {Biometrics},
  volume = {61},
  number = {1},
  pages = {223--229},
  issn = {1541-0420},
  doi = {10.1111/j.0006-341X.2005.031209.x},
  urldate = {2023-10-12},
  abstract = {Typically, regression models for competing risks outcomes are based on proportional hazards models for the crude hazard rates. These estimates often do not agree with impressions drawn from plots of cumulative incidence functions for each level of a risk factor. We present a technique which models the cumulative incidence functions directly. The method is based on the pseudovalues from a jackknife statistic constructed from the cumulative incidence curve. These pseudovalues are used in a generalized estimating equation to obtain estimates of model parameters. We study the properties of this estimator and apply the technique to a study of the effect of alternative donors on relapse for patients given a bone marrow transplant for leukemia.},
  langid = {english},
  keywords = {Bone marrow transplantation,Generalized estimating equations,Jackknife statistics,Regression models},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\QIGBAV8W\\Klein and Andersen - 2005 - Regression Modeling of Competing Risks Data Based .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\7TK9WFMQ\\j.0006-341X.2005.031209.html}
}

@book{kleinSurvivalAnalysisTechniques2006,
  title = {Survival Analysis: {{Techniques}} for Censored and Truncated Data (2nd Edition)},
  author = {Klein, J.P. and Moeschberger, M.L.},
  year = {2003},
  publisher = {Springer-Verlag},
  address = {New York}
}

@article{kosterCompetitiveRepopulationAlloImmunologic2023,
  title = {Competitive {{Repopulation}} and {{Allo-Immunologic Pressure Determine Chimerism Kinetics}} after {{T Cell-Depleted Allogeneic Stem Cell Transplantation}} and {{Donor Lymphocyte Infusion}}},
  author = {Koster, Eva A. S. and {von dem Borne}, Peter A. and {van Balen}, Peter and {van Egmond}, Esther H. M. and Marijt, Erik W. A. and Veld, Sabrina A. J. and Jedema, Inge and Snijders, Tjeerd J. F. and {van Lammeren}, Dani{\"e}lle and Veelken, Hendrik and Falkenburg, J. H. Frederik and {de Wreede}, Liesbeth C. and Halkes, Constantijn J. M.},
  year = {2023},
  month = apr,
  journal = {Transplantation and Cellular Therapy},
  volume = {29},
  number = {4},
  pages = {268.e1-268.e10},
  issn = {2666-6367},
  doi = {10.1016/j.jtct.2022.12.022},
  urldate = {2023-11-04},
  abstract = {After allogeneic stem cell transplantation (alloSCT), patient-derived stem cells that survived the pretransplantation conditioning compete with engrafting donor stem cells for bone marrow (BM) repopulation. In addition, donor-derived alloreactive T cells present in the stem cell product may favor establishment of complete donor-derived hematopoiesis by eliminating patient-derived lymphohematopoietic cells. T cell-depleted alloSCT with sequential transfer of potentially alloreactive T cells by donor lymphocyte infusion (DLI) provides a unique opportunity to selectively study how competitive repopulation and allo-immunologic pressure influence lymphohematopoietic recovery. This study aimed to determine the relative contribution of competitive repopulation and donor-derived anti-recipient alloimmunologic pressure on the establishment of lymphohematopoietic chimerism after alloSCT. In this retrospective cohort study of 281 acute leukemia patients treated according to a protocol combining alemtuzumab-based T cell-depleted alloSCT with prophylactic DLI, we investigated engraftment and quantitative donor chimerism in the BM and immune cell subsets. DLI-induced increase of chimerism and development of graft-versus-host disease (GVHD) were analyzed as complementary indicators for donor-derived anti-recipient alloimmunologic pressure. Profound suppression of patient immune cells by conditioning sufficed for sustained engraftment without necessity for myeloablative conditioning or development of clinically significant GVHD. Although 61\% of the patients without any DLI or GVHD showed full donor chimerism (FDC) in the BM at 6 months after alloSCT, only 24\% showed FDC in the CD4+ T cell compartment. In contrast, 75\% of the patients who had received DLI and 83\% of the patients with clinically significant GVHD had FDC in this compartment. In addition, 72\% of the patients with mixed hematopoiesis receiving DLI converted to complete donor-derived hematopoiesis, of whom only 34\% developed clinically significant GVHD. Our data show that competitive repopulation can be sufficient to reach complete donor-derived hematopoiesis, but that some alloimmunologic pressure is needed for the establishment of a completely donor-derived T cell compartment, either by the development of GVHD or by administration of DLI. We illustrate that it is possible to separate the graft-versus-leukemia effect from GVHD, as conversion to durable complete donor-derived hematopoiesis following DLI did not require induction of clinically significant GVHD.},
  keywords = {Acute lymphoblastic leukemia,Acute myeloid leukemia,Allogeneic stem cell transplantation,Chimerism,Donor lymphocyte infusion,Graft-versus-host disease,T cell depletion},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\8KSBNYZ6\\Koster et al. - 2023 - Competitive Repopulation and Allo-Immunologic Pres.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\LNGLL4F2\\S2666636722018693.html}
}

@article{kosterJointModelsQuantify2023,
  title = {Joint Models Quantify Associations between Immune Cell Kinetics and Allo-Immunological Events after Allogeneic Stem Cell Transplantation and Subsequent Donor Lymphocyte Infusion},
  author = {Koster, Eva A. S. and Bonneville, Edouard F. and von dem Borne, Peter A. and {van Balen}, Peter and Marijt, Erik W. A. and Tjon, Jennifer M. L. and Snijders, Tjeerd J. F. and {van Lammeren}, Dani{\"e}lle and Veelken, Hendrik and Putter, Hein and Falkenburg, J. H. Frederik and Halkes, Constantijn J. M. and {de Wreede}, Liesbeth C.},
  year = {2023},
  month = aug,
  journal = {Frontiers in Immunology},
  volume = {14},
  publisher = {Frontiers},
  issn = {1664-3224},
  doi = {10.3389/fimmu.2023.1208814},
  urldate = {2024-08-28},
  abstract = {{$<$}p{$>$}Alloreactive donor-derived T-cells play a pivotal role in alloimmune responses after allogeneic hematopoietic stem cell transplantation (alloSCT); both in the relapse-preventing Graft-versus-Leukemia (GvL) effect and the potentially lethal complication Graft-versus-Host-Disease (GvHD). The balance between GvL and GvHD can be shifted by removing T-cells via T-cell depletion (TCD) to reduce the risk of GvHD, and by introducing additional donor T-cells (donor lymphocyte infusions [DLI]) to boost the GvL effect. However, the association between T-cell kinetics and the occurrence of allo-immunological events has not been clearly demonstrated yet. Therefore, we investigated the complex associations between the T-cell kinetics and alloimmune responses in a cohort of 166 acute leukemia patients receiving alemtuzumab-based TCD alloSCT. Of these patients, 62 with an anticipated high risk of relapse were scheduled to receive a prophylactic DLI at 3 months after transplant. In this setting, we applied joint modelling which allowed us to better capture the complex interplay between DLI, T-cell kinetics, GvHD and relapse than traditional statistical methods. We demonstrate that DLI can induce detectable T-cell expansion, leading to an increase in total, CD4+ and CD8+ T-cell counts starting at 3 months after alloSCT. CD4+ T-cells showed the strongest association with the development of alloimmune responses: higher CD4 counts increased the risk of GvHD (hazard ratio 2.44, 95\% confidence interval 1.45-4.12) and decreased the risk of relapse (hazard ratio 0.65, 95\% confidence interval 0.45-0.92). Similar models showed that natural killer cells recovered rapidly after alloSCT and were associated with a lower risk of relapse (HR 0.62, 95\%-CI 0.41-0.93). The results of this study advocate the use of joint models to further study immune cell kinetics in different settings.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Acute Lymphoblastic Leukemia,Acute Myeloid Leukemia,allogeneic stem cell transplantation,Donor lymphocyte infusion,graft-versus-host-disease,Joint modelling,T-cell depletion,T-cell kinetics},
  file = {C:\Users\efbonneville\Zotero\storage\C38A6M7P\Koster et al. - 2023 - Joint models quantify associations between immune .pdf}
}

@article{krishnamurthyOutcomeDonorLymphocyte2013,
  title = {Outcome of {{Donor Lymphocyte Infusion}} after {{T Cell}}--Depleted {{Allogeneic Hematopoietic Stem Cell~Transplantation}} for {{Acute Myelogenous Leukemia}}~and~{{Myelodysplastic Syndromes}}},
  author = {Krishnamurthy, Pramila and Potter, Victoria T. and Barber, Linda D. and Kulasekararaj, Austin G. and Lim, Zi Yi and Pearce, Rachel M. and {de Lavallade}, Hugues and Kenyon, Michelle and Ireland, Robin M. and Marsh, Judith C. W. and Devereux, Stephen and Pagliuca, Antonio and Mufti, Ghulam J.},
  year = {2013},
  month = apr,
  journal = {Biology of Blood and Marrow Transplantation},
  volume = {19},
  number = {4},
  pages = {562--568},
  issn = {1083-8791},
  doi = {10.1016/j.bbmt.2012.12.013},
  urldate = {2023-11-04},
  abstract = {Relapse occurs in 30\%-50\% of recipients of T cell--depleted (TCD) reduced-intensity conditioned (RIC) hematopoietic stem cell transplantation (HSCT) for acute myelogenous leukemia (AML) and myelodysplastic syndromes (MDS). Despite limited published supportive data, donor lymphocyte infusion (DLI) is used preemptively (pDLI) to improve donor chimerism and prevent relapse, and therapeutically (tDLI) after disease recurrence. We evaluated the efficacy and toxicity of pDLI and tDLI in 113 patients after TCD (alemtuzumab, n~=~99; antithymocyte globulin, n~=~14) RIC HSCT for AML or MDS. Recipients of pDLI (n~=~62) had an estimated 5-year overall survival (OS) of 80\% and an event-free survival of 65\%. More than one-half (52\%; n~=~32) of the patients received pDLI within 6 months post-HSCT; despite this, the 5-year incidence of graft-versus-host disease was only 31\% (95\% confidence interval [CI], 19\%-43\%). Recipients of tDLI (n~=~51) had an estimated 5-year OS of 40\% and a 5-year relapse/progression rate of 69\% (95\% CI, 54\%-81\%). Recipients of tDLI at {$>$}6 months post-HSCT had a significantly superior 5-year OS after tDLI compared with those treated earlier (P~=~.008). The cumulative incidence of graft-versus-host disease at 5 years after tDLI was 45\% (95\% CI, 23\%-65\%). We demonstrate that pDLI safely promotes durable remission after TCD RIC HSCT for AML or MDS, and that tDLI salvages patients after late relapse with greater efficacy.},
  keywords = {Myeloid cancer,Reduced-intensity conditioning,T cell depletion},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\RUHGRN98\\Krishnamurthy et al. - 2013 - Outcome of Donor Lymphocyte Infusion after T Cell–.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\UF6HZTZF\\S108387911201172X.html}
}

@article{krogerIndicationManagementAllogeneic2024,
  title = {Indication and Management of Allogeneic Haematopoietic Stem-Cell Transplantation in Myelofibrosis: Updated Recommendations by the {{EBMT}}/{{ELN International Working Group}}},
  shorttitle = {Indication and Management of Allogeneic Haematopoietic Stem-Cell Transplantation in Myelofibrosis},
  author = {Kr{\"o}ger, Nicolaus and Bacigalupo, Andrea and Barbui, Tiziano and Ditschkowski, Markus and Gagelmann, Nico and Griesshammer, Martin and Gupta, Vikas and Hamad, Nada and Harrison, Claire and {Hernandez-Boluda}, Juan Carlos and Koschmieder, Steffen and Jain, Tania and Mascarenhas, John and Mesa, Ruben and Popat, Uday R. and Passamonti, Francesco and Polverelli, Nicola and Rambaldi, Alessandro and Robin, Marie and Salit, Rachel B. and Schroeder, Thomas and Scott, Bart L. and Tamari, Roni and Tefferi, Ayalew and Vannucchi, Alessandro M. and McLornan, Donal P. and Barosi, Giovanni},
  year = {2024},
  month = jan,
  journal = {The Lancet Haematology},
  volume = {11},
  number = {1},
  pages = {e62-e74},
  publisher = {Elsevier},
  issn = {2352-3026},
  doi = {10.1016/S2352-3026(23)00305-8},
  urldate = {2024-03-20},
  langid = {english},
  pmid = {38061384},
  file = {C:\Users\efbonneville\Zotero\storage\FGKGEUUY\Kröger et al. - 2024 - Indication and management of allogeneic haematopoi.pdf}
}

@article{lairdMissingDataLongitudinal1988,
  title = {Missing Data in Longitudinal Studies},
  author = {Laird, Nan M.},
  year = {1988},
  journal = {Statistics in Medicine},
  volume = {7},
  number = {1-2},
  pages = {305--315},
  issn = {1097-0258},
  doi = {10.1002/sim.4780070131},
  urldate = {2024-08-23},
  abstract = {When observations are made repeatedly over time on the same experimental units, unbalanced patterns of observations are a common occurrence. This complication makes standard analyses more difficult or inappropriate to implement, means loss of efficiency, and may introduce bias into the results as well. Some possible approaches to dealing with missing data include complete case analyses, univariate analyses with adjustments for variance estimates, two-step analyses, and likelihood based approaches. Likelihood approaches can be further categorized as to whether or not an explicit model is introduced for the nonresponse mechanism. This paper will review the use of likelihood based analyses for longitudinal data with missing responses, both from the point of view of ease of implementation and appropriateness in view of the non-response mechanism. Models for both measured and dichotomous outcome data will be discussed. The appropriateness of some non-likelihood based analyses is briefly considered.},
  copyright = {Copyright {\copyright} 1988 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Dropouts,EM algorithm,Ignorable and non-ignorable non-response,Maximum likelihood},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\PP9WN95S\\Laird - 1988 - Missing data in longitudinal studies.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\WYLQDUJK\\sim.html}
}

@article{lambertFlexibleParametricModelling2017,
  title = {Flexible Parametric Modelling of the Cause-Specific Cumulative Incidence Function},
  author = {Lambert, Paul C. and Wilkes, Sally R. and Crowther, Michael J.},
  year = {2017},
  journal = {Statistics in Medicine},
  volume = {36},
  number = {9},
  pages = {1429--1446},
  issn = {1097-0258},
  doi = {10.1002/sim.7208},
  urldate = {2023-10-06},
  abstract = {Competing risks arise with time-to-event data when individuals are at risk of more than one type of event and the occurrence of one event precludes the occurrence of all other events. A useful measure with competing risks is the cause-specific cumulative incidence function (CIF), which gives the probability of experiencing a particular event as a function of follow-up time, accounting for the fact that some individuals may have a competing event. When modelling the cause-specific CIF, the most common model is a semi-parametric proportional subhazards model. In this paper, we propose the use of flexible parametric survival models to directly model the cause-specific CIF where the effect of follow-up time is modelled using restricted cubic splines. The models provide smooth estimates of the cause-specific CIF with the important advantage that the approach is easily extended to model time-dependent effects. The models can be fitted using standard survival analysis tools by a combination of data expansion and introducing time-dependent weights. Various link functions are available that allow modelling on different scales and have proportional subhazards, proportional odds and relative absolute risks as particular cases. We conduct a simulation study to evaluate how well the spline functions approximate subhazard functions with complex shapes. The methods are illustrated using data from the European Blood and Marrow Transplantation Registry showing excellent agreement between parametric estimates of the cause-specific CIF and those obtained from a semi-parametric model. We also fit models relaxing the proportional subhazards assumption using alternative link functions and/or including time-dependent effects. Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {competing risks,cumulative incidence function,flexible parametric models},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\ZJZP8R3V\\Lambert et al. - 2017 - Flexible parametric modelling of the cause-specifi.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\Y2QXEER7\\sim.html}
}

@article{landauTargetsPackageDynamic2021,
  title = {The Targets {{R}} Package: A Dynamic {{Make-like}} Function-Oriented Pipeline Toolkit for Reproducibility and High-Performance Computing},
  author = {Landau, William Michael},
  year = {2021},
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {57},
  pages = {2959}
}

@article{latoucheCompetingRisksAnalysis2013,
  title = {A Competing Risks Analysis Should Report Results on All Cause-Specific Hazards and Cumulative Incidence Functions},
  author = {Latouche, Aurelien and Allignol, Arthur and Beyersmann, Jan and Labopin, Myriam and Fine, Jason P.},
  year = {2013},
  month = jun,
  journal = {Journal of Clinical Epidemiology},
  volume = {66},
  number = {6},
  pages = {648--653},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2012.09.017},
  urldate = {2023-10-05},
  abstract = {Competing risks endpoints are frequently encountered in hematopoietic stem cell transplantation where patients are exposed to relapse and treatment-related mortality. Both cause-specific hazards and direct models for the cumulative incidence functions have been used for analyzing such competing risks endpoints. For both approaches, the popular models are of a proportional hazards type. Such models have been used for studying prognostic factors in acute and chronic leukemias. We argue that a complete understanding of the event dynamics requires that both hazards and cumulative incidence be analyzed side by side, and that this is generally the most rigorous scientific approach to analyzing competing risks data. That is, understanding the effects of covariates on cause-specific hazards and cumulative incidence functions go hand in hand. A case study illustrates our proposal.},
  keywords = {Bone marrow transplant,Competing risks,Cumulative incidence,Endpoints,Proportional hazards,Survival analysis},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\4ZKEVRZB\\Latouche et al. - 2013 - A competing risks analysis should report results o.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\XTMTP9Q3\\S0895435612003484.html}
}

@article{latoucheMisspecifiedRegressionModel2007,
  title = {Misspecified Regression Model for the Subdistribution Hazard of a Competing Risk},
  author = {Latouche, A. and Boisson, V. and Chevret, S. and Porcher, R.},
  year = {2007},
  journal = {Statistics in Medicine},
  volume = {26},
  number = {5},
  pages = {965--974},
  issn = {1097-0258},
  doi = {10.1002/sim.2600},
  urldate = {2023-10-06},
  abstract = {We consider a competing risks setting, when evaluating the prognostic influence of an exposure on a specific cause of failure. Two main regression models are used in such analyses, the Cox cause-specific proportional hazards model and the subdistribution proportional hazards model. They are exemplified in a real data example focusing on relapse-free interval in acute leukaemia patients. We examine the properties of the estimator based on the latter model when the true model is the former. An explicit relationship between subdistribution hazards ratio and cause-specific hazards ratio is derived, assuming a flexible parametric distribution for latent failure times. Copyright {\copyright} 2006 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2006 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {cause-specific,cumulative incidence,model misspecification,proportional hazards},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\KSTYAX8S\\Latouche et al. - 2007 - Misspecified regression model for the subdistribut.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\5CI4ZSZ7\\sim.html}
}

@article{lauCompetingRiskRegression2009,
  title = {Competing {{Risk Regression Models}} for {{Epidemiologic Data}}},
  author = {Lau, Bryan and Cole, Stephen R. and Gange, Stephen J.},
  year = {2009},
  month = jul,
  journal = {American Journal of Epidemiology},
  volume = {170},
  number = {2},
  pages = {244--256},
  issn = {0002-9262},
  doi = {10.1093/aje/kwp107},
  urldate = {2023-10-06},
  abstract = {Competing events can preclude the event of interest from occurring in epidemiologic data and can be analyzed by using extensions of survival analysis methods. In this paper, the authors outline 3 regression approaches for estimating 2 key quantities in competing risks analysis: the cause-specific relative hazard (csRH) and the subdistribution relative hazard (sdRH). They compare and contrast the structure of the risk sets and the interpretation of parameters obtained with these methods. They also demonstrate the use of these methods with data from the Women's Interagency HIV Study established in 1993, treating time to initiation of highly active antiretroviral therapy or to clinical disease progression as competing events. In our example, women with an injection drug use history were less likely than those without a history of injection drug use to initiate therapy prior to progression to acquired immunodeficiency syndrome or death by both measures of association (csRH\,=\,0.67, 95\% confidence interval: 0.57, 0.80 and sdRH\,=\,0.60, 95\% confidence interval: 0.50, 0.71). Moreover, the relative hazards for disease progression prior to treatment were elevated (csRH\,=\,1.71, 95\% confidence interval: 1.37, 2.13 and sdRH\,=\,2.01, 95\% confidence interval: 1.62, 2.51). Methods for competing risks should be used by epidemiologists, with the choice of method guided by the scientific question.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\9TKPDCDZ\\Lau et al. - 2009 - Competing Risk Regression Models for Epidemiologic.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\9UAXP5WX\\111339.html}
}

@article{lauMissingnessSettingCompeting2018,
  title = {Missingness in the {{Setting}} of {{Competing Risks}}: From {{Missing Values}} to {{Missing Potential Outcomes}}},
  shorttitle = {Missingness in the {{Setting}} of {{Competing Risks}}},
  author = {Lau, Bryan and Lesko, Catherine},
  year = {2018},
  month = jun,
  journal = {Current Epidemiology Reports},
  volume = {5},
  number = {2},
  pages = {153--159},
  issn = {2196-2995},
  doi = {10.1007/s40471-018-0142-3},
  urldate = {2020-10-21},
  abstract = {The setting of competing risks in which there is an event that precludes the event of interest from occurring is prevalent in epidemiological research. Unless studying all-cause mortality, any study following up individuals is subject to having a competing risk should individuals die during time period that the study covers. While there are prior papers discussing the need for competing risk methods in epidemiologic research, we are not aware of any review that discusses issues of missing data in a competing risk setting.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\BL6W9EZK\Lau and Lesko - 2018 - Missingness in the Setting of Competing Risks fro.pdf}
}

@article{leeAssumptionsAnalysisPlanning2023,
  title = {Assumptions and Analysis Planning in Studies with Missing Data in Multiple Variables: Moving beyond the {{MCAR}}/{{MAR}}/{{MNAR}} Classification},
  shorttitle = {Assumptions and Analysis Planning in Studies with Missing Data in Multiple Variables},
  author = {Lee, Katherine J and Carlin, John B and Simpson, Julie A and {Moreno-Betancur}, Margarita},
  year = {2023},
  month = aug,
  journal = {International Journal of Epidemiology},
  volume = {52},
  number = {4},
  pages = {1268--1275},
  issn = {0300-5771},
  doi = {10.1093/ije/dyad008},
  urldate = {2024-08-06},
  abstract = {Researchers faced with incomplete data are encouraged to consider whether their data are `missing completely at random' (MCAR), `missing at random' (MAR) or `missing not at random' (MNAR) when planning their analysis. However, there are two major problems with this classification as originally defined by Rubin in the 1970s. First, when there are missing data in multiple variables, the plausibility of the MAR assumption is difficult to assess using substantive knowledge and is more stringent than is generally appreciated. Second, although MCAR and MAR are sufficient conditions for consistent estimation with specific methods, they are not necessary conditions and therefore this categorization does not directly determine the best approach for handling the missing data in an analysis. How best to handle missing data depends on the assumed causal relationships between variables and their missingness, and what these relationships imply in terms of the `recoverability' of the target estimand (the population parameter that encodes the answer to the underlying research question). Recoverability is defined as whether the estimand can be consistently estimated from the patterns and associations in the observed data without needing to invoke external information on the extent to which the distribution of missing values might differ from that of observed values. In this manuscript we outline an approach for deciding which method to use to handle multivariable missing data in an analysis, using directed acyclic graphs to depict missingness assumptions and determining the implications in terms of recoverability of the target estimand.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\VSPVQAZE\\Lee et al. - 2023 - Assumptions and analysis planning in studies with .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\QPSXTIAC\\7034967.html}
}

@article{leeFrameworkTreatmentReporting2021,
  title = {Framework for the Treatment and Reporting of Missing Data in Observational Studies: {{The Treatment And Reporting}} of {{Missing}} Data in {{Observational Studies}} Framework},
  shorttitle = {Framework for the Treatment and Reporting of Missing Data in Observational Studies},
  author = {Lee, Katherine J. and Tilling, Kate M. and Cornish, Rosie P. and Little, Roderick J. A. and Bell, Melanie L. and Goetghebeur, Els and Hogan, Joseph W. and Carpenter, James R.},
  year = {2021},
  month = jun,
  journal = {Journal of Clinical Epidemiology},
  volume = {134},
  pages = {79--88},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2021.01.008},
  urldate = {2022-10-10},
  abstract = {Missing data are ubiquitous in medical research. Although there is increasing guidance on how to handle missing data, practice is changing slowly and misapprehensions abound, particularly in observational research. Importantly, the lack of transparency around methodological decisions is threatening the validity and reproducibility of modern research. We present a practical framework for handling and reporting the analysis of incomplete data in observational studies, which we illustrate using a case study from the Avon Longitudinal Study of Parents and Children. The framework consists of three steps: 1) Develop an analysis plan specifying the analysis model and how missing data are going to be addressed. An important consideration is whether a complete records' analysis is likely to be valid, whether multiple imputation or an alternative approach is likely to offer benefits and whether a sensitivity analysis regarding the missingness mechanism is required; 2) Examine the data, checking the methods outlined in the analysis plan are appropriate, and conduct the preplanned analysis; and 3) Report the results, including a description of the missing data, details on how the missing data were addressed, and the results from all analyses, interpreted in light of the missing data and the clinical relevance. This framework seeks to support researchers in thinking systematically about missing data and transparently reporting the potential effect on the study results, therefore increasing the confidence in and reproducibility of research findings.},
  langid = {english},
  keywords = {ALSPAC,Missing data,Multiple imputation,Observational studies,Reporting,STRATOS initiative},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\BNUPS9A8\\Lee et al. - 2021 - Framework for the treatment and reporting of missi.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\T942MSMF\\S089543562100010X.html}
}

@article{leeMultipleImputationPresence2017,
  title = {Multiple Imputation in the Presence of Non-Normal Data},
  author = {Lee, Katherine J. and Carlin, John B.},
  year = {2017},
  journal = {Statistics in Medicine},
  volume = {36},
  number = {4},
  pages = {606--617},
  issn = {1097-0258},
  doi = {10.1002/sim.7173},
  urldate = {2020-12-23},
  abstract = {Multiple imputation (MI) is becoming increasingly popular for handling missing data. Standard approaches for MI assume normality for continuous variables (conditionally on the other variables in the imputation model). However, it is unclear how to impute non-normally distributed continuous variables. Using simulation and a case study, we compared various transformations applied prior to imputation, including a novel non-parametric transformation, to imputation on the raw scale and using predictive mean matching (PMM) when imputing non-normal data. We generated data from a range of non-normal distributions, and set 50\% to missing completely at random or missing at random. We then imputed missing values on the raw scale, following a zero-skewness log, Box--Cox or non-parametric transformation and using PMM with both type 1 and 2 matching. We compared inferences regarding the marginal mean of the incomplete variable and the association with a fully observed outcome. We also compared results from these approaches in the analysis of depression and anxiety symptoms in parents of very preterm compared with term-born infants. The results provide novel empirical evidence that the decision regarding how to impute a non-normal variable should be based on the nature of the relationship between the variables of interest. If the relationship is linear in the untransformed scale, transformation can introduce bias irrespective of the transformation used. However, if the relationship is non-linear, it may be important to transform the variable to accurately capture this relationship. A useful alternative is to impute the variable using PMM with type 1 matching. Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {missing data,multiple imputation,non-normal data,predictive mean matching,transformation}
}

@article{leeMultipleImputationPresence2017a,
  title = {Multiple Imputation in the Presence of Non-Normal Data},
  author = {Lee, Katherine J. and Carlin, John B.},
  year = {2017},
  journal = {Statistics in Medicine},
  volume = {36},
  number = {4},
  pages = {606--617},
  issn = {1097-0258},
  doi = {10.1002/sim.7173},
  urldate = {2024-10-22},
  abstract = {Multiple imputation (MI) is becoming increasingly popular for handling missing data. Standard approaches for MI assume normality for continuous variables (conditionally on the other variables in the imputation model). However, it is unclear how to impute non-normally distributed continuous variables. Using simulation and a case study, we compared various transformations applied prior to imputation, including a novel non-parametric transformation, to imputation on the raw scale and using predictive mean matching (PMM) when imputing non-normal data. We generated data from a range of non-normal distributions, and set 50\% to missing completely at random or missing at random. We then imputed missing values on the raw scale, following a zero-skewness log, Box--Cox or non-parametric transformation and using PMM with both type 1 and 2 matching. We compared inferences regarding the marginal mean of the incomplete variable and the association with a fully observed outcome. We also compared results from these approaches in the analysis of depression and anxiety symptoms in parents of very preterm compared with term-born infants. The results provide novel empirical evidence that the decision regarding how to impute a non-normal variable should be based on the nature of the relationship between the variables of interest. If the relationship is linear in the untransformed scale, transformation can introduce bias irrespective of the transformation used. However, if the relationship is non-linear, it may be important to transform the variable to accurately capture this relationship. A useful alternative is to impute the variable using PMM with type 1 matching. Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {missing data,multiple imputation,non-normal data,predictive mean matching,transformation},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\E8FVGFQB\\Lee and Carlin - 2017 - Multiple imputation in the presence of non-normal .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\UTM7KAE7\\sim.html}
}

@article{lennonObesityParadoxCancer2016,
  title = {The {{Obesity Paradox}} in {{Cancer}}: A {{Review}}},
  shorttitle = {The {{Obesity Paradox}} in {{Cancer}}},
  author = {Lennon, Hannah and Sperrin, Matthew and Badrick, Ellena and Renehan, Andrew G.},
  year = {2016},
  month = jul,
  journal = {Current Oncology Reports},
  volume = {18},
  number = {9},
  pages = {56},
  issn = {1534-6269},
  doi = {10.1007/s11912-016-0539-4},
  urldate = {2024-10-22},
  abstract = {There is a common perception that excess adiposity, commonly approximated by body mass index (BMI), is associated with reduced cancer survival. A number of studies have emerged challenging this by demonstrating that overweight and early obese states are associated with improved survival. This finding is termed the ``obesity paradox'' and is well recognized in the cardio-metabolic literature but less so in oncology. Here, we summarize the epidemiological findings related to the obesity paradox in cancer. Our review highlights that many observations of the obesity paradox in cancer reflect methodological mechanisms including the crudeness of BMI as an obesity measure, confounding, detection bias, reverse causality, and a specific form of the selection bias, known as collider bias. It is imperative for the oncologist to interpret the observation of the obesity paradox against the above methodological framework and avoid the misinterpretation that being obese might be ``good'' or ``protective'' for cancer patients.},
  langid = {english},
  keywords = {Adiposity,BMI,Body mass index,Cancer,Cancer survival,Epidemiology,Excess weight,Mortality,Obesity,Overweight,Prognosis},
  file = {C:\Users\efbonneville\Zotero\storage\EC2KJ2AA\Lennon et al. - 2016 - The Obesity Paradox in Cancer a Review.pdf}
}

@article{lewalleDonorLymphocyteInfusions2003,
  title = {Donor Lymphocyte Infusions in Adult Haploidentical Transplant: A Dose Finding Study},
  shorttitle = {Donor Lymphocyte Infusions in Adult Haploidentical Transplant},
  author = {Lewalle, P. and Triffet, A. and Delforge, A. and Crombez, P. and Selleslag, D. and De Muynck, H. and Bron, D. and Martiat, P.},
  year = {2003},
  month = jan,
  journal = {Bone Marrow Transplantation},
  volume = {31},
  number = {1},
  pages = {39--44},
  publisher = {Nature Publishing Group},
  issn = {1476-5365},
  doi = {10.1038/sj.bmt.1703779},
  urldate = {2023-11-04},
  abstract = {Haploidentical transplantation has become a clinical option for patients lacking a compatible donor. However, patients are still referred at advanced stages and are usually heavily pretreated. This results in a high risk of toxicity, relapses and infections. We therefore started a donor lymphocyte infusion (DLI) dose-finding protocol, to try to improve both relapse rate and immunity reconstitution. In all, 12 consecutive patients were investigated. All had a refractory, some progressive, disease. Conditioning consisted of TBI, melphalan, ATG, fludarabine and CSA pretransplant. In four rapidly progressive patients, Ara-C had to be given 1 week preconditioning. The graft was T- and B-cell depleted with a fixed reinfused CD3 dose of 5{\texttimes}104/kg. All patients engrafted before day 20. G-CSF was given from day 5 post-transplant and replaced with GM-CSF in the last three patients. Nonrelapse related mortality was 0/12 at 1 year. DLI were started at day 28 (3{\texttimes}104\,CD3/kg) in the two first patients. This resulted in acute graft-versus-host disease (aGVHD) and chronic graft-versus-host disease (cGVHD) in both, but they did not relapse. The next dose was 1{\texttimes}104/kg monthly for 3 months. This was well tolerated with only one grade I GVHD. Given the high relapse rate, we escalated doses (1, 3 and 10{\texttimes}104/kg). This produced GVHD in all. We next moved, to GM-CSF and 1{\texttimes}104 CD3/kg monthly. Overall, 6/12 patients relapsed and received therapeutic DLI, starting at 1{\texttimes}105 CD3/kg with escalation every 2 weeks. We conclude that prophylactic DLI are feasible in adult haploidentical transplantation, without GVHD at a monthly dose of 1{\texttimes}104 CD3/kg. They result in faster CD4 recovery and a low rate of infections. The impact of GM-CSF remains to be further investigated. This scheme seems ideal for patients transplanted early in the course of their disease. In very bad prognosis patients, it remains insufficient to rapidly induce a GVL effect. Escalated doses are feasible but the price is aGVHD. Therapeutic DLI can be given at higher doses, depending on the time post-transplant. Haploidentical transplantation with low-dose DLI is a safe procedure that should be considered in all patients needing a transplant, but lacking a matched donor, early in the course of the disease.},
  copyright = {2003 Springer Nature Limited},
  langid = {english},
  keywords = {Cell Biology,general,Hematology,Internal Medicine,Medicine/Public Health,Public Health,Stem Cells},
  file = {C:\Users\efbonneville\Zotero\storage\6UE7LAK6\Lewalle et al. - 2003 - Donor lymphocyte infusions in adult haploidentical.pdf}
}

@article{liCheckingFineGray2015,
  title = {Checking {{Fine}} and {{Gray Subdistribution Hazards Model}} with {{Cumulative Sums}} of {{Residuals}}},
  author = {Li, Jianing and Scheike, Thomas H. and Zhang, Mei-Jie},
  year = {2015},
  month = apr,
  journal = {Lifetime Data Analysis},
  volume = {21},
  number = {2},
  pages = {197--217},
  issn = {1380-7870},
  doi = {10.1007/s10985-014-9313-9},
  urldate = {2021-05-10},
  abstract = {Recently,  proposed a semi-parametric proportional regression model for the subdistribution hazard function which has been used extensively for analyzing competing risks data. However, failure of model adequacy could lead to severe bias in parameter estimation, and only a limited contribution has been made to check the model assumptions. In this paper, we present a class of analytical methods and graphical approaches for checking the assumptions of Fine and Gray's model. The proposed goodness-of-fit test procedures are based on the cumulative sums of residuals, which validate the model in three aspects: (1) proportionality of hazard ratio, (2) the linear functional form and (3) the link function. For each assumption testing, we provide a p-values and a visualized plot against the null hypothesis using a simulation-based approach. We also consider an omnibus test for overall evaluation against any model misspecification. The proposed tests perform well in simulation studies and are illustrated with two real data examples.},
  pmcid = {PMC4386671},
  pmid = {25421251}
}

@article{little1987statistical,
  title = {Statistical Analysis with Missing Data},
  author = {Little, Roderick JA and Rubin, Donald B},
  year = {1987},
  journal = {New York: Wiley}
}

@article{littleComparisonThreePopular2024,
  title = {A {{Comparison}} of {{Three Popular Methods}} for {{Handling Missing Data}}: {{Complete-Case Analysis}}, {{Inverse Probability Weighting}}, and {{Multiple Imputation}}},
  shorttitle = {A {{Comparison}} of {{Three Popular Methods}} for {{Handling Missing Data}}},
  author = {Little, Roderick J. and Carpenter, James R. and Lee, Katherine J.},
  year = {2024},
  month = aug,
  journal = {Sociological Methods \& Research},
  volume = {53},
  number = {3},
  pages = {1105--1135},
  publisher = {SAGE Publications Inc},
  issn = {0049-1241},
  doi = {10.1177/00491241221113873},
  urldate = {2024-08-12},
  abstract = {Missing data are a pervasive problem in data analysis. Three common methods for addressing the problem are (a) complete-case analysis, where only units that are complete on the variables in an analysis are included; (b) weighting, where the complete cases are weighted by the inverse of an estimate of the probability of being complete; and (c) multiple imputation (MI), where missing values of the variables in the analysis are imputed as draws from their predictive distribution under an implicit or explicit statistical model, the imputation process is repeated to create multiple filled-in data sets, and analysis is carried out using simple MI combining rules. This article provides a non-technical discussion of the strengths and weakness of these approaches, and when each of the methods might be adopted over the others. The methods are illustrated on data from the Youth Cohort (Time) Series (YCS) for England, Wales and Scotland, 1984--2002.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\4NV6ISBS\Little et al. - 2024 - A Comparison of Three Popular Methods for Handling.pdf}
}

@article{littleMissingDataAnalysis2024,
  title = {Missing {{Data Analysis}}},
  author = {Little, Roderick J.},
  year = {2024},
  month = jul,
  journal = {Annual Review of Clinical Psychology},
  volume = {20},
  number = {Volume 20, 2024},
  pages = {149--173},
  publisher = {Annual Reviews},
  issn = {1548-5943, 1548-5951},
  doi = {10.1146/annurev-clinpsy-080822-051727},
  urldate = {2024-08-20},
  abstract = {Methods for handling missing data in clinical psychology studies are reviewed. Missing data are defined, and a taxonomy of main approaches to analysis is presented, including complete-case and available-case analysis, weighting, maximum likelihood, Bayes, single and multiple imputation, and augmented inverse probability weighting. Missingness mechanisms, which play a key role in the performance of alternative methods, are defined. Approaches to robust inference, and to inference when the mechanism is potentially missing not at random, are discussed.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\7MBZLUC9\annurev-clinpsy-080822-051727.html}
}

@article{littleMissingDataAssumptions2021,
  title = {Missing {{Data Assumptions}}},
  author = {Little, Roderick J.},
  year = {2021},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {8},
  number = {Volume 8, 2021},
  pages = {89--107},
  publisher = {Annual Reviews},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-040720-031104},
  urldate = {2024-09-04},
  abstract = {I review assumptions about the missing-data mechanisms that underlie methods for the statistical analysis of data with missing values. I describe Rubin\&apos;s original definition of missing at random (MAR), its motivation and criticisms, and his sufficient conditions for ignoring the missingness mechanism for likelihood-based, Bayesian, and frequentist inference. Related definitions, including missing completely at random, always MAR, always missing completely at random, and partially MAR, are also covered. I present a formal argument for weakening Rubin\&apos;s sufficient conditions for frequentist maximum likelihood inference with precision based on the observed information. Some simple examples of MAR are described, together with an example where the missingness mechanism can be ignored even though MAR does not hold. Alternative approaches to statistical inference based on the likelihood function are reviewed, along with non-likelihood frequentist approaches, including weighted generalized estimating equations. Connections with the causal inference literature are also discussed. Finally, alternatives to Rubin\&apos;s MAR definition are discussed, including informative missingness, informative censoring, and coarsening at random. The intent is to provide a relatively nontechnical discussion, although some of the underlying issues are challenging and touch on fundamental questions of statistical inference.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\JI47473U\annurev-statistics-040720-031104.html}
}

@article{littleModelingDropOutMechanism1995,
  title = {Modeling the {{Drop-Out Mechanism}} in {{Repeated-Measures Studies}}},
  author = {Little, Roderick J. A.},
  year = {1995},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {90},
  number = {431},
  pages = {1112--1121},
  publisher = {ASA Website},
  issn = {0162-1459},
  doi = {10.1080/01621459.1995.10476615},
  urldate = {2024-08-23},
  keywords = {Attrition,Longitudinal data,Missing data,Nonrandom nonresponse,Selection bias},
  file = {C:\Users\efbonneville\Zotero\storage\L8G8QRK6\Little - 1995 - Modeling the Drop-Out Mechanism in Repeated-Measur.pdf}
}

@book{littleStatisticalAnalysisMissing2019,
  title = {Statistical {{Analysis}} with {{Missing Data}}},
  author = {Little, Roderick J. A. and Rubin, Donald B.},
  year = {2019},
  month = apr,
  publisher = {John Wiley \& Sons},
  abstract = {An up-to-date, comprehensive treatment of a classic text on missing data in statisticsThe topic of missing data has gained considerable attention in recent decades. This new edition by two acknowledged experts on the subject offers an up-to-date account of practical methodology for handling missing data problems. Blending theory and application, authors Roderick Little and Donald Rubin review historical approaches to the subject and describe simple methods for multivariate analysis with missing values. They then provide a coherent theory for analysis of problems based on likelihoods derived from statistical models for the data and the missing data mechanism, and then they apply the theory to a wide range of important missing data problems.Statistical Analysis with Missing Data, Third Edition starts by introducing readers to the subject and approaches toward solving it. It looks at the patterns and mechanisms that create the missing data, as well as a taxonomy of missing data. It then goes on to examine missing data in experiments, before discussing complete-case and available-case analysis, including weighting methods. The new edition expands its coverage to include recent work on topics such as nonresponse in sample surveys, causal inference, diagnostic methods, and sensitivity analysis, among a host of other topics.  An updated ``classic'' written by renowned authorities on the subject Features over 150 exercises (including many new ones) Covers recent work on important methods like multiple imputation, robust alternatives to weighting, and Bayesian methods Revises previous topics based on past student feedback and class experience Contains an updated and expanded bibliography  The authors were awarded The Karl Pearson Prize in 2017 by the International Statistical Institute, for a research contribution that has had profound influence on statistical theory, methodology or applications. Their work "has been no less than defining and transforming." (ISI)Statistical Analysis with Missing Data, Third Edition is an ideal textbook for upper undergraduate and/or beginning graduate level students of the subject. It is also an excellent source of information for applied statisticians and practitioners in government and industry.},
  googlebooks = {BemMDwAAQBAJ},
  isbn = {978-0-470-52679-8},
  langid = {english},
  keywords = {Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes}
}

@article{liuStationaryDistributionIterative2014,
  title = {On the Stationary Distribution of Iterative Imputations},
  author = {Liu, Jingchen and Gelman, Andrew and Hill, Jennifer and Su, Yu-Sung and Kropko, Jonathan},
  year = {2014},
  journal = {Biometrika},
  volume = {101},
  number = {1},
  eprint = {43305601},
  eprinttype = {jstor},
  pages = {155--173},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  urldate = {2024-08-23},
  abstract = {Iterative imputation, in which variables are imputed one at a time conditional on all the others, is a popular technique that can be convenient and flexible, as it replaces a potentially difficult multivariate modelling problem with relatively simple univariate regressions. In this paper, we begin to characterize the stationary distributions of iterative imputations and their statistical properties, accounting for the conditional models being iteratively estimated from data rather than being prespecified. When the families of conditional models are compatible, we provide sufficient conditions under which the imputation distribution converges in total variation to the posterior distribution of a Bayesian model. When the conditional models are incompatible but valid, we show that the combined imputation estimator is consistent.},
  file = {C:\Users\efbonneville\Zotero\storage\3UMGLTXW\Liu et al. - 2014 - On the stationary distribution of iterative imputa.pdf}
}

@article{lohmannItsTimeTen2022,
  title = {It's Time! {{Ten}} Reasons to Start Replicating Simulation Studies},
  author = {Lohmann, Anna and Astivia, Oscar L. O. and Morris, Tim P. and Groenwold, Rolf H. H.},
  year = {2022},
  month = sep,
  journal = {Frontiers in Epidemiology},
  volume = {2},
  publisher = {Frontiers},
  issn = {2674-1199},
  doi = {10.3389/fepid.2022.973470},
  urldate = {2024-10-22},
  abstract = {{$<$}p{$>$}The quantitative analysis of research data is a core element of empirical research. The performance of statistical methods that are used for analyzing empirical data can be evaluated and compared using computer simulations. A single simulation study can influence the analyses of thousands of empirical studies to follow. With great power comes great responsibility. Here, we argue that this responsibility includes replication of simulation studies to ensure a sound foundation for data analytical decisions. Furthermore, being designed, run, and reported by humans, simulation studies face challenges similar to other experimental empirical research and hence should not be exempt from replication attempts. We highlight that the potential replicability of simulation studies is an opportunity quantitative methodology as a field should pay more attention to.{$<$}/p{$>$}},
  langid = {english},
  keywords = {data analysis,Replication,Reproduction,Research statistics,simulation study},
  file = {C:\Users\efbonneville\Zotero\storage\N58ISNV9\Lohmann et al. - 2022 - It's time! Ten reasons to start replicating simula.pdf}
}

@article{luLogisticRegressionAnalysis2017,
  title = {On Logistic Regression Analysis of Dichotomized Responses},
  author = {Lu, Kaifeng},
  year = {2017},
  journal = {Pharmaceutical Statistics},
  volume = {16},
  number = {1},
  pages = {55--63},
  issn = {1539-1612},
  doi = {10.1002/pst.1777},
  urldate = {2021-09-07},
  abstract = {We study the properties of treatment effect estimate in terms of odds ratio at the study end point from logistic regression model adjusting for the baseline value when the underlying continuous repeated measurements follow a multivariate normal distribution. Compared with the analysis that does not adjust for the baseline value, the adjusted analysis produces a larger treatment effect as well as a larger standard error. However, the increase in standard error is more than offset by the increase in treatment effect so that the adjusted analysis is more powerful than the unadjusted analysis for detecting the treatment effect. On the other hand, the true adjusted odds ratio implied by the normal distribution of the underlying continuous variable is a function of the baseline value and hence is unlikely to be able to be adequately represented by a single value of adjusted odds ratio from the logistic regression model. In contrast, the risk difference function derived from the logistic regression model provides a reasonable approximation to the true risk difference function implied by the normal distribution of the underlying continuous variable over the range of the baseline distribution. We show that different metrics of treatment effect have similar statistical power when evaluated at the baseline mean. Copyright {\copyright} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {binary response,odds ratio,relative risk,risk difference}
}

@article{mackinnonUseReportingMultiple2010,
  title = {The Use and Reporting of Multiple Imputation in Medical Research -- a Review},
  author = {Mackinnon, A.},
  year = {2010},
  journal = {Journal of Internal Medicine},
  volume = {268},
  number = {6},
  pages = {586--593},
  issn = {1365-2796},
  doi = {10.1111/j.1365-2796.2010.02274.x},
  urldate = {2022-10-10},
  abstract = {Abstract. Mackinnon A (Centre for Youth Mental Health, University of Melbourne, Parkville, Victoria, Australia) The use and reporting of multiple imputation in medical research -- a review. J Intern Med 2010; 268: 586--593. Background. Multiple imputation (MI) is an advanced, principled method of dealing with missing data in statistical analyses, a common problem in medical research. This paper sought to document the use of MI in general medical journals and to evaluate the information provided to readers about the application of the procedure in studies. Methods. Research articles using MI in analyses published in JAMA, New England Journal of Medicine, BMJ and the Lancet were identified using full text searches from the earliest date each journal offered such searches until the end of 2008. Ninety-nine articles were found. Studies were classified according to their design. Results. Multiple imputation was used in 49 RCTs and 50 other types of studies. A third of the articles (n = 33) reported no details of the procedure used. In a third of these (n = 11), it was not possible to infer the approach used from references cited or software used. The nature of the imputation model was rarely reported. MI was frequently used as a secondary analysis (n = 40) either to justify reporting a simpler approach or as a form of sensitivity analysis. Conclusions. Whilst still relatively uncommon, the use of MI has risen substantially, particularly in trials. MI is rarely adequately reported, leading to doubt about its appropriateness in some cases. This gives rise to uncertainty about conclusions reached and poses a barrier to attempts to replicate analyses. Guidelines for the reporting of MI should be developed.},
  langid = {english},
  keywords = {biostatistics,missing data handling,multiple imputation,randomized controlled trials,reporting standards},
  file = {C:\Users\efbonneville\Zotero\storage\M5YTVNYG\Mackinnon - 2010 - The use and reporting of multiple imputation in me.pdf}
}

@article{madley-dowdProportionMissingData2019,
  title = {The Proportion of Missing Data Should Not Be Used to Guide Decisions on Multiple Imputation},
  author = {{Madley-Dowd}, Paul and Hughes, Rachael and Tilling, Kate and Heron, Jon},
  year = {2019},
  month = jun,
  journal = {Journal of Clinical Epidemiology},
  volume = {110},
  pages = {63--73},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2019.02.016},
  urldate = {2020-12-04},
  abstract = {Objectives Researchers are concerned whether multiple imputation (MI) or complete case analysis should be used when a large proportion of data are missing. We aimed to provide guidance for drawing conclusions from data with a large proportion of missingness. Study Design and Setting Via simulations, we investigated how the proportion of missing data, the fraction of missing information (FMI), and availability of auxiliary variables affected MI performance. Outcome data were missing completely at random or missing at random (MAR). Results Provided sufficient auxiliary information was available; MI was beneficial in terms of bias and never detrimental in terms of efficiency. Models with similar FMI values, but differing proportions of missing data, also had similar precision for effect estimates. In the absence of bias, the FMI was a better guide to the efficiency gains using MI than the proportion of missing data. Conclusion We provide evidence that for MAR data, valid MI reduces bias even when the proportion of missingness is large. We advise researchers to use FMI to guide choice of auxiliary variables for efficiency gain in imputation analyses, and that sensitivity analyses including different imputation models may be needed if the number of complete cases is small.},
  langid = {english},
  keywords = {ALSPAC,Bias,Methods,Missing data,Multiple imputation,Simulation}
}

@article{mainzerGapsUsageReporting2024,
  title = {Gaps in the Usage and Reporting of Multiple Imputation for Incomplete Data: Findings from a Scoping Review of Observational Studies Addressing Causal Questions},
  shorttitle = {Gaps in the Usage and Reporting of Multiple Imputation for Incomplete Data},
  author = {Mainzer, Rheanna M. and {Moreno-Betancur}, Margarita and Nguyen, Cattram D. and Simpson, Julie A. and Carlin, John B. and Lee, Katherine J.},
  year = {2024},
  month = sep,
  journal = {BMC Medical Research Methodology},
  volume = {24},
  number = {1},
  pages = {193},
  issn = {1471-2288},
  doi = {10.1186/s12874-024-02302-6},
  urldate = {2024-10-02},
  abstract = {Missing data are common in observational studies and often occur in several of the variables required when estimating a causal effect, i.e. the exposure, outcome and/or variables used to control for confounding. Analyses involving multiple incomplete variables are not as straightforward as analyses with a single incomplete variable. For example, in the context of multivariable missingness, the standard missing data assumptions (``missing completely at random'', ``missing at random'' [MAR], ``missing not at random'') are difficult to interpret and assess. It is not clear how the complexities that arise due to multivariable missingness are being addressed in practice. The aim of this study was to review how missing data are managed and reported in observational studies that use multiple imputation (MI) for causal effect estimation, with a particular focus on missing data summaries, missing data assumptions, primary and sensitivity analyses, and MI implementation.},
  langid = {english},
  keywords = {Causal inference,Missing data,Missingness mechanism},
  file = {C:\Users\efbonneville\Zotero\storage\PP37IDWF\Mainzer et al. - 2024 - Gaps in the usage and reporting of multiple imputa.pdf}
}

@article{maoEfficientEstimationSemiparametric2017,
  title = {Efficient {{Estimation}} of {{Semiparametric Transformation Models}} for the {{Cumulative Incidence}} of {{Competing Risks}}},
  author = {Mao, Lu and Lin, D. Y.},
  year = {2017},
  month = mar,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {79},
  number = {2},
  pages = {573--587},
  issn = {1369-7412},
  doi = {10.1111/rssb.12177},
  urldate = {2023-07-11},
  abstract = {The cumulative incidence is the probability of failure from the cause of interest over a certain time period in the presence of other risks. A semiparametric regression model proposed by Fine and Gray has become the method of choice for formulating the effects of covariates on the cumulative incidence. Its estimation, however, requires modelling of the censoring distribution and is not statistically efficient. We present a broad class of semiparametric transformation models which extends the Fine and Gray model, and we allow for unknown causes of failure. We derive the non-parametric maximum likelihood estimators and develop simple and fast numerical algorithms using the profile likelihood. We establish the consistency, asymptotic normality and semiparametric efficiency of the non-parametric maximum likelihood estimators. In addition, we construct graphical and numerical procedures to evaluate and select models. Finally, we demonstrate the advantages of the proposed methods over the existing methods through extensive simulation studies and an application to a major study on bone marrow transplantation.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\68EZMWQP\\Mao and Lin - 2017 - Efficient Estimation of Semiparametric Transformat.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\2EZFELNM\\7041929.html}
}

@article{marianoAssessmentExtendedCoverageNextGeneration2019,
  title = {Assessment by {{Extended-Coverage Next-Generation Sequencing Typing}} of {{DPA1}} and {{DPB1 Mismatches}} in {{Siblings Matching}} at {{HLA-A}}, -{{B}}, -{{C}}, -{{DRB1}}, and -{{DQ Loci}}},
  author = {Mariano, Livia and Zhang, Bing Melody and Osoegawa, Kazutoyo and Lowsky, Robert and {Fernandez-Vina}, Marcelo},
  year = {2019},
  month = dec,
  journal = {Biology of Blood and Marrow Transplantation},
  volume = {25},
  number = {12},
  pages = {2507--2509},
  issn = {1083-8791},
  doi = {10.1016/j.bbmt.2019.07.033},
  urldate = {2023-11-04},
  abstract = {Allogeneic hematopoietic stem cell transplant from an HLA matched sibling donor is usually the preferable choice. The use of next-generation sequencing (NGS) for HLA typing in clinical practice provides broader coverage and higher resolution of HLA genes. We evaluated the frequency of DPB1 crossing-over events among patients and potential related donors typed with NGS. From July 2016 to January 2018, 593 patients and 2385 siblings were typed. We evaluated sibling matching status in 546 patients, and 44.8\% of these patients had siblings that matched at HLA-A, -B, -C, -DRB1, and -DQB1 loci. In 306 patient--HLA matched sibling pairs, we found 6 pairs (1.96\%) with 1 DPB1 mismatch, and 5 of these pairs included an additional mismatch in DPA1. No additional mismatches were observed at the low expression loci. Using the T cell epitope algorithm, 4 of these DP mismatches were classified as permissive, 1 as nonpermissive in the host-versus-graft direction, and 1 as nonpermissive in the graft-versus-host direction. The frequency of DPB1 and DPA1 mismatches is low, and their impact in related donor transplants is not well established. Although DP typing in related transplants goes beyond guidelines, it is especially relevant for sensitized patients. NGS-based HLA typing provides full gene coverage, and its use in clinical practice can enable better donor selection.},
  keywords = {Allogeneic stem cell transplant,DPB1 mismatch,HLA,Related donor selection},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\4BVTMRY7\\Mariano et al. - 2019 - Assessment by Extended-Coverage Next-Generation Se.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\KCFLKL3C\\S1083879119305051.html}
}

@article{mariniMaximumLikelihoodEstimationPanel1980,
  title = {Maximum-{{Likelihood Estimation}} in {{Panel Studies}} with {{Missing Data}}},
  author = {Marini, Margaret Mooney and Olsen, Anthony R. and Rubin, Donald B.},
  year = {1980},
  journal = {Sociological Methodology},
  volume = {11},
  eprint = {270868},
  eprinttype = {jstor},
  pages = {314--357},
  publisher = {[American Sociological Association, Wiley, Sage Publications, Inc.]},
  issn = {0081-1750},
  doi = {10.2307/270868},
  urldate = {2024-08-22},
  file = {C:\Users\efbonneville\Zotero\storage\ZQ3MLM9S\Marini et al. - 1980 - Maximum-Likelihood Estimation in Panel Studies wit.pdf}
}

@article{marshallCombiningEstimatesInterest2009,
  title = {Combining Estimates of Interest in Prognostic Modelling Studies after Multiple Imputation: Current Practice and Guidelines},
  shorttitle = {Combining Estimates of Interest in Prognostic Modelling Studies after Multiple Imputation},
  author = {Marshall, Andrea and Altman, Douglas G. and Holder, Roger L. and Royston, Patrick},
  year = {2009},
  month = jul,
  journal = {BMC Medical Research Methodology},
  volume = {9},
  number = {1},
  pages = {57},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-9-57},
  urldate = {2020-12-16},
  abstract = {Multiple imputation (MI) provides an effective approach to handle missing covariate data within prognostic modelling studies, as it can properly account for the missing data uncertainty. The multiply imputed datasets are each analysed using standard prognostic modelling techniques to obtain the estimates of interest. The estimates from each imputed dataset are then combined into one overall estimate and variance, incorporating both the within and between imputation variability. Rubin's rules for combining these multiply imputed estimates are based on asymptotic theory. The resulting combined estimates may be more accurate if the posterior distribution of the population parameter of interest is better approximated by the normal distribution. However, the normality assumption may not be appropriate for all the parameters of interest when analysing prognostic modelling studies, such as predicted survival probabilities and model performance measures.},
  keywords = {Imputation Model,Impute Dataset,Likelihood Ratio Statistic,Multiple Imputation,Prognostic Modelling}
}

@article{marshallComparisonImputationMethods2010,
  title = {Comparison of Imputation Methods for Handling Missing Covariate Data When Fitting a {{Cox}} Proportional Hazards Model: A Resampling Study},
  shorttitle = {Comparison of Imputation Methods for Handling Missing Covariate Data When Fitting a {{Cox}} Proportional Hazards Model},
  author = {Marshall, Andrea and Altman, Douglas G. and Holder, Roger L.},
  year = {2010},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {10},
  number = {1},
  pages = {112},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-10-112},
  urldate = {2022-10-10},
  abstract = {The appropriate handling of missing covariate data in prognostic modelling studies is yet to be conclusively determined. A resampling study was performed to investigate the effects of different missing data methods on the performance of a prognostic model.},
  keywords = {Complete Case,Multiple Imputation,Regression Coefficient Estimate,Regression Switching,Single Imputation},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\JF4AYBPW\\Marshall et al. - 2010 - Comparison of imputation methods for handling miss.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\XFITCS4T\\1471-2288-10-112.html}
}

@article{martinussenEstimationSeparableDirect2023,
  title = {Estimation of Separable Direct and Indirect Effects in Continuous Time},
  author = {Martinussen, Torben and Stensrud, Mats Julius},
  year = {2023},
  journal = {Biometrics},
  volume = {79},
  number = {1},
  pages = {127--139},
  issn = {1541-0420},
  doi = {10.1111/biom.13559},
  urldate = {2024-06-18},
  abstract = {Many research questions involve time-to-event outcomes that can be prevented from occurring due to competing events. In these settings, we must be careful about the causal interpretation of classical statistical estimands. In particular, estimands on the hazard scale, such as ratios of cause-specific or subdistribution hazards, are fundamentally hard to interpret causally. Estimands on the risk scale, such as contrasts of cumulative incidence functions, do have a clear causal interpretation, but they only capture the total effect of the treatment on the event of interest; that is, effects both through and outside of the competing event. To disentangle causal treatment effects on the event of interest and competing events, the separable direct and indirect effects were recently introduced. Here we provide new results on the estimation of direct and indirect separable effects in continuous time. In particular, we derive the nonparametric influence function in continuous time and use it to construct an estimator that has certain robustness properties. We also propose a simple estimator based on semiparametric models for the two cause-specific hazard functions. We describe the asymptotic properties of these estimators and present results from simulation studies, suggesting that the estimators behave satisfactorily in finite samples. Finally, we reanalyze the prostate cancer trial from Stensrud et al. (2020).},
  copyright = {{\copyright} 2021 The International Biometric Society.},
  langid = {english},
  keywords = {competing events,hazard functions,influence function,separable effects,survival analysis},
  file = {C:\Users\efbonneville\Zotero\storage\JJ56X3QE\Martinussen and Stensrud - 2023 - Estimation of separable direct and indirect effect.pdf}
}

@article{martinussenEstimationSeparableDirect2023a,
  title = {Estimation of Separable Direct and Indirect Effects in Continuous Time},
  author = {Martinussen, Torben and Stensrud, Mats Julius},
  year = {2023},
  journal = {Biometrics},
  volume = {79},
  number = {1},
  pages = {127--139},
  issn = {1541-0420},
  doi = {10.1111/biom.13559},
  urldate = {2024-08-22},
  abstract = {Many research questions involve time-to-event outcomes that can be prevented from occurring due to competing events. In these settings, we must be careful about the causal interpretation of classical statistical estimands. In particular, estimands on the hazard scale, such as ratios of cause-specific or subdistribution hazards, are fundamentally hard to interpret causally. Estimands on the risk scale, such as contrasts of cumulative incidence functions, do have a clear causal interpretation, but they only capture the total effect of the treatment on the event of interest; that is, effects both through and outside of the competing event. To disentangle causal treatment effects on the event of interest and competing events, the separable direct and indirect effects were recently introduced. Here we provide new results on the estimation of direct and indirect separable effects in continuous time. In particular, we derive the nonparametric influence function in continuous time and use it to construct an estimator that has certain robustness properties. We also propose a simple estimator based on semiparametric models for the two cause-specific hazard functions. We describe the asymptotic properties of these estimators and present results from simulation studies, suggesting that the estimators behave satisfactorily in finite samples. Finally, we reanalyze the prostate cancer trial from Stensrud et al. (2020).},
  copyright = {{\copyright} 2021 The International Biometric Society.},
  langid = {english},
  keywords = {competing events,hazard functions,influence function,separable effects,survival analysis},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\HIBF7L3S\\Martinussen and Stensrud - 2023 - Estimation of separable direct and indirect effect.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\7W96JZK5\\biom.html}
}

@article{maurerDelicateBalanceGraft2023,
  title = {The Delicate Balance of Graft versus Leukemia and Graft versus Host Disease after Allogeneic Hematopoietic Stem Cell Transplantation},
  author = {Maurer, Katie and Soiffer, Robert J.},
  year = {2023},
  journal = {Expert Review of Hematology},
  volume = {0},
  number = {ja},
  publisher = {Taylor \& Francis},
  issn = {1747-4086},
  doi = {10.1080/17474086.2023.2273847},
  urldate = {2023-11-04},
  pmid = {37906445},
  keywords = {Chronic graft versus host disease,GvHD,GvL,HSCT,relapse},
  file = {C:\Users\efbonneville\Zotero\storage\L4G4L7EU\Maurer and Soiffer - 2023 - The delicate balance of graft versus leukemia and .pdf}
}

@book{mccullagh1989generalized,
  title = {Generalized Linear Models, Second Edition},
  author = {McCullagh, P. and Nelder, J.A.},
  year = {1989},
  series = {Chapman and {{Hall}}/{{CRC}} Monographs on Statistics and Applied Probability Series},
  publisher = {Chapman \& Hall},
  added-at = {2012-11-18T14:31:40.000+0100},
  biburl = {https://www.bibsonomy.org/bibtex/23a882b729f3f7333923cd713869d3f5f/peter.ralph},
  interhash = {1c22cedb7c518df1a6b0999d3f04f629},
  intrahash = {3a882b729f3f7333923cd713869d3f5f},
  isbn = {978-0-412-31760-6},
  lccn = {99013896},
  keywords = {GLM reference statistics},
  timestamp = {2012-11-18T14:31:40.000+0100}
}

@article{mccurdySignaturesGVHDRelapse2022,
  title = {Signatures of {{GVHD}} and Relapse after Posttransplant Cyclophosphamide Revealed by Immune Profiling and Machine Learning},
  author = {McCurdy, Shannon R. and Radojcic, Vedran and Tsai, Hua-Ling and Vulic, Ante and Thompson, Elizabeth and Ivcevic, Sanja and Kanakry, Christopher G. and Powell, Jonathan D. and Lohman, Brian and Adom, Djamilatou and Paczesny, Sophie and Cooke, Kenneth R. and Jones, Richard J. and Varadhan, Ravi and Symons, Heather J. and Luznik, Leo},
  year = {2022},
  month = jan,
  journal = {Blood},
  volume = {139},
  number = {4},
  pages = {608--623},
  issn = {0006-4971},
  doi = {10.1182/blood.2021013054},
  urldate = {2023-11-04},
  abstract = {The key immunologic signatures associated with clinical outcomes after posttransplant cyclophosphamide (PTCy)-based HLA-haploidentical (haplo) and HLA-matched bone marrow transplantation (BMT) are largely unknown. To address this gap in knowledge, we used machine learning to decipher clinically relevant signatures from immunophenotypic, proteomic, and clinical data and then examined transcriptome changes in the lymphocyte subsets that predicted major posttransplant outcomes. Kinetics of immune subset reconstitution after day 28 were similar for 70 patients undergoing haplo and 75 patients undergoing HLA-matched BMT. Machine learning based on 35 candidate factors (10 clinical, 18 cellular, and 7 proteomic) revealed that combined elevations in effector CD4+ conventional T cells (Tconv) and CXCL9 at day 28 predicted acute graft-versus-host disease (aGVHD). Furthermore, higher NK cell counts predicted improved overall survival (OS) due to a reduction in both nonrelapse mortality and relapse. Transcriptional and flow-cytometric analyses of recovering lymphocytes in patients with aGVHD identified preserved hallmarks of functional CD4+ regulatory T cells (Tregs) while highlighting a Tconv-driven inflammatory and metabolic axis distinct from that seen with conventional GVHD prophylaxis. Patients developing early relapse displayed a loss of inflammatory gene signatures in NK cells and a transcriptional exhaustion phenotype in CD8+ T cells. Using a multimodality approach, we highlight the utility of systems biology in BMT biomarker discovery and offer a novel understanding of how PTCy influences alloimmune responses. Our work charts future directions for novel therapeutic interventions after these increasingly used GVHD prophylaxis platforms. Specimens collected on NCT0079656226 and NCT0080927627 https://clinicaltrials.gov/.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\Q8NX3KWG\\McCurdy et al. - 2022 - Signatures of GVHD and relapse after posttransplan.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\AV5F455K\\Signatures-of-GVHD-and-relapse-after.html}
}

@article{mealliClarifyingMissingRandom2015,
  title = {Clarifying Missing at Random and Related Definitions, and Implications When Coupled with Exchangeability},
  author = {Mealli, Fabrizia and Rubin, Donald B.},
  year = {2015},
  month = dec,
  journal = {Biometrika},
  volume = {102},
  number = {4},
  pages = {995--1000},
  issn = {0006-3444},
  doi = {10.1093/biomet/asv035},
  urldate = {2024-08-22},
  abstract = {We clarify the key concept of missingness at random in incomplete data analysis. We first distinguish between data being missing at random and the missingness mechanism being a missing-at-random one, which we call missing always at random and which is more restrictive. We further discuss how, in general, neither of these conditions is a statement about conditional independence. We then consider the implication of the more restrictive missing-always-at-random assumption when coupled with full unit-exchangeability for the matrix of the variables of interest and the missingness indicators: the conditional distribution of the missingness indicators for any variable that can have a missing value can depend only on variables that are always fully observed. We discuss implications of this for modelling missingness mechanisms.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\AUCQ8N4T\\Mealli and Rubin - 2015 - Clarifying missing at random and related definitio.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\ZQP2FUFX\\236905.html}
}

@article{mengMultipleImputationInferencesUncongenial1994,
  title = {Multiple-{{Imputation Inferences}} with {{Uncongenial Sources}} of {{Input}}},
  author = {Meng, Xiao-Li},
  year = {1994},
  journal = {Statistical Science},
  volume = {9},
  number = {4},
  eprint = {2246252},
  eprinttype = {jstor},
  pages = {538--558},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237},
  urldate = {2024-09-04},
  abstract = {Conducting sample surveys, imputing incomplete observations, and analyzing the resulting data are three indispensable phases of modern practice with public-use data files and with many other statistical applications. Each phase inherits different input, including the information preceding it and the intellectual assessments available, and aims to provide output that is one step closer to arriving at statistical inferences with scientific relevance. However, the role of the imputation phase has often been viewed as merely providing computational convenience for users of data. Although facilitating computation is very important, such a viewpoint ignores the imputer's assessments and information inaccessible to the users. This view underlies the recent controversy over the validity of multiple-imputation inference when a procedure for analyzing multiply imputed data sets cannot be derived from (is "uncongenial" to) the model adopted for multiple imputation. Given sensible imputations and complete-data analysis procedures, inferences from standard multiple-imputation combining rules are typically superior to, and thus different from, users' incomplete-data analyses. The latter may suffer from serious nonresponse biases because such analyses often must rely on convenient but unrealistic assumptions about the nonresponse mechanism. When it is desirable to conduct inferences under models for nonresponse other than the original imputation model, a possible alternative to recreating imputations is to incorporate appropriate importance weights into the standard combining rules. These points are reviewed and explored by simple examples and general theory, from both Bayesian and frequentist perspectives, particularly from the randomization perspective. Some convenient terms are suggested for facilitating communication among researchers from different perspectives when evaluating multiple-imputation inferences with uncongenial sources of input.},
  file = {C:\Users\efbonneville\Zotero\storage\RSZNFTP4\Meng - 1994 - Multiple-Imputation Inferences with Uncongenial So.pdf}
}

@article{mengSimulatingTimetoeventData2023,
  title = {Simulating Time-to-Event Data Subject to Competing Risks and Clustering: {{A}} Review and Synthesis},
  shorttitle = {Simulating Time-to-Event Data Subject to Competing Risks and Clustering},
  author = {Meng, Can and Esserman, Denise and Li, Fan and Zhao, Yize and Blaha, Ondrej and Lu, Wenhan and Wang, Yuxuan and Peduzzi, Peter and Greene, Erich J.},
  year = {2023},
  month = feb,
  journal = {Statistical Methods in Medical Research},
  volume = {32},
  number = {2},
  pages = {305--333},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/09622802221136067},
  urldate = {2023-06-13},
  abstract = {Simulation studies play an important role in evaluating the performance of statistical models developed for analyzing complex survival data such as those with competing risks and clustering. This article aims to provide researchers with a basic understanding of competing risks data generation, techniques for inducing cluster-level correlation, and ways to combine them together in simulation studies, in the context of randomized clinical trials with a binary exposure or treatment. We review data generation with competing and semi-competing risks and three approaches of inducing cluster-level correlation for time-to-event data: the frailty model framework, the probability transform, and Moran's algorithm. Using exponentially distributed event times as an example, we discuss how to introduce cluster-level correlation into generating complex survival outcomes, and illustrate multiple ways of combining these methods to simulate clustered, competing and semi-competing risks data with pre-specified correlation values or degree of clustering.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\FQYU2E57\Meng et al. - 2023 - Simulating time-to-event data subject to competing.pdf}
}

@article{mertensConstructionAssessmentPrediction2020,
  title = {Construction and Assessment of Prediction Rules for Binary Outcome in the Presence of Missing Predictor Data Using Multiple Imputation and Cross-Validation: {{Methodological}} Approach and Data-Based Evaluation},
  shorttitle = {Construction and Assessment of Prediction Rules for Binary Outcome in the Presence of Missing Predictor Data Using Multiple Imputation and Cross-Validation},
  author = {Mertens, Bart J. A. and Banzato, Erika and {de Wreede}, Liesbeth C.},
  year = {2020},
  journal = {Biometrical Journal},
  volume = {62},
  number = {3},
  pages = {724--741},
  issn = {1521-4036},
  doi = {10.1002/bimj.201800289},
  urldate = {2020-10-21},
  abstract = {We investigate calibration and assessment of predictive rules when missing values are present in the predictors. Our paper has two key objectives. The first is to investigate how the calibration of the prediction rule can be combined with use of multiple imputation to account for missing predictor observations. The second objective is to propose such methods that can be implemented with current multiple imputation software, while allowing for unbiased predictive assessment through validation on new observations for which outcome is not yet available. We commence with a review of the methodological foundations of multiple imputation as a model estimation approach as opposed to a purely algorithmic description. We specifically contrast application of multiple imputation for parameter (effect) estimation with predictive calibration. Based on this review, two approaches are formulated, of which the second utilizes application of the classical Rubin's rules for parameter estimation, while the first approach averages probabilities from models fitted on single imputations to directly approximate the predictive density for future observations. We present implementations using current software that allow for validation and estimation of performance measures by cross-validation, as well as imputation of missing data in predictors on the future data where outcome is missing by definition. To simplify, we restrict discussion to binary outcome and logistic regression throughout. Method performance is verified through application on two real data sets. Accuracy (Brier score) and variance of predicted probabilities are investigated. Results show substantial reductions in variation of calibrated probabilities when using the first approach.},
  copyright = {{\copyright} 2020 The Authors. Biometrical Journal published by WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim.},
  langid = {english},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\9SAL9JLV\\Mertens et al. - 2020 - Construction and assessment of prediction rules fo.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\GNIJQZAW\\bimj.html}
}

@article{minculescuEarlyNaturalKiller2016,
  title = {Early {{Natural Killer Cell Reconstitution Predicts Overall Survival}} in {{T Cell}}--{{Replete Allogeneic Hematopoietic Stem Cell Transplantation}}},
  author = {Minculescu, Lia and Marquart, Hanne Vibeke and Friis, Lone Smidstrup and Petersen, Soeren Lykke and Schi{\o}dt, Ida and Ryder, Lars Peter and Andersen, Niels Smedegaard and Sengeloev, Henrik},
  year = {2016},
  month = dec,
  journal = {Biology of Blood and Marrow Transplantation},
  volume = {22},
  number = {12},
  pages = {2187--2193},
  issn = {1083-8791},
  doi = {10.1016/j.bbmt.2016.09.006},
  urldate = {2023-11-04},
  abstract = {Early immune reconstitution plays a critical role in clinical outcome after allogeneic hematopoietic stem cell transplantation (HSCT). Natural killer (NK) cells are the first lymphocytes to recover after transplantation and are considered powerful effector cells in HSCT. We aimed to evaluate the clinical impact of early NK cell recovery in T cell--replete transplant recipients. Immune reconstitution was studied in 298 adult patients undergoing HSCT for acute myeloid leukemia, acute lymphoblastic leukemia, and myelodysplastic syndrome from 2005 to 2013. In multivariate analysis NK cell numbers on day 30 (NK30)\,{$>$}\,150 cells/{\textmu}L were independently associated with superior overall survival (hazard ratio, .79; 95\% confidence interval, .66 to .95; P\,=\,.01). Cumulative incidence analyses showed that patients with NK30 {$>$} 150 cells/{\textmu}L had significantly less transplant-related mortality (TRM), P = .01. Patients with NK30 {$>$} 150 cells/{\textmu}L experienced significantly lower numbers of life-threatening bacterial infections as well as viral infections, including cytomegalovirus. No association was observed in relation to relapse. These results suggest an independent protective effect of high early NK cell reconstitution on TRM that translates into improved overall survival after T cell--replete HSCT.},
  keywords = {Allogeneic hematopoietic stem cell transplantation,HSCT,Immune reconstitution,Infections,Natural killer cells,T cell replete},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\LZNR782U\\Minculescu et al. - 2016 - Early Natural Killer Cell Reconstitution Predicts .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\QCI6C6SC\\S1083879116303329.html}
}

@book{molenberghsHandbookMissingData2014,
  title = {Handbook of {{Missing Data Methodology}}},
  author = {Molenberghs, Geert and Fitzmaurice, Garrett and Kenward, Michael G. and Tsiatis, Anastasios and Verbeke, Geert},
  year = {2014},
  month = nov,
  publisher = {CRC Press},
  abstract = {Missing data affect nearly every discipline by complicating the statistical analysis of collected data. But since the 1990s, there have been important developments in the statistical methodology for handling missing data. Written by renowned statisticians in this area, Handbook of Missing Data Methodology presents many methodological advances and t},
  googlebooks = {tqvMBQAAQBAJ},
  isbn = {978-1-4398-5462-4},
  langid = {english},
  keywords = {Mathematics / Probability & Statistics / General}
}

@incollection{molenberghsIncompleteDataIntroduction2008,
  title = {Incomplete Data: {{Introduction}} and Overview},
  shorttitle = {Incomplete Data},
  booktitle = {Longitudinal {{Data Analysis}}},
  author = {Molenberghs, Geert and Fitzmaurice, Garrett},
  year = {2008},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {Although most longitudinal studies are designed to collect data on every individual in the sample at each time of follow-up, many studies have some missing observations. With longitudinal studies problems of missing data are far more acute than in cross-sectional studies because non-response can occur at any occasion. The term ``non-response,'' as used in this context, denotes that intended observations are missing. An individual's response can be missing at one follow-up time and then be measured at a later follow-up time, resulting in a large number of distinct missingness patterns. At the same time, longitudinal studies often suffer from the problem of attrition or ``dropout,'' that is, some individuals ``drop out'' or withdraw from the study before its intended completion. In either case, the term ``missing data'' is used to indicate that intended measurements could not be obtained. Although missing data are ubiquitous in longitudinal studies when the study participants are human subjects, and much of the statistical literature has focused on these settings, similar problems with missing data also arise in longitudinal studies in the biological sciences, agriculture, and veterinary medicine.},
  isbn = {978-0-429-14267-3}
}

@article{monterrubio-gomezReviewStatisticalMachine2024,
  title = {A Review on Statistical and Machine Learning Competing Risks Methods},
  author = {{Monterrubio-G{\'o}mez}, Karla and {Constantine-Cooke}, Nathan and Vallejos, Catalina A.},
  year = {2024},
  journal = {Biometrical Journal},
  volume = {66},
  number = {2},
  pages = {2300060},
  issn = {1521-4036},
  doi = {10.1002/bimj.202300060},
  urldate = {2024-08-22},
  abstract = {When modeling competing risks (CR) survival data, several techniques have been proposed in both the statistical and machine learning literature. State-of-the-art methods have extended classical approaches with more flexible assumptions that can improve predictive performance, allow high-dimensional data and missing values, among others. Despite this, modern approaches have not been widely employed in applied settings. This article aims to aid the uptake of such methods by providing a condensed compendium of CR survival methods with a unified notation and interpretation across approaches. We highlight available software and, when possible, demonstrate their usage via reproducible R vignettes. Moreover, we discuss two major concerns that can affect benchmark studies in this context: the choice of performance metrics and reproducibility.},
  copyright = {{\copyright} 2024 The Authors. Biometrical Journal published by Wiley-VCH GmbH.},
  langid = {english},
  keywords = {competing risks,risk prediction,survival analysis,time-to-event data},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\DPWR9KNF\\Monterrubio-Gómez et al. - 2024 - A review on statistical and machine learning compe.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\FWPTZ5D9\\bimj.html}
}

@article{montez-rathAddressingMissingData2014,
  title = {Addressing {{Missing Data}} in {{Clinical Studies}} of {{Kidney Diseases}}},
  author = {{Montez-Rath}, Maria E. and Winkelmayer, Wolfgang C. and Desai, Manisha},
  year = {2014},
  month = jul,
  journal = {Clinical Journal of the American Society of Nephrology},
  volume = {9},
  number = {7},
  pages = {1328--1335},
  publisher = {American Society of Nephrology},
  issn = {1555-9041, 1555-905X},
  doi = {10.2215/CJN.10141013},
  urldate = {2022-10-10},
  abstract = {Missing data constitute a problem present in all studies of medical research. The most common approach to handling missing data---complete case analysis---relies on assumptions about missing data that rarely hold in practice. The implications of this approach are biased and inefficient descriptions of relationships of interest. Here, various approaches for handling missing data in clinical studies are described. In particular, this work promotes the use of multiple imputation methods that rely on assumptions about missingness that are more flexible than those assumptions relied on by the most common method in use. Furthermore, multiple imputation methods are becoming increasingly more accessible in mainstream statistical software packages, making them both a sound and practical choice. The use of multiple imputation methods is illustrated with examples pertinent to kidney research, and concrete guidance on their use is provided.},
  chapter = {Special Features},
  copyright = {Copyright {\copyright} 2014 by the American Society of Nephrology},
  langid = {english},
  pmid = {24509298},
  keywords = {biostatistics,epidemiology and outcomes,outcomes},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\MRJC9VMJ\\Montez-Rath et al. - 2014 - Addressing Missing Data in Clinical Studies of Kid.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\S6GRHFYK\\1328.html}
}

@article{moreno-betancurDirectLikelihoodInference2015,
  title = {Direct Likelihood Inference and Sensitivity Analysis for Competing Risks Regression with Missing Causes of Failure},
  author = {{Moreno-Betancur}, Margarita and Rey, Gr{\'e}goire and Latouche, Aur{\'e}lien},
  year = {2015},
  journal = {Biometrics},
  volume = {71},
  number = {2},
  pages = {498--507},
  issn = {1541-0420},
  doi = {10.1111/biom.12295},
  urldate = {2023-10-07},
  abstract = {Competing risks arise in the analysis of failure times when there is a distinction between different causes of failure. In many studies, it is difficult to obtain complete cause of failure information for all individuals. Thus, several authors have proposed strategies for semi-parametric modeling of competing risks when some causes of failure are missing under the missing at random (MAR) assumption. As many authors have stressed, while semi-parametric models are convenient, fully-parametric regression modeling of the cause-specific hazards (CSH) and cumulative incidence functions (CIF) may be of interest for prediction and is likely to contribute towards a fuller understanding of the time-dynamics of the competing risks mechanism. We propose a so-called ``direct likelihood'' approach for fitting fully-parametric regression models for these two functionals under MAR. The MAR assumption not being verifiable from the observed data, we propose an approach for performing sensitivity analyses to assess the robustness of inferences to departures from this assumption. The method relies on so-called ``pattern-mixture models'' from the missing data literature and was evaluated in a simulation study. This sensitivity analysis approach is applicable to various competing risks regression models (fully-parametric or semi-parametric, for the CSH or the CIF). We illustrate the proposed methods with the analysis of a breast cancer clinical trial, including suggestions for ad hoc graphical goodness-of-fit assessments under MAR.},
  copyright = {{\copyright} 2015, The International Biometric Society},
  langid = {english},
  keywords = {Cause-specific hazard,Competing risks,Cumulative incidence function,Missing at random,Missing cause of failure,Sensitivity analysis},
  file = {C:\Users\efbonneville\Zotero\storage\FEL35BZZ\Moreno-Betancur et al. - 2015 - Direct likelihood inference and sensitivity analys.pdf}
}

@article{morisotProstateCancerNet2015,
  title = {Prostate Cancer: Net Survival and Cause-Specific Survival Rates after Multiple Imputation},
  shorttitle = {Prostate Cancer},
  author = {Morisot, Adeline and Bessaoud, Fa{\"i}za and Landais, Paul and R{\'e}billard, Xavier and Tr{\'e}tarre, Brigitte and Daur{\`e}s, Jean-Pierre},
  year = {2015},
  month = jul,
  journal = {BMC Medical Research Methodology},
  volume = {15},
  number = {1},
  pages = {54},
  issn = {1471-2288},
  doi = {10.1186/s12874-015-0048-4},
  urldate = {2020-12-16},
  abstract = {Estimations of survival rates are diverse and the choice of the appropriate method depends on the context. Given the increasing interest in multiple imputation methods, we explored the interest of a multiple imputation approach in the estimation of cause-specific survival, when a subset of causes of death was observed.},
  keywords = {Cause-specific survival,ERSPC,Multiple imputation,Net survival}
}

@article{morrisUsingSimulationStudies2019,
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  year = {2019},
  journal = {Statistics in Medicine},
  volume = {38},
  number = {11},
  pages = {2074--2102},
  issn = {1097-0258},
  doi = {10.1002/sim.8086},
  urldate = {2020-04-29},
  abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some ``truth'' (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (``ADEMP''); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
  copyright = {{\copyright} 2019 The Authors. Statistics~in~Medicine Published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {graphics for simulation,Monte Carlo,simulation design,simulation reporting,simulation studies},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\2GVJIAS7\\Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\TXYCHCGB\\Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\SB24XQN6\\sim.html}
}

@article{mozumderDirectLikelihoodInference2018,
  title = {Direct Likelihood Inference on the Cause-Specific Cumulative Incidence Function: {{A}} Flexible Parametric Regression Modelling Approach},
  shorttitle = {Direct Likelihood Inference on the Cause-Specific Cumulative Incidence Function},
  author = {Mozumder, Sarwar Islam and Rutherford, Mark and Lambert, Paul},
  year = {2018},
  journal = {Statistics in Medicine},
  volume = {37},
  number = {1},
  pages = {82--97},
  issn = {1097-0258},
  doi = {10.1002/sim.7498},
  urldate = {2023-10-06},
  abstract = {In a competing risks analysis, interest lies in the cause-specific cumulative incidence function (CIF) that can be calculated by either (1) transforming on the cause-specific hazard or (2) through its direct relationship with the subdistribution hazard. We expand on current competing risks methodology from within the flexible parametric survival modelling framework (FPM) and focus on approach (2). This models all cause-specific CIFs simultaneously and is more useful when we look to questions on prognosis. We also extend cure models using a similar approach described by Andersson et al for flexible parametric relative survival models. Using SEER public use colorectal data, we compare and contrast our approach with standard methods such as the Fine \& Gray model and show that many useful out-of-sample predictions can be made after modelling the cause-specific CIFs using an FPM approach. Alternative link functions may also be incorporated such as the logit link. Models can also be easily extended for time-dependent effects.},
  copyright = {Copyright {\copyright} 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {competing risks,cumulative incidence,flexible parametric,regression,subdistribution hazards,survival analysis},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\9RERN9XJ\\Mozumder et al. - 2018 - Direct likelihood inference on the cause-specific .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\3HUG2FPY\\sim.html}
}

@article{murrayMultipleImputationReview2018,
  title = {Multiple {{Imputation}}: {{A Review}} of {{Practical}} and {{Theoretical Findings}}},
  shorttitle = {Multiple {{Imputation}}},
  author = {Murray, Jared S.},
  year = {2018},
  month = may,
  journal = {Statistical Science},
  volume = {33},
  number = {2},
  pages = {142--159},
  issn = {0883-4237},
  doi = {10.1214/18-STS644},
  urldate = {2020-10-24},
  abstract = {Multiple imputation is a straightforward method for handling missing data in a principled fashion. This paper presents an overview of multiple imputation, including important theoretical results and their practical implications for generating and using multiple imputations. A review of strategies for generating imputations follows, including recent developments in flexible joint modeling and sequential regression/chained equations/fully conditional specification approaches. Finally, we compare and contrast different methods for generating imputations on a range of criteria before identifying promising avenues for future research.},
  langid = {english}
}

@article{nicolaieVerticalModelingAnalysis2019,
  title = {Vertical Modeling: Analysis of Competing Risks Data with a Cure Fraction},
  shorttitle = {Vertical Modeling},
  author = {Nicolaie, Mioara Alina and Taylor, Jeremy M. G. and Legrand, Catherine},
  year = {2019},
  month = jan,
  journal = {Lifetime Data Analysis},
  volume = {25},
  number = {1},
  pages = {1--25},
  issn = {1572-9249},
  doi = {10.1007/s10985-018-9417-8},
  urldate = {2021-09-28},
  abstract = {In this paper, we extend the vertical modeling approach for the analysis of survival data with competing risks to incorporate a cure fraction in the population, that is, a proportion of the population for which none of the competing events can occur. The proposed method has three components: the proportion of cure, the risk of failure, irrespective of the cause, and the relative risk of a certain cause of failure, given a failure occurred. Covariates may affect each of these components. An appealing aspect of the method is that it is a natural extension to competing risks of the semi-parametric mixture cure model in ordinary survival analysis; thus, causes of failure are assigned only if a failure occurs. This contrasts with the existing mixture cure model for competing risks of Larson and Dinse, which conditions at the onset on the future status presumably attained. Regression parameter estimates are obtained using an EM-algorithm. The performance of the estimators is evaluated in a simulation study. The method is illustrated using a melanoma cancer data set.},
  langid = {english}
}

@article{nielsenSurvivalAnalysisCoarsely2003,
  title = {Survival Analysis with Coarsely Observed Covariates},
  author = {Nielsen, S{\o}ren Feodor},
  year = {2003},
  publisher = {Institut d'Estad{\'i}stica de Catalunya},
  issn = {1696-2281},
  urldate = {2024-12-11},
  copyright = {Attribution-NonCommercial-NoDerivs 2.5 Spain},
  langid = {english},
  keywords = {Classificacio AMS::62 Statistics::62N Survival analysis and censored data,Estadistica,Survival Analysis},
  annotation = {Accepted: 2007-11-12T17:50:34Z},
  file = {C:\Users\efbonneville\Zotero\storage\V9I7URJG\Nielsen - 2003 - Survival analysis with coarsely observed covariate.pdf}
}

@article{nikiforowPhaseStudyCD252016,
  title = {A Phase {{I}} Study of {{CD25}}/Regulatory {{T-cell-depleted}} Donor Lymphocyte Infusion for Relapse after Allogeneic Stem Cell Transplantation},
  author = {Nikiforow, Sarah and Kim, Haesook T. and Daley, Heather and Reynolds, Carol and Jones, Kyle Thomas and Armand, Philippe and Ho, Vincent T. and Alyea, Edwin P. and Cutler, Corey S. and Ritz, Jerome and Antin, Joseph H. and Soiffer, Robert J. and Koreth, John},
  year = {2016},
  month = oct,
  journal = {Haematologica},
  volume = {101},
  number = {10},
  pages = {1251--1259},
  issn = {0390-6078},
  doi = {10.3324/haematol.2015.141176},
  urldate = {2023-11-04},
  abstract = {Donor lymphocyte infusions are used to treat relapse after allogeneic hematopoietic stem cell transplantation, but responses are inadequate. In addition to effector cells, infusions contain CD25+ regulatory T cells (Treg) that may suppress graft-versus-tumor responses. We undertook a phase I study of donor lymphocyte infusions depleted of CD25+ T cells in patients with hematologic malignancies who had relapsed after transplantation. Twenty-one subjects received CD25/Treg-depleted infusions following removal of CD25+ cells using antibody-conjugated magnetic beads. Sixteen subjects received prior cytoreductive therapy. Four were in complete remission at the time of infusion. Two dose levels were administered: 1{\texttimes}107 (n=6) and 3{\texttimes}107 CD3+ cells/kg (n=15). A median 2.3 log-depletion of CD4+CD25+FOXP3+ Treg was achieved. Seven subjects (33\%) developed clinically significant graft-versus-host disease by 1 year, including one patient who died. At dose level 1, five subjects had progressive disease and one had stable disease. At dose level 2, nine subjects (60\%) achieved or maintained responses (8 complete responses, 1 partial response), including seven with active disease at the time of infusion. A shorter period between relapse and infusion was associated with response at dose level 2 (P=0.016). The 1-year survival rate was 53\% among patients treated with dose level 2. Four of eight subjects with acute myeloid leukemia remained in remission at 1 year. When compared to unmodified donor lymphocyte infusions in 14 contemporaneous patients meeting study eligibility, CD25/Treg depletion was associated with a better response rate and improved event-free survival. Circulating na{\"i}ve and central memory CD4+ T cells increased after CD25/Treg-depleted infusion, but no immunophenotypic signature for response was noted. CD25/Treg-depleted donor infusion appears feasible and capable of inducing graft-versus-tumor responses without excessive graft-versus-host disease. (ClinicalTrials.gov NCT\#00675831)},
  pmcid = {PMC5046655},
  pmid = {27354021},
  file = {C:\Users\efbonneville\Zotero\storage\FEMTD4IX\Nikiforow et al. - 2016 - A phase I study of CD25regulatory T-cell-depleted.pdf}
}

@article{nowokSynthpopBespokeCreation2016,
  title = {Synthpop: {{Bespoke Creation}} of {{Synthetic Data}} in {{R}}},
  shorttitle = {Synthpop},
  author = {Nowok, Beata and Raab, Gillian M. and Dibben, Chris},
  year = {2016},
  month = oct,
  journal = {Journal of Statistical Software},
  volume = {74},
  pages = {1--26},
  issn = {1548-7660},
  doi = {10.18637/jss.v074.i11},
  urldate = {2024-10-31},
  abstract = {In many contexts, confidentiality constraints severely restrict access to unique and valuable microdata. Synthetic data which mimic the original observed data and preserve the relationships between variables but do not contain any disclosive records are one possible solution to this problem. The synthpop package for R, introduced in this paper, provides routines to generate synthetic versions of original data sets. We describe the methodology and its consequences for the data characteristics. We illustrate the package features using a survey data example.},
  copyright = {Copyright (c) 2016 Beata Nowok, Gillian M. Raab, Chris Dibben},
  langid = {english},
  keywords = {CART,disclosure control,R,synthetic data,UK longitudinal studies},
  file = {C:\Users\efbonneville\Zotero\storage\UN2SS77R\Nowok et al. - 2016 - synthpop Bespoke Creation of Synthetic Data in R.pdf}
}

@article{obermanStandardizedEvaluationImputation,
  title = {Toward a Standardized Evaluation of Imputation Methodology},
  author = {Oberman, Hanne I. and Vink, Gerko},
  journal = {Biometrical Journal},
  volume = {n/a},
  number = {n/a},
  pages = {2200107},
  issn = {1521-4036},
  doi = {10.1002/bimj.202200107},
  urldate = {2023-05-23},
  abstract = {Developing new imputation methodology has become a very active field. Unfortunately, there is no consensus on how to perform simulation studies to evaluate the properties of imputation methods. In part, this may be due to different aims between fields and studies. For example, when evaluating imputation techniques aimed at prediction, different aims may be formulated than when statistical inference is of interest. The lack of consensus may also stem from different personal preferences or scientific backgrounds. All in all, the lack of common ground in evaluating imputation methodology may lead to suboptimal use in practice. In this paper, we propose a move toward a standardized evaluation of imputation methodology. To demonstrate the need for standardization, we highlight a set of possible pitfalls that bring forth a chain of potential problems in the objective assessment of the performance of imputation routines. Additionally, we suggest a course of action for simulating and evaluating missing data problems. Our suggested course of action is by no means meant to serve as a complete cookbook, but rather meant to incite critical thinking and a move to objective and fair evaluations of imputation methodology. We invite the readers of this paper to contribute to the suggested course of action.},
  langid = {english},
  keywords = {evaluation,imputation,missing data,simulation studies},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\RLTIP7DM\\Oberman and Vink - Toward a standardized evaluation of imputation met.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\L3MR83FA\\bimj.html}
}

@inproceedings{orchard1972missing,
  title = {A Missing Information Principle: Theory and Applications},
  booktitle = {Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: {{Theory}} of Statistics},
  author = {Orchard, Terence and Woodbury, Max A},
  year = {1972},
  volume = {6},
  pages = {697--716},
  publisher = {University of California Press},
  file = {C:\Users\efbonneville\Zotero\storage\VSUFB4LJ\Orchard and Woodbury - 1972 - A missing information principle theory and applic.pdf}
}

@article{papageorgiouAlternativeCharacterizationMAR2021,
  title = {An Alternative Characterization of {{MAR}} in Shared Parameter Models for Incomplete Longitudinal Data and Its Utilization for Sensitivity Analysis},
  author = {Papageorgiou, Grigorios and Rizopoulos, Dimitris},
  year = {2021},
  month = feb,
  journal = {Statistical Modelling},
  volume = {21},
  number = {1-2},
  pages = {95--114},
  publisher = {SAGE Publications India},
  issn = {1471-082X},
  doi = {10.1177/1471082X20927114},
  urldate = {2024-06-19},
  abstract = {Dropout is a common complication in longitudinal studies, especially since the distinction between missing not at random (MNAR) and missing at random (MAR) dropout is intractable. Consequently, one starts with an analysis that is valid under MAR and then performs a sensitivity analysis by considering MNAR departures from it. To this end, specific classes of joint models, such as pattern-mixture models (PMMs) and selection models (SeMs), have been proposed. On the contrary, shared-parameter models (SPMs) have received less attention, possibly because they do not embody a characterization of MAR. A few approaches to achieve MAR in SPMs exist, but are difficult to implement in existing software. In this article, we focus on SPMs for incomplete longitudinal and time-to-dropout data and propose an alternative characterization of MAR by exploiting the conditional independence assumption, under which outcome and missingness are independent given a set of random effects. By doing so, the censoring distribution can be utilized to cover a wide range of assumptions for the missing data mechanism on the subject-specific level. This approach offers substantial advantages over its counterparts and can be easily implemented in existing software. More specifically, it offers flexibility over the assumption for the missing data generating mechanism that governs dropout by allowing subject-specific perturbations of the censoring distribution, whereas in PMMs and SeMs dropout is considered MNAR strictly.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\WG2K9BLT\Papageorgiou and Rizopoulos - 2021 - An alternative characterization of MAR in shared p.pdf}
}

@article{papageorgiouOverviewJointModeling2019a,
  title = {An {{Overview}} of {{Joint Modeling}} of {{Time-to-Event}} and {{Longitudinal Outcomes}}},
  author = {Papageorgiou, Grigorios and Mauff, Katya and Tomer, Anirudh and Rizopoulos, Dimitris},
  year = {2019},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {6},
  number = {Volume 6, 2019},
  pages = {223--240},
  publisher = {Annual Reviews},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-030718-105048},
  urldate = {2024-08-06},
  abstract = {In this review, we present an overview of joint models for longitudinal and time-to-event data. We introduce a generalized formulation for the joint model that incorporates multiple longitudinal outcomes of varying types. We focus on extensions for the parametrization of the association structure that links the longitudinal and time-to-event outcomes, estimation techniques, and dynamic predictions. We also outline the software available for the application of these models.},
  langid = {english},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\R6N6QQ2D\\Papageorgiou et al. - 2019 - An Overview of Joint Modeling of Time-to-Event and.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\KPU27L58\\annurev-statistics-030718-105048.html}
}

@article{papageorgiouStatisticalPrimerHow2018,
  title = {Statistical Primer: How to Deal with Missing Data in Scientific Research?{\dag}},
  shorttitle = {Statistical Primer},
  author = {Papageorgiou, Grigorios and Grant, Stuart W and Takkenberg, Johanna J M and Mokhles, Mostafa M},
  year = {2018},
  month = aug,
  journal = {Interactive CardioVascular and Thoracic Surgery},
  volume = {27},
  number = {2},
  pages = {153--158},
  issn = {1569-9285},
  doi = {10.1093/icvts/ivy102},
  urldate = {2022-10-10},
  abstract = {Missing data are a common challenge encountered in research which can compromise the results of statistical inference when not handled appropriately. This paper aims to introduce basic concepts of missing data to a non-statistical audience, list and compare some of the most popular approaches for handling missing data in practice and provide guidelines and recommendations for dealing with and reporting missing data in scientific research. Complete case analysis and single imputation are simple approaches for handling missing data and are popular in practice, however, in most cases they are not guaranteed to provide valid inferences. Multiple imputation is a robust and general alternative which is appropriate for data missing at random, surpassing the disadvantages of the simpler approaches, but should always be conducted with care. The aforementioned approaches are illustrated and compared in an example application using Cox regression.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\CSS7GNXK\\Papageorgiou et al. - 2018 - Statistical primer how to deal with missing data .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\LZJKULFL\\4995008.html}
}

@article{parkSemiparametricCompetingRisks2019,
  title = {Semiparametric Competing Risks Regression under Interval Censoring Using the {{R}} Package Intccr},
  author = {Park, Jun and Bakoyannis, Giorgos and Yiannoutsos, Constantin T.},
  year = {2019},
  month = may,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {173},
  pages = {167--176},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2019.03.002},
  urldate = {2023-10-05},
  abstract = {Background and Objective: Competing risk data are frequently interval-censored in real-world applications, that is, the exact event time is not precisely observed but is only known to lie between two time points such as clinic visits. This type of data requires special handling because the actual event times are unknown. To deal with this problem we have developed an easy-to-use open-source statistical software. Methods: An approach to perform semiparametric regression analysis of the cumulative incidence function with interval-censored competing risks data is the sieve maximum likelihood method based on B-splines. An important feature of this approach is that it does not impose restrictive parametric assumptions. Also, this methodology provides semiparametrically efficient estimates. Implementation of this methodology can be easily performed using our new R package intccr. Results: The R package intccr performs semiparametric regression analysis of the cumulative incidence function based on interval-censored competing risks data. It supports a large class of models including the proportional odds and the Fine--Gray proportional subdistribution hazards model as special cases. It also provides the estimated cumulative incidence functions for a particular combination of covariate values. The package also provides some data management functionality to handle data sets which are in a long format involving multiple lines of data per subject. Conclusions: The R package intccr provides a convenient and flexible software for the analysis of the cumulative incidence function based on interval-censored competing risks data.},
  keywords = {Competing risks,Interval censoring,Proportional hazards model,Proportional odds model,Semiparametric regression,Survival analysis},
  file = {C:\Users\efbonneville\Zotero\storage\7JD84NQV\Park et al. - 2019 - Semiparametric competing risks regression under in.pdf}
}

@article{passamontiDynamicPrognosticModel2010,
  title = {A Dynamic Prognostic Model to Predict Survival in Primary Myelofibrosis: A Study by the {{IWG-MRT}} ({{International Working Group}} for {{Myeloproliferative Neoplasms Research}} and {{Treatment}})},
  shorttitle = {A Dynamic Prognostic Model to Predict Survival in Primary Myelofibrosis},
  author = {Passamonti, Francesco and Cervantes, Francisco and Vannucchi, Alessandro Maria and Morra, Enrica and Rumi, Elisa and Pereira, Arturo and Guglielmelli, Paola and Pungolino, Ester and Caramella, Marianna and Maffioli, Margherita and Pascutto, Cristiana and Lazzarino, Mario and Cazzola, Mario and Tefferi, Ayalew},
  year = {2010},
  month = mar,
  journal = {Blood},
  volume = {115},
  number = {9},
  pages = {1703--1708},
  issn = {0006-4971},
  doi = {10.1182/blood-2009-09-245837},
  urldate = {2024-10-22},
  abstract = {Age older than 65 years, hemoglobin level lower than 100 g/L (10 g/dL), white blood cell count greater than 25 {\texttimes} 109/L, peripheral blood blasts 1\% or higher, and constitutional symptoms have been shown to predict poor survival in primary myelofibrosis (PMF) at diagnosis. To investigate whether the acquisition of these factors during follow-up predicts survival, we studied 525 PMF patients regularly followed. All 5 variables had a significant impact on survival when analyzed as time-dependent covariates in a multivariate Cox proportional hazard model and were included in 2 separate models, 1 for all patients (Dynamic International Prognostic Scoring System [DIPSS]) and 1 for patients younger than 65 years (age-adjusted DIPSS). Risk factors were assigned score values based on hazard ratios (HRs). Risk categories were low, intermediate-1, intermediate-2, and high in both models. Survival was estimated by the HR. When shifting to the next risk category, the HR was 4.13 for low risk, 4.61 for intermediate-1, and 2.54 for intermediate-2 according to DIPSS; 3.97 for low risk, 2.84 for intermediate-1, and 1.81 for intermediate-2 according to the age-adjusted DIPSS. The novelty of these models is the prognostic assessment of patients with PMF anytime during their clinical course, which may be useful for treatment decision-making.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\WX43IP45\\Passamonti et al. - 2010 - A dynamic prognostic model to predict survival in .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\D7EQG756\\A-dynamic-prognostic-model-to-predict-survival-in.html}
}

@article{pazGenomicAnalysisPrimary2021,
  title = {Genomic Analysis of Primary and Secondary Myelofibrosis Redefines the Prognostic Impact of {{ASXL1}} Mutations: {{A FIM}} Study},
  author = {Paz, D.L. and {Riou J.} and {Verger E.} and {Cassinat B.} and {Chauveau A.} and {Ianotto J.-C.} and {Dupriez B.} and {Boyer F.} and {Renard M.} and {Mansier O.} and {Murati A.} and {Rey J.} and {Etienne G.} and {Mas V.M.-D.} and {Tavitian S.} and {Nibourel O.} and {Girault S.} and {Bris Y.L.} and {Girodon F.} and {Ranta D.} and {Chomel J.-C.} and {Cony-Makhoul P.} and {Sujobert P.} and {Robles M.} and {Abdelali R.B.} and {Kosmider O.} and {Cottin L.} and {Roy L.} and {Sloma I.} and {Vacheret F.} and {Wemeau M.} and {Mossuz P.} and {Slama B.} and {Cussac V.} and {Denis G.} and {Walter-Petrich A.} and {Burroni B.} and {Jezequel N.} and {Giraudier S.} and {Lippert E.} and {Socie G.} and {Kiladjian J.-J.} and {Ugo V.}},
  year = {2021},
  journal = {Blood Advances},
  volume = {5},
  number = {5},
  pages = {1442--1451},
  publisher = {American Society of Hematology},
  address = {United States},
  issn = {2473-9529},
  doi = {10.1182/bloodadvances.2020003444},
  abstract = {We aimed to study the prognostic impact of the mutational landscape in primary and secondary myelofibrosis. The study included 479 patients with myelofibrosis recruited from 24 French Intergroup of Myeloproliferative Neoplasms (FIM) centers. The molecular landscape was studied by high-throughput sequencing of 77 genes. A Bayesian network allowed the identification of genomic groups whose prognostic impact was studied in a multistate model considering transitions from the 3 conditions: Myelofibrosis, acute leukemia, and death. Results were validated using an independent, previously published cohort (n 5 276). Four genomic groups were identified: Patients with TP53 mutation; patients with \$1 mutation in EZH2, CBL, U2AF1, SRSF2, IDH1, IDH2, NRAS, or KRAS (highrisk group); patients with ASXL1-only mutation (ie, no associated mutation in TP53 or highrisk genes); and other patients. A multistate model found that both TP53 and high-risk groups were associated with leukemic transformation (hazard ratios [HRs] [95\% confidence interval], 8.68 [3.32-22.73] and 3.24 [1.58-6.64], respectively) and death from myelofibrosis (HRs, 3.03 [1.66-5.56] and 1.77 [1.18-2.67], respectively). ASXL1-only mutations had no prognostic value that was confirmed in the validation cohort. However, ASXL1 mutations conferred a worse prognosis when associated with a mutation in TP53 or high-risk genes. This study provides a new definition of adverse mutations in myelofibrosis with the addition of TP53, CBL, NRAS, KRAS, and U2AF1 to previously described genes. Furthermore, our results argue that ASXL1 mutations alone cannot be considered detrimental.Copyright {\copyright} 2021 by The American Society of Hematology.},
  langid = {english},
  keywords = {*ASXL1 gene,*gene,*gene mutation,*genetic analysis,*genetic predisposition,*myelofibrosis/et [Etiology],acute leukemia/et [Etiology],article,Bayesian network,CBL gene,cohort analysis,controlled study,death,Ezh2 gene,female,gene identification,genetic susceptibility,genetic variation,high throughput sequencing,human,human cell,IDH1 gene,IDH2 gene,KRAS gene,major clinical study,male,NRAS gene,priority journal,somatic mutation,srsf2 gene,TP53 gene,U2AF1 gene},
  file = {C:\Users\efbonneville\Zotero\storage\TEAKWS8T\Paz D.L. et al. - 2021 - Genomic analysis of primary and secondary myelofib.pdf}
}

@article{penackSerotherapyThymoglobulinAlemtuzumab2008,
  title = {Serotherapy with Thymoglobulin and Alemtuzumab Differentially Influences Frequency and Function of Natural Killer Cells after Allogeneic Stem Cell Transplantation},
  author = {Penack, O. and Fischer, L. and Stroux, A. and Gentilini, C. and Nogai, A. and Muessig, A. and Rieger, K. and Ganepola, S. and Herr, W. and Meyer, R. G. and Thiel, E. and Uharek, L.},
  year = {2008},
  month = feb,
  journal = {Bone Marrow Transplantation},
  volume = {41},
  number = {4},
  pages = {377--383},
  publisher = {Nature Publishing Group},
  issn = {1476-5365},
  doi = {10.1038/sj.bmt.1705911},
  urldate = {2023-11-04},
  abstract = {Although thymoglobulin and alemtuzumab are frequently used in hematopoietic stem cell transplantation (HSCT), little is known of their effects on NK cells, which mediate important functions in post-transplantation immunology. In the present study, we determined NK cell death in vitro using propidium iodide and Annexin V. The NK cell activity in 34 patients at day +30 after allogeneic HSCT was assessed using the CD107a assay. Alemtuzumab and thymoglobulin were similarly very potent in inducing NK cell death in vitro. Even in low concentrations ({$<$}1\,{$\mu$}g/ml) the antibodies induced apoptosis and necrosis in a relevant percentage of NK cells ({$>$}30\%). However, the number of tumor reactive (CD107a+) NK cells was 13.16 per {$\mu$}l and 1.15 per {$\mu$}l (mean) in patients receiving T-cell depletion with 6\,mg/kg thymoglobulin and in patients receiving 100\,mg alemtuzumab, respectively (P=0.02). Although thymoglobulin and alemtuzumab are equally NK cell toxic in vitro, the recovery of NK cell frequency and anti-tumor reactivity is reduced in recipients of alemtuzumab. Our findings can be explained by a longer half-life of alemtuzumab as compared to active thymoglobulin under therapeutic conditions. Prolonged immunosuppression with increased risk of infections and tumor relapse are a potential threat to patients undergoing HCST and receiving alemtuzumab as T-cell depletion.},
  copyright = {2008 Springer Nature Limited},
  langid = {english},
  keywords = {Cell Biology,general,Hematology,Internal Medicine,Medicine/Public Health,Public Health,Stem Cells},
  file = {C:\Users\efbonneville\Zotero\storage\NUMEK5U9\Penack et al. - 2008 - Serotherapy with thymoglobulin and alemtuzumab dif.pdf}
}

@manual{pinheiroNlmeLinearNonlinear2023,
  type = {Manual},
  title = {Nlme: {{Linear}} and Nonlinear Mixed Effects Models},
  author = {Pinheiro, Jos{\'e} and Bates, Douglas and {R Core Team}},
  year = {2023}
}

@article{pintilieAnalysingInterpretingCompeting2007,
  title = {Analysing and Interpreting Competing Risk Data},
  author = {Pintilie, Melania},
  year = {2007},
  journal = {Statistics in Medicine},
  volume = {26},
  number = {6},
  pages = {1360--1367},
  issn = {1097-0258},
  doi = {10.1002/sim.2655},
  urldate = {2021-05-18},
  abstract = {When competing risks are present, two types of analysis can be performed: modelling the cause specific hazard and modelling the hazard of the subdistribution. This paper contrasts these two methods and presents the benefits of each. The interpretation is specific to the analysis performed. When modelling the cause specific hazard, one performs the analysis under the assumption that the competing risks do not exist. This could be beneficial when, for example, the main interest is whether the treatment works in general. In modelling the hazard of the subdistribution, one incorporates the competing risks in the analysis. This analysis compares the observed incidence of the event of interest between groups. The latter analysis is specific to the structure of the observed data and it can be generalized only to another population with similar competing risks. Copyright {\copyright} 2006 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2006 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {cause specific hazard,competing risk,hazard of the subdistribution,interpretation,modelling}
}

@article{polverelliImpactComorbiditiesBody2024a,
  title = {Impact of Comorbidities and Body Mass Index on the Outcomes of Allogeneic Hematopoietic Cell Transplantation in Myelofibrosis: {{A}} Study on Behalf of the {{Chronic Malignancies Working Party}} of {{EBMT}}},
  author = {Polverelli, Nicola and Bonneville, Edouard F and {de Wreede}, Liesbeth C and Koster, Linda and Kr{\"o}ger, Nicolaus Martin and Schroeder, Thomas and {Peffault de Latour}, R{\'e}gis and Passweg, Jakob and Sockel, Katja and Broers, Annoek E C and Clark, Andrew and Dreger, Peter and Blaise, Didier and {Yakoub-Agha}, Ibrahim and Petersen, Soeren Lykke and Finke, J{\"u}rgen and Chevallier, Patrice and Helbig, Grzegorz and Rabitsch, Werner and Sammassimo, Simona and Arcaini, Luca and Russo, Domenico and {Drozd-Sokolowska}, Joanna and Raj, Kavita and Robin, Marie and Battipaglia, Giorgia and Czerw, Tomasz and {Hern{\'a}ndez-Boluda}, Juan Carlos and McLornan, Donal P},
  year = {2024},
  month = may,
  journal = {American journal of hematology},
  volume = {99},
  number = {5},
  pages = {993---996},
  issn = {0361-8609},
  doi = {10.1002/ajh.27262},
  abstract = {Investigating the evaluation of eligibility for transplant in myelofibrosis (MF): The role of HCT-CI and BMI. HCT-CI emerges as a key prognostic factor, while BMI shows limited impact. This study expands insights for better clinical decision-making in MF allo-HCT.}
}

@article{polverelliImpactSpleenSize2021a,
  title = {Impact of Spleen Size and Splenectomy on Outcomes of Allogeneic Hematopoietic Cell Transplantation for Myelofibrosis: {{A}} Retrospective Analysis by the Chronic Malignancies Working Party on Behalf of {{European}} Society for Blood and Marrow Transplantation ({{EBMT}})},
  shorttitle = {Impact of Spleen Size and Splenectomy on Outcomes of Allogeneic Hematopoietic Cell Transplantation for Myelofibrosis},
  author = {Polverelli, Nicola and Mauff, Katya and Kr{\"o}ger, Nicolaus and Robin, Marie and Beelen, Dietrich and Beauvais, David and Chevallier, Patrice and Mohty, Mohamad and Passweg, Jakob and Rubio, Marie Th{\'e}r{\`e}se and Maertens, Johan and Finke, J{\"u}rgen and Bornh{\"a}user, Martin and Vrhovac, Radovan and Helbig, Grzegorz and Mear, Jean-Baptiste and Castagna, Luca and Rem{\'e}nyi, P{\'e}ter and Angelucci, Emanuele and Karakasis, Dimitrios and Rif{\`o}n, Jose and Sirait, Tiarlan and Russo, Domenico and {de Wreede}, Liesbeth and Czerw, Tomasz and {Hern{\'a}ndez-Boluda}, Juan Carlos and Hayden, Patrick and McLornan, Donal and {Yakoub-Agha}, Ibrahim},
  year = {2021},
  journal = {American Journal of Hematology},
  volume = {96},
  number = {1},
  pages = {69--79},
  issn = {1096-8652},
  doi = {10.1002/ajh.26020},
  urldate = {2024-10-14},
  abstract = {The role of spleen size and splenectomy for the prediction of post-allogeneic hematopoietic stem cell transplant (allo-HCT) outcome in myelofibrosis remains under debate. In EBMT registry, we identified a cohort of 1195 myelofibrosis patients transplanted between 2000-2017 after either fludarabine-busulfan or fludarabine-melphalan regimens. Overall, splenectomy was performed in 202 (16.9\%) patients and its use decreased over time (28.3\% in 2000-2009 vs 14.1\% in 2010-2017 period). By multivariate analysis, splenectomy was associated with less NRM (HR 0.64, 95\% CI 0.44-0.93, P = .018) but increased risk of relapse (HR 1.43, 95\% CI 1.01-2.02, P = .042), with no significant impact on OS (HR 0.86, 95\% CI 0.67-1.12, P = .274). However, in subset analysis comparing the impact of splenectomy vs specific spleen sizes, for patients with progressive disease, an improved survival was seen in splenectomised subjects compared to those patients with a palpable spleen length {$\geq$} 15 cm (HR 0.44, 95\% CI 0.28-0.69, P {$<$} .001), caused by a significant reduction in NRM (HR 0.26, 95\% CI 0.14-0.49, P {$<$} .001), without significantly increased relapse risk (HR 1.47, 95\% CI 0.87-2.49, P = .147). Overall, despite the possible biases typical of retrospective cohorts, this study highlights the potential detrimental effect of massive splenomegaly in transplant outcome and supports the role of splenectomy for myelofibrosis patients with progressive disease and large splenomegaly.},
  copyright = {{\copyright} 2020 Wiley Periodicals LLC},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\PI67KMSW\Polverelli et al. - 2021 - Impact of spleen size and splenectomy on outcomes .pdf}
}

@article{polverelliMultidimensionalGeriatricAssessment2020,
  title = {Multidimensional Geriatric Assessment for Elderly Hematological Patients ({$\geq$}60 Years) Submitted to Allogeneic Stem Cell Transplantation. {{A French}}--{{Italian}} 10-Year Experience on 228 Patients},
  author = {Polverelli, Nicola and Tura, Paolo and Battipaglia, Giorgia and Malagola, Michele and Bernardi, Simona and Gandolfi, Lisa and Zollner, Tatiana and Zanaglio, Camilla and Farina, Mirko and Morello, Enrico and Turra, Alessandro and Mohty, Mohamad and Russo, Domenico},
  year = {2020},
  month = dec,
  journal = {Bone Marrow Transplantation},
  volume = {55},
  number = {12},
  pages = {2224--2233},
  publisher = {Nature Publishing Group},
  issn = {1476-5365},
  doi = {10.1038/s41409-020-0934-1},
  urldate = {2024-10-14},
  abstract = {Nowadays, the evaluation of elderly patients' eligibility for allogeneic stem cell transplantation (allo-SCT) is crucial. We evaluated the feasibility and efficacy of a multidimensional geriatric assessment, the Fondazione Italiana Linfomi (FIL) score, on a cohort of 228 patients older than 60 years submitted to allo-SCT in Italy and France from 2008 to 2018. Based on FIL score, available in 215 patients, 125 (58\%) patients were classified as ``fit'' and 90 as ``unfit/frail.'' The hematopoietic cell transplantation-specific comorbidity index (HCT-CI) was measured in 222 patients (97\%); 71 (32\%) patients had HCT-CI 0, 75 (34\%) patients scored 1--2, and 76 (34\%) {$\geq$}3. A total of 121 (53\%) patients died after a median follow-up of 36 months. FIL score was found to highly predict survival, due to an excess of NRM in unfit/frail group, and confirmed its independent prognostic role on OS (HR: 0.37; 95\% CI: 0.25--0.55; p\,{$<$}\,0.0001). On the contrary, the HCI-CI failed in allo-SCT outcome prediction (HR: 1.06; 95\% CI: 0.96--1.16; p\,=\,0.27). In summary, a comprehensive geriatric assessment with FIL score seems to add significant prognostic information in elderly patients submitted to allo-SCT. The pretransplant adoption of this easy-to-use tool could help the patients' selection and management.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Prognosis,Risk factors,Stem-cell research},
  file = {C:\Users\efbonneville\Zotero\storage\VEBIEUCT\Polverelli et al. - 2020 - Multidimensional geriatric assessment for elderly .pdf}
}

@article{polverelliSplenomegalyPatientsPrimary2023,
  title = {Splenomegaly in Patients with Primary or Secondary Myelofibrosis Who Are Candidates for Allogeneic Hematopoietic Cell Transplantation: A {{Position Paper}} on Behalf of the {{Chronic Malignancies Working Party}} of the {{EBMT}}},
  shorttitle = {Splenomegaly in Patients with Primary or Secondary Myelofibrosis Who Are Candidates for Allogeneic Hematopoietic Cell Transplantation},
  author = {Polverelli, Nicola and {Hern{\'a}ndez-Boluda}, Juan Carlos and Czerw, Tomasz and Barbui, Tiziano and D'Adda, Mariella and Deeg, Hans Joachim and Ditschkowski, Markus and Harrison, Claire and Kr{\"o}ger, Nicolaus Martin and Mesa, Ruben and Passamonti, Francesco and Palandri, Francesca and Pemmaraju, Naveen and Popat, Uday and Rondelli, Damiano and Vannucchi, Alessandro Maria and Verstovsek, Srdan and Robin, Marie and Colecchia, Antonio and Grazioli, Luigi and Damiani, Enrico and Russo, Domenico and Brady, Jessica and Patch, David and Blamek, Slawomir and Damaj, Gandhi Laurent and Hayden, Patrick and McLornan, Donal P. and {Yakoub-Agha}, Ibrahim},
  year = {2023},
  month = jan,
  journal = {The Lancet Haematology},
  volume = {10},
  number = {1},
  pages = {e59-e70},
  publisher = {Elsevier},
  issn = {2352-3026},
  doi = {10.1016/S2352-3026(22)00330-1},
  urldate = {2024-10-14},
  langid = {english},
  pmid = {36493799},
  file = {C:\Users\efbonneville\Zotero\storage\A3WNKJ7J\Polverelli et al. - 2023 - Splenomegaly in patients with primary or secondary.pdf}
}

@article{poythressPlanningAnalyzingClinical2020a,
  title = {Planning and Analyzing Clinical Trials with Competing Risks: {{Recommendations}} for Choosing Appropriate Statistical Methodology},
  shorttitle = {Planning and Analyzing Clinical Trials with Competing Risks},
  author = {Poythress, J.c. and Lee, Misun Yu and Young, James},
  year = {2020},
  journal = {Pharmaceutical Statistics},
  volume = {19},
  number = {1},
  pages = {4--21},
  issn = {1539-1612},
  doi = {10.1002/pst.1966},
  urldate = {2023-10-06},
  abstract = {In the analysis of time-to-event data, competing risks occur when multiple event types are possible, and the occurrence of a competing event precludes the occurrence of the event of interest. In this situation, statistical methods that ignore competing risks can result in biased inference regarding the event of interest. We review the mechanisms that lead to bias and describe several statistical methods that have been proposed to avoid bias by formally accounting for competing risks in the analyses of the event of interest. Through simulation, we illustrate that Gray's test should be used in lieu of the logrank test for nonparametric hypothesis testing. We also compare the two most popular models for semiparametric modelling: the cause-specific hazards (CSH) model and Fine-Gray (F-G) model. We explain how to interpret estimates obtained from each model and identify conditions under which the estimates of the hazard ratio and subhazard ratio differ numerically. Finally, we evaluate several model diagnostic methods with respect to their sensitivity to detect lack of fit when the CSH model holds, but the F-G model is misspecified and vice versa. Our results illustrate that adequacy of model fit can strongly impact the validity of statistical inference. We recommend analysts incorporate a model diagnostic procedure and contingency to explore other appropriate models when designing trials in which competing risks are anticipated.},
  copyright = {{\copyright} 2019 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {cause-specific hazards model,competing risks,Fine-Gray model,goodness of fit,logrank test},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\PV37MLNZ\\Poythress et al. - 2020 - Planning and analyzing clinical trials with compet.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\Z7L5288W\\pst.html}
}

@article{prenticeAnalysisFailureTimes1978,
  title = {The {{Analysis}} of {{Failure Times}} in the {{Presence}} of {{Competing Risks}}},
  author = {Prentice, R. L. and Kalbfleisch, J. D. and Peterson, A. V. and Flournoy, N. and Farewell, V. T. and Breslow, N. E.},
  year = {1978},
  journal = {Biometrics},
  volume = {34},
  number = {4},
  eprint = {2530374},
  eprinttype = {jstor},
  pages = {541--554},
  publisher = {[Wiley, International Biometric Society]},
  issn = {0006-341X},
  doi = {10.2307/2530374},
  urldate = {2020-11-21},
  abstract = {Distinct problems in the analysis of failure times with competing causes of failure include the estimation of treatment or exposure effects on specific failure types, the study of interrelations among failure types, and the estimation of failure rates for some causes given the removal of certain other failure types. The usual formulation of these problems is in terms of conceptual or latent failure times for each failure type. This approach is criticized on the basis of unwarranted assumptions, lack of physical interpretation and identifiability problems. An alternative approach utilizing cause-specific hazard functions for observable quantities, including time-dependent covariates, is proposed. Cause-specific hazard functions are shown to be the basic estimable quantities in the competing risks framework. A method, involving the estimation of parameters that relate time-dependent risk indicators for some causes to cause-specific hazard functions for other causes, is proposed for the study of interrelations among failure types. Further, it is argued that the problem of estimation of failure rates under the removal of certain causes is not well posed until a mechanism for cause removal is specified. Following such a specification, one will sometimes be in a position to make sensible extrapolations from available data to situations involving cause removal. A clinical program in bone marrow transplantation for leukemia provides a setting for discussion and illustration of each of these ideas. Failure due to censoring in a survivorship study leads to further discussion.}
}

@article{putterRelationCausespecificHazard2020,
  title = {On the Relation between the Cause-Specific Hazard and the Subdistribution Rate for Competing Risks Data: {{The Fine}}--{{Gray}} Model Revisited},
  author = {Putter, Hein and Schumacher, Martin and {van Houwelingen}, Hans C.},
  year = {2020},
  journal = {Biometrical Journal},
  volume = {62},
  number = {3},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201800274},
  pages = {790--807},
  doi = {10.1002/bimj.201800274},
  abstract = {Abstract The Fine--Gray proportional subdistribution hazards model has been puzzling many people since its introduction. The main reason for the uneasy feeling is that the approach considers individuals still at risk for an event of cause 1 after they fell victim to the competing risk of cause 2. The subdistribution hazard and the extended risk sets, where subjects who failed of the competing risk remain in the risk set, are generally perceived as unnatural . One could say it is somewhat of a riddle why the Fine--Gray approach yields valid inference. To take away these uneasy feelings, we explore the link between the Fine--Gray and cause-specific approaches in more detail. We introduce the reduction factor as representing the proportion of subjects in the Fine--Gray risk set that has not yet experienced a competing event. In the presence of covariates, the dependence of the reduction factor on a covariate gives information on how the effect of the covariate on the cause-specific hazard and the subdistribution hazard relate. We discuss estimation and modeling of the reduction factor, and show how they can be used in various ways to estimate cumulative incidences, given the covariates. Methods are illustrated on data of the European Society for Blood and Marrow Transplantation.},
  keywords = {cause-specific hazard,competing risks,cumulative incidence,proportional hazards,subdistribution hazard},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\CK5EE39F\\Putter et al. - 2020 - On the relation between the cause-specific hazard .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\WZW22FMR\\Putter et al. - 2020 - On the relation between the cause-specific hazard .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\N84XGGP9\\bimj.html}
}

@article{putterTutorialBiostatisticsCompeting2007,
  title = {Tutorial in Biostatistics: Competing Risks and Multi-State Models},
  shorttitle = {Tutorial in Biostatistics},
  author = {Putter, H. and Fiocco, M. and Geskus, R. B.},
  year = {2007},
  journal = {Statistics in Medicine},
  volume = {26},
  number = {11},
  pages = {2389--2430},
  issn = {1097-0258},
  doi = {10.1002/sim.2712},
  urldate = {2020-11-08},
  abstract = {Standard survival data measure the time span from some time origin until the occurrence of one type of event. If several types of events occur, a model describing progression to each of these competing risks is needed. Multi-state models generalize competing risks models by also describing transitions to intermediate events. Methods to analyze such models have been developed over the last two decades. Fortunately, most of the analyzes can be performed within the standard statistical packages, but may require some extra effort with respect to data preparation and programming. This tutorial aims to review statistical methods for the analysis of competing risks and multi-state models. Although some conceptual issues are covered, the emphasis is on practical issues like data preparation, estimation of the effect of covariates, and estimation of cumulative incidence functions and state and transition probabilities. Examples of analysis with standard software are shown. Copyright {\copyright} 2006 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2006 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {competing risks,multi-state model,prediction,prognostic factors,survival analysis}
}

@article{qiComparisonMultipleImputation2010,
  title = {A Comparison of Multiple Imputation and Fully Augmented Weighted Estimators for {{Cox}} Regression with Missing Covariates},
  author = {Qi, Lihong and Wang, Ying-Fang and He, Yulei},
  year = {2010},
  journal = {Statistics in Medicine},
  volume = {29},
  number = {25},
  pages = {2592--2604},
  issn = {1097-0258},
  doi = {10.1002/sim.4016},
  urldate = {2023-11-01},
  abstract = {Several approaches exist for handling missing covariates in the Cox proportional hazards model. The multiple imputation (MI) is relatively easy to implement with various software available and results in consistent estimates if the imputation model is correct. On the other hand, the fully augmented weighted estimators (FAWEs) recover a substantial proportion of the efficiency and have the doubly robust property. In this paper, we compare the FAWEs and the MI through a comprehensive simulation study. For the MI, we consider the multiple imputation by chained equation and focus on two imputation methods: Bayesian linear regression imputation and predictive mean matching. Simulation results show that the imputation methods can be rather sensitive to model misspecification and may have large bias when the censoring time depends on the missing covariates. In contrast, the FAWEs allow the censoring time to depend on the missing covariates and are remarkably robust as long as getting either the conditional expectations or the selection probability correct due to the doubly robust property. The comparison suggests that the FAWEs show the potential for being a competitive and attractive tool for tackling the analysis of survival data with missing covariates. Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {accelerated failure time model,augmented inverse probability weighted estimators,doubly robust property,missing data,proportional hazards model,survival analysis},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\U7FW4D7D\\Qi et al. - 2010 - A comparison of multiple imputation and fully augm.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\JDVIW7ZW\\sim.html}
}

@article{quartagnoMultipleImputationDiscrete2019,
  title = {Multiple Imputation for Discrete Data: {{Evaluation}} of the Joint Latent Normal Model},
  shorttitle = {Multiple Imputation for Discrete Data},
  author = {Quartagno, Matteo and Carpenter, James R.},
  year = {2019},
  journal = {Biometrical Journal},
  volume = {61},
  number = {4},
  pages = {1003--1019},
  issn = {1521-4036},
  doi = {10.1002/bimj.201800222},
  urldate = {2020-12-23},
  abstract = {Missing data are ubiquitous in clinical and social research, and multiple imputation (MI) is increasingly the methodology of choice for practitioners. Two principal strategies for imputation have been proposed in the literature: joint modelling multiple imputation (JM-MI) and full conditional specification multiple imputation (FCS-MI). While JM-MI is arguably a preferable approach, because it involves specification of an explicit imputation model, FCS-MI is pragmatically appealing, because of its flexibility in handling different types of variables. JM-MI has developed from the multivariate normal model, and latent normal variables have been proposed as a natural way to extend this model to handle categorical variables. In this article, we evaluate the latent normal model through an extensive simulation study and an application on data from the German Breast Cancer Study Group, comparing the results with FCS-MI. We divide our investigation in four sections, focusing on (i) binary, (ii) categorical, (iii) ordinal, and (iv) count data. Using data simulated from both the latent normal model and the general location model, we find that in all but one extreme general location model setting JM-MI works very well, and sometimes outperforms FCS-MI. We conclude the latent normal model, implemented in the R package jomo, can be used with confidence by researchers, both for single and multilevel multiple imputation.},
  copyright = {{\copyright} 2019 The Authors. Biometrical Journal Published by WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim.},
  langid = {english},
  keywords = {categorical data,joint model,latent normal model,missing data,multiple imputation}
}

@article{rathouzIdentifiabilityAssumptionsMissing2007,
  title = {Identifiability Assumptions for Missing Covariate Data in Failure Time Regression Models},
  author = {Rathouz, Paul J.},
  year = {2007},
  month = apr,
  journal = {Biostatistics},
  volume = {8},
  number = {2},
  pages = {345--356},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxl014},
  urldate = {2023-11-01},
  abstract = {Methods in the literature for missing covariate data in survival models have relied on the missing at random (MAR) assumption to render regression parameters identifiable. MAR means that missingness can depend on the observed exit time, and whether or not that exit is a failure or a censoring event. By considering ways in which missingness of covariate X could depend on the true but possibly censored failure time T and the true censoring time C, we attempt to identify missingness mechanisms which would yield MAR data. We find that, under various reasonable assumptions about how missingness might depend on T and/or C, additional strong assumptions are needed to obtain MAR. We conclude that MAR is difficult to justify in practical applications. One exception arises when missingness is independent of T, and C is independent of the value of the missing X. As alternatives to MAR, we propose two new missingness assumptions. In one, the missingness depends on T but not on C; in the other, the situation is reversed. For each, we show that the failure time model is identifiable. When missingness is independent of T, we show that the naive complete record analysis will yield a consistent estimator of the failure time distribution. When missingness is independent of C, we develop a complete record likelihood function and a corresponding estimator for parametric failure time models. We propose analyses to evaluate the plausibility of either assumption in a particular data set, and illustrate the ideas using data from the literature on this problem.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\F2IR4JLI\\Rathouz - 2007 - Identifiability assumptions for missing covariate .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\4NJKMRXY\\231500.html}
}

@manual{rcoreteamLanguageEnvironmentStatistical2020,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2020},
  address = {Vienna, Austria},
  organization = {R Foundation for Statistical Computing}
}

@manual{rcoreteamLanguageEnvironmentStatistical2023,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2023},
  address = {Vienna, Austria},
  institution = {R Foundation for Statistical Computing}
}

@manual{rcoreteamLanguageEnvironmentStatistical2024,
  type = {Manual},
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2024},
  address = {Vienna, Austria},
  institution = {R Foundation for Statistical Computing}
}

@inproceedings{resche-rigonImputingMissingCovariate2012,
  title = {Imputing Missing Covariate Values in Presence of Competing Risk},
  booktitle = {International {{Society}} for {{Clinical Biostatistics Conference}}},
  author = {{Resche-Rigon}, Matthieu and White, Ian and Chevret, Sylvie},
  year = {2012},
  month = aug,
  address = {Bergen, Norway, 19-23 August 2012, P22.10},
  abstract = {Due to its flexibility, its practicability and its efficiency compared to the complete case analysis, multiple imputation by chained equations is widely used to impute missing data. Imputation models are built using regression models and it is well known that to avoid bias in the analysis model, the imputation model must include all the analysis model variables including the outcome. In survival analyses, outcome is defined by a binary event indicator D and the observed event or censoring time T. Unfortunately, estimates obtained by direct inclusion of D and T in the imputation model are biased. Using a Cox model, I. White and P. Royston showed that the imputation model should include the event indicator and the cumulative baseline hazard, and therefore recommended to include the Nelson-Aalen estimator. In the competing-risks setting, subjects may experiment one out of K distinct and exclusive events. Two main approaches have been proposed. The most common approach models the cause-specific hazard of the event of interest while the second approach models the sub-distribution hazard associated with the cumulative incidence. We propose to extend the work of I. White and P. Royston to the competing-risks setting by including in the imputation model the cumulative hazard of the competing events. Moreover, we will show that cumulative hazards of all the events that compete to each other should be included. Performance of our approach will be evaluated by a simulation study, then applied to a sample of 278 adult patients with acute myeloid leukaemia.},
  file = {C:\Users\efbonneville\Zotero\storage\9YJIXXCL\Resche-Rigon - 2012 - .pdf}
}

@manual{rizopoulosJMbayes2ExtendedJoint2023,
  type = {Manual},
  title = {{{JMbayes2}}: {{Extended}} Joint Models for Longitudinal and Time-to-Event Data},
  author = {Rizopoulos, Dimitris and Papageorgiou, Grigorios and Miranda Afonso, Pedro},
  year = {2023}
}

@article{rizopoulosJMPackageJoint2010,
  title = {{{JM}}: {{An R}} Package for the Joint Modelling of Longitudinal and Time-to-Event Data},
  author = {Rizopoulos, Dimitris},
  year = {2010},
  journal = {Journal of Statistical Software},
  volume = {35},
  number = {9},
  pages = {1--33}
}

@book{rizopoulosJointModelsLongitudinal2012,
  title = {Joint {{Models}} for {{Longitudinal}} and {{Time-to-Event Data}}},
  author = {Rizopoulos, Dimitris},
  year = {2012},
  month = jul,
  edition = {1st edition},
  publisher = {Routledge},
  address = {Boca Raton},
  isbn = {978-1-4398-7286-4},
  langid = {english}
}

@article{RJ-2017-062,
  title = {{{riskRegression}}: {{Predicting}} the Risk of an Event Using Cox Regression Models},
  author = {Ozenne, Brice and S{\o}rensen, Anne Lyngholm and Scheike, Thomas and {Torp-Pedersen}, Christian and Gerds, Thomas Alexander},
  year = {2017},
  journal = {The R Journal},
  volume = {9},
  number = {2},
  pages = {440--460},
  doi = {10.32614/RJ-2017-062}
}

@article{rouanetInterpretationMixedModels2019a,
  title = {Interpretation of Mixed Models and Marginal Models with Cohort Attrition Due to Death and Drop-Out},
  author = {Rouanet, Ana{\"i}s and Helmer, Catherine and Dartigues, Jean-Fran{\c c}ois and {Jacqmin-Gadda}, H{\'e}l{\`e}ne},
  year = {2019},
  month = feb,
  journal = {Statistical Methods in Medical Research},
  volume = {28},
  number = {2},
  pages = {343--356},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280217723675},
  urldate = {2024-08-23},
  abstract = {Mixed models estimated by maximum likelihood and marginal models estimated by generalized estimating equations are the standard methods for the analysis of longitudinal data. However, their use is highly debated when attrition may be due to death. While some authors consider that mixed model estimates are interpretable only in an immortal cohort, we show that their subject-specific interpretation still holds in the population currently alive, but their population-averaged interpretation is valid only in the immortal cohort. We propose an approximation of the population-averaged mean among the population alive that highlights the difference with the population-averaged mean in the immortal cohort. The interpretation of ML estimates of mixed models and joint models for the marker and the time-to-death as well as unweighted and weighted GEE of marginal models is then illustrated in a simulation study and in an application regarding cognitive decline in the elderly.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\RT2KWA4C\Rouanet et al. - 2019 - Interpretation of mixed models and marginal models.pdf}
}

@article{ruanAnalysesCumulativeIncidence2008,
  title = {Analyses of Cumulative Incidence Functions via Non-Parametric Multiple Imputation},
  author = {Ruan, Ping K. and Gray, Robert J.},
  year = {2008},
  journal = {Statistics in Medicine},
  volume = {27},
  number = {27},
  pages = {5709--5724},
  issn = {1097-0258},
  doi = {10.1002/sim.3402},
  urldate = {2022-08-08},
  abstract = {We describe a non-parametric multiple imputation method that recovers the missing potential censoring information from competing risks failure times for the analysis of cumulative incidence functions. The method can be applied in the settings of stratified analyses, time-varying covariates, weighted analysis of case-cohort samples and clustered survival data analysis, where no current available methods can be readily implemented. The method uses a Kaplan--Meier imputation method for the censoring times to form an imputed data set, so cumulative incidence can be analyzed using techniques and software developed for ordinary right censored survival data. We discuss the methodology and show from both simulations and real data examples that the method yields valid estimates and performs well. The method can be easily implemented via available software with a minor programming requirement (for the imputation step). It provides a practical, alternative analysis tool for otherwise complicated analyses of cumulative incidence of competing risks data. Copyright {\copyright} 2008 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {censoring complete data,competing risks,missing data,multiple imputation,partial likelihood estimation,penalized spline},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\23XT7SDJ\\Ruan and Gray - 2008 - Analyses of cumulative incidence functions via non.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\JGLJXIMW\\sim.html}
}

@book{rubin:1987,
  title = {Multiple Imputation for Nonresponse in Surveys},
  author = {Rubin, D.},
  year = {1987},
  publisher = {John Wiley \& Sons},
  address = {New York},
  doi = {10.1002/9780470316696}
}

@article{rubinInferenceMissingData1976,
  title = {Inference and Missing Data},
  author = {Rubin, Donald B.},
  year = {1976},
  month = dec,
  journal = {Biometrika},
  volume = {63},
  number = {3},
  pages = {581--592},
  issn = {0006-3444},
  doi = {10.1093/biomet/63.3.581},
  urldate = {2024-08-09},
  abstract = {When making sampling distribution inferences about the parameter of the data, {\texttheta}, it is appropriate to ignore the process that causes missing data if the missing data are `missing at random' and the observed data are `observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about {\texttheta}, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is `distinct' from {\texttheta}. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\YMV2Y3MV\\RUBIN - 1976 - Inference and missing data.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\DZ7GRENI\\270932.html}
}

@article{ruckerPresentingSimulationResults2014,
  title = {Presenting Simulation Results in a Nested Loop Plot},
  author = {R{\"u}cker, Gerta and Schwarzer, Guido},
  year = {2014},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {14},
  number = {1},
  pages = {129},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-129},
  urldate = {2020-12-13},
  abstract = {Statisticians investigate new methods in simulations to evaluate their properties for future real data applications. Results are often presented in a number of figures, e.g., Trellis plots. We had conducted a simulation study on six statistical methods for estimating the treatment effect in binary outcome meta-analyses, where selection bias (e.g., publication bias) was suspected because of apparent funnel plot asymmetry. We varied five simulation parameters: true treatment effect, extent of selection, event proportion in control group, heterogeneity parameter, and number of studies in meta-analysis. In combination, this yielded a total number of 768 scenarios. To present all results using Trellis plots, 12 figures were needed.},
  keywords = {Diagram,Graphical representation,Plot,Simulation study,Trellis plot}
}

@article{rudolphCausalInferenceFace2020,
  title = {Causal {{Inference}} in the {{Face}} of {{Competing Events}}},
  author = {Rudolph, Jacqueline E. and Lesko, Catherine R. and Naimi, Ashley I.},
  year = {2020},
  month = sep,
  journal = {Current Epidemiology Reports},
  volume = {7},
  number = {3},
  pages = {125--131},
  issn = {2196-2995},
  doi = {10.1007/s40471-020-00240-7},
  urldate = {2023-10-09},
  abstract = {Epidemiologists frequently must handle competing events, which prevent the event of interest from occurring. We review considerations for handling competing events when interpreting results causally.},
  langid = {english},
  keywords = {Causal inference,Competing events,Survival analysis},
  file = {C:\Users\efbonneville\Zotero\storage\TI66ZMM3\Rudolph et al. - 2020 - Causal Inference in the Face of Competing Events.pdf}
}

@article{saadatiPredictionAccuracyVariable2018,
  title = {Prediction Accuracy and Variable Selection for Penalized Cause-Specific Hazards Models},
  author = {Saadati, Maral and Beyersmann, Jan and {Kopp-Schneider}, Annette and Benner, Axel},
  year = {2018},
  journal = {Biometrical Journal},
  volume = {60},
  number = {2},
  pages = {288--306},
  issn = {1521-4036},
  doi = {10.1002/bimj.201600242},
  urldate = {2021-10-13},
  abstract = {We consider modeling competing risks data in high dimensions using a penalized cause-specific hazards (CSHs) approach. CSHs have conceptual advantages that are useful for analyzing molecular data. First, working on hazards level can further understanding of the underlying biological mechanisms that drive transition hazards. Second, CSH models can be used to extend the multistate framework for high-dimensional data. The CSH approach is implemented by fitting separate proportional hazards models for each event type (iCS). In the high-dimensional setting, this might seem too complex and possibly prone to overfitting. Therefore, we consider an extension, namely ``linking'' the separate models by choosing penalty tuning parameters that in combination yield best prediction of the incidence of the event of interest (penCR). We investigate whether this extension is useful with respect to prediction accuracy and variable selection. The two approaches are compared to the subdistribution hazards (SDH) model, which is an established method that naturally achieves ``linking'' by working on incidence level, but loses interpretability of the covariate effects. Our simulation studies indicate that in many aspects, iCS is competitive to penCR and the SDH approach. There are some instances that speak in favor of linking the CSH models, for example, in the presence of opposing effects on the CSHs. We conclude that penalized CSH models are a viable solution for competing risks models in high dimensions. Linking the CSHs can be useful in some particular cases; however, simple models using separately penalized CSH are often justified.},
  langid = {english},
  keywords = {competing risks,high-dimensional data,penalization,prediction},
  file = {C:\Users\efbonneville\Zotero\storage\BS67EFIC\Saadati et al. - 2018 - Prediction accuracy and variable selection for pen.pdf}
}

@article{saccardiBenchmarkingSurvivalOutcomes2023,
  title = {Benchmarking of Survival Outcomes Following {{Haematopoietic Stem Cell Transplantation}} ({{HSCT}}): An Update of the Ongoing Project of the {{European Society}} for {{Blood}} and {{Marrow Transplantation}} ({{EBMT}}) and {{Joint Accreditation Committee}} of {{ISCT}} and {{EBMT}} ({{JACIE}})},
  shorttitle = {Benchmarking of Survival Outcomes Following {{Haematopoietic Stem Cell Transplantation}} ({{HSCT}})},
  author = {Saccardi, Riccardo and Putter, Hein and Eikema, Dirk-Jan and Busto, Mar{\'i}a Paula and McGrath, Eoin and Middelkoop, Bas and Adams, Gillian and Atlija, Marina and Ayuk, Francis Ayuketang and Baldomero, Helen and Beguin, Yves and {de la C{\'a}mara}, Rafael and Cedillo, {\'A}ngel and Balari, Anna Mar{\'i}a Sureda and Chabannon, Christian and Corbacioglu, Selim and Dolstra, Harry and Duarte, Rafael F. and Dulery, R{\'e}my and Greco, Raffaella and Gusi, Andreu and Hamad, Nada and Kenyon, Michelle and Kr{\"o}ger, Nicolaus and Labopin, Myriam and Lee, Julia and Ljungman, Per and Manson, Lynn and Mensil, Florence and Milpied, Noel and Mohty, Mohamad and Oldani, Elena and Orchard, Kim and Passweg, Jakob and Pearce, Rachel and {de Latour}, R{\'e}gis Peffault and Poirel, H{\'e}l{\`e}ne A. and Rintala, Tuula and Rizzo, J. Douglas and Ruggeri, Annalisa and {Sanchez-Martinez}, Carla and {Sanchez-Guijo}, Fermin and {S{\'a}nchez-Ortega}, Isabel and Trnkov{\'a}, Marie and Ferreiras, David Valc{\'a}rcel and Wilcox, Leonie and {de Wreede}, Liesbeth C. and Snowden, John A.},
  year = {2023},
  month = jun,
  journal = {Bone Marrow Transplantation},
  volume = {58},
  number = {6},
  pages = {659--666},
  publisher = {Nature Publishing Group},
  issn = {1476-5365},
  doi = {10.1038/s41409-023-01924-6},
  urldate = {2023-11-04},
  abstract = {From 2016 EBMT and JACIE developed an international risk-adapted benchmarking program of haematopoietic stem cell transplant (HSCT) outcome to provide individual EBMT Centers with a means of quality-assuring the HSCT process and meeting FACT-JACIE accreditation requirements relating to 1-year survival outcomes. Informed by previous experience from Europe, North America and Australasia, the Clinical Outcomes Group (COG) established criteria for patient and Center selection, and a set of key clinical variables within a dedicated statistical model adapted to the capabilities of the EBMT Registry. The first phase of the project was launched in 2019 to test the acceptability of the benchmarking model through assessment of Centers' performance for 1-year data completeness and survival outcomes of autologous and allogeneic HSCT covering 2013--2016. A second phase was delivered in July 2021 covering 2015--2019 and including survival outcomes. Reports of individual Center performance were shared directly with local principal investigators and their responses were assimilated. The experience thus far has supported the feasibility, acceptability and reliability of the system as well as identifying its limitations. We provide a summary of experience and learning so far in this `work in progress', as well as highlighting future challenges of delivering a modern, robust, data-complete, risk-adapted benchmarking program across new EBMT Registry systems.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Haematological cancer,Therapeutics},
  file = {C:\Users\efbonneville\Zotero\storage\5Y9466PS\Saccardi et al. - 2023 - Benchmarking of survival outcomes following Haemat.pdf}
}

@article{salzmann-manriqueJointModelingImmune2018a,
  title = {Joint {{Modeling}} of {{Immune Reconstitution Post Haploidentical Stem Cell Transplantation}} in {{Pediatric Patients With Acute Leukemia Comparing CD34}}+-{{Selected}} to {{CD3}}/{{CD19-Depleted Grafts}} in a {{Retrospective Multicenter Study}}},
  author = {{Salzmann-Manrique}, Emilia and Bremm, Melanie and Huenecke, Sabine and Stech, Milena and Orth, Andreas and Eyrich, Matthias and Schulz, Ansgar and Esser, Ruth and Klingebiel, Thomas and Bader, Peter and Herrmann, Eva and Koehl, Ulrike},
  year = {2018},
  journal = {Frontiers in Immunology},
  volume = {9},
  issn = {1664-3224},
  urldate = {2023-11-04},
  abstract = {Rapid immune reconstitution (IR) following stem cell transplantation (SCT) is essential for a favorable outcome. The optimization of graft composition should not only enable a sufficient IR but also improve graft vs. leukemia/tumor effects, overcome infectious complications and, finally, improve patient survival. Especially in haploidentical SCT, the optimization of graft composition is controversial. Therefore, we analyzed the influence of graft manipulation on IR in 40 patients with acute leukemia in remission. We examined the cell recovery post haploidentical SCT in patients receiving a CD34+-selected or CD3/CD19-depleted graft, considering the applied conditioning regimen. We used joint model analysis for overall survival (OS) and analyzed the dynamics of age-adjusted leukocytes; lymphocytes; monocytes; CD3+, CD3+CD4+, and CD3+CD8+ T cells; natural killer (NK) cells; and B cells over the course of time after SCT. Lymphocytes, NK cells, and B cells expanded more rapidly after SCT with CD34+-selected grafts (P\,=\,0.036, P\,=\,0.002, and P\,{$<$}\,0.001, respectively). Contrarily, CD3+CD4+ helper T cells recovered delayer in the CD34 selected group (P\,=\,0.026). Furthermore, reduced intensity conditioning facilitated faster immune recovery of lymphocytes and T cells and their subsets (P\,{$<$}\,0.001). However, the immune recovery for NK cells and B cells was comparable for patients who received reduced-intensity or full preparative regimens. Dynamics of all cell types had a significant influence on OS, which did not differ between patients receiving CD34+-selected and those receiving CD3/CD19-depleted grafts. In conclusion, cell reconstitution dynamics showed complex diversity with regard to the graft manufacturing procedure and conditioning regimen.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\55NJNZM9\\Salzmann-Manrique et al. - 2018 - Joint Modeling of Immune Reconstitution Post Haplo.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\MXX866C8\\Salzmann-Manrique et al. - 2018 - Joint Modeling of Immune Reconstitution Post Haplo.pdf}
}

@article{schaferMissingDataOur2002,
  title = {Missing Data: {{Our}} View of the State of the Art},
  shorttitle = {Missing Data},
  author = {Schafer, Joseph L. and Graham, John W.},
  year = {2002},
  journal = {Psychological Methods},
  volume = {7},
  number = {2},
  pages = {147--177},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/1082-989X.7.2.147},
  abstract = {Statistical procedures for missing data have vastly improved, yet misconception and unsound practice still abound. The authors frame the missing-data problem, review methods, offer advice, and raise issues that remain unresolved. They clear up common misunderstandings regarding the missing at random (MAR) concept. They summarize the evidence against older procedures and, with few exceptions, discourage their use. They present, in both technical and practical language, 2 general approaches that come highly recommended: maximum likelihood (ML) and Bayesian multiple imputation (MI). Newer developments are discussed, including some for dealing with missing data that are not MAR. Although not yet in the mainstream, these procedures may eventually extend the ML and MI methods that currently represent the state of the art. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Maximum Likelihood,Methodology,Statistical Data,Statistical Estimation},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\P8TPLRR4\\Schafer and Graham - 2002 - Missing data Our view of the state of the art.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\WF9HFG8D\\2002-13431-001.html}
}

@article{scheikeEfficientEstimationFine2022,
  title = {Efficient {{Estimation}} in the {{Fine}} and {{Gray Model}}},
  author = {Scheike, Thomas H. and Martinussen, Torben and Ozenne, Brice},
  year = {2022},
  journal = {Journal of the American Statistical Association},
  volume = {0},
  number = {0},
  pages = {1--9},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2022.2057860},
  urldate = {2023-10-05},
  abstract = {Direct regression for the cumulative incidence function (CIF) has become increasingly popular since the Fine and Gray model was suggested (Fine and Gray) due to its more direct interpretation on the probability risk scale. We here consider estimation within the Fine and Gray model using the theory of semiparametric efficient estimation. We show that the Fine and Gray estimator is semiparametrically efficient in the case without censoring. In the case of right-censored data, however, we show that the Fine and Gray estimator is no longer semiparametrically efficient and derive the semiparametrically efficient estimator. This estimation approach involves complicated integral equations, and we therefore also derive a simpler estimator as an augmented version of the Fine and Gray estimator with respect to the censoring nuisance space. While the augmentation term involves the CIF of the competing risk, it also leads to a robustness property: the proposed estimators remain consistent even if one of the models for the censoring mechanism or the CIF of the competing risk are misspecified. We illustrate this robustness property using simulation studies, comparing the Fine--Gray estimator and its augmented version. When the competing cause has a high cumulative incidence we see a substantial gain in efficiency from adding the augmentation term with a very reasonable computation time. Supplementary materials for this article are available online.},
  keywords = {Censoring,Counting processes,Double robustness,Efficient estimation,Fine-Gray model,Inverse probability of censoring weighting},
  file = {C:\Users\efbonneville\Zotero\storage\2P4AC3PY\Scheike et al. - 2022 - Efficient Estimation in the Fine and Gray Model.pdf}
}

@article{scheteligLateTreatmentrelatedMortality2019,
  title = {Late Treatment-Related Mortality versus Competing Causes of Death after Allogeneic Transplantation for Myelodysplastic Syndromes and Secondary Acute Myeloid Leukemia},
  author = {Schetelig, Johannes and de Wreede, Liesbeth C. and van Gelder, Michel and Koster, Linda and Finke, J{\"u}rgen and Niederwieser, Dietger and Beelen, Dietrich and Mufti, G. J. and Platzbecker, Uwe and Ganser, Arnold and Heidenreich, Silke and Maertens, Johan and Soci{\'e}, Gerard and Brecht, Arne and Stelljes, Matthias and Kobbe, Guido and Volin, Liisa and Nagler, Arnon and Vitek, Antonin and Luft, Thomas and Ljungman, Per and {Yakoub-Agha}, Ibrahim and Robin, Marie and Kr{\"o}ger, Nicolaus},
  year = {2019},
  month = mar,
  journal = {Leukemia},
  volume = {33},
  number = {3},
  pages = {686--695},
  publisher = {Nature Publishing Group},
  issn = {1476-5551},
  doi = {10.1038/s41375-018-0302-y},
  urldate = {2020-03-31},
  abstract = {The causes and rates of late patient-mortality following alloHCT for myelodysplastic syndromes or secondary acute myeloid leukemia were studied, to assess the contribution of relapse-related, treatment-related, and population factors. Data from EBMT on 6434 adults, who received a first alloHCT from January 2000 to December 2012, were retrospectively studied using combined land-marking, relative-survival methods and multi-state modeling techniques. Median age at alloHCT increased from 49 to 58 years, and the number of patients aged {$\geq$}65 years at alloHCT increased from 5 to 17\%. Overall survival probability was 53\% at 2 years and 35\% at 10 years post-alloHCT. Survival probability at 5 years from the 2-year landmark was 88\% for patients {$<$}45-year old and 63\% for patients {$\geq$}65-year old at alloHCT. Cumulative incidence of nonrelapse mortality (NRM) for patients {$<$}45-year old at transplant was 7\% rising to 25\% for patients aged {$\geq$}65. For older patients, 31\% of NRM-deaths could be attributed to population mortality. Favorable post-alloHCT long-term survival was seen; however, excess mortality-risk for all age groups was shown compared to the general population. A substantial part of total NRM for older patients was attributable to population mortality, information which aids the balanced explanation of post-HCT risk and helps improve long-term care.},
  copyright = {2018 The Author(s)},
  langid = {english},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\IKIUZ784\\Schetelig et al. - 2019 - Late treatment-related mortality versus competing .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\9FJC7IJW\\s41375-018-0302-y.html}
}

@article{schmaelterAlterationsPeripheralBlood2021,
  title = {Alterations of {{Peripheral Blood T Cell Subsets}} Following {{Donor Lymphocyte Infusion}} in {{Patients}} after {{Allogeneic Stem Cell Transplantation}}},
  author = {Schmaelter, Ann-Kristin and Waidhauser, Johanna and Kaiser, Dina and Lenskaja, Tatjana and Gruetzner, Stefanie and Claus, Rainer and Trepel, Martin and Schmid, Christoph and Rank, Andreas},
  year = {2021},
  month = dec,
  journal = {Hemato},
  volume = {2},
  number = {4},
  pages = {692--702},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2673-6357},
  doi = {10.3390/hemato2040046},
  urldate = {2023-11-04},
  abstract = {Donor lymphocyte infusion (DLI) after allogeneic stem cell transplantation (alloSCT) is an established method to enhance the Graft-versus-Leukemia (GvL) effect. However, alterations of cellular subsets in the peripheral blood of DLI recipients have not been studied. We investigated the changes in lymphocyte subpopulations in 16 patients receiving DLI after successful alloSCT. Up to three DLIs were applied in escalating doses, prophylactically for relapse prevention in high-risk disease (n = 5), preemptively for mixed chimerism and/or a molecular relapse/persistence (n = 8), or as part of treatment for hematological relapse (n = 3). We used immunophenotyping to measure the absolute numbers of CD4+, CD8+, NK, and CD56+ T cells and their respective subsets in patients' peripheral blood one day before DLI (d-1) and compared the results at day + 1 and + 7 post DLI to the values before DLI. After the administration of 1 {\texttimes} 106 CD3+ cells/kg body weight, we observed an overall increase in the CD8+ and CD56+ T cell counts. We determined significant changes between day - 1 compared to day + 1 and day + 7 in memory and activated CD8+ subsets and CD56+ T cells. Applying a higher dose of DLI (5 {\texttimes} 106 CD3+ cells/kg) led to a significant increase in the overall counts and subsets of CD8+, CD4+, and NK cells. In conclusion, serial immune phenotyping in the peripheral blood of DLI recipients revealed significant changes in immune effector cells, in particular for various CD8+ T cell subtypes, indicating proliferation and differentiation.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {allogeneic stem cell transplantation,donor lymphocyte infusion,graft-versus-host disease,graft-versus-leukemia effect,immunophenotyping,T lymphocytes},
  file = {C:\Users\efbonneville\Zotero\storage\FDY23WIT\Schmaelter et al. - 2021 - Alterations of Peripheral Blood T Cell Subsets fol.pdf}
}

@article{schoopQuantifyingPredictiveAccuracy2011,
  title = {{Quantifying the predictive accuracy of time-to-event models in the presence of competing risks}},
  author = {Schoop, Rotraut and Beyersmann, Jan and Schumacher, Martin and Binder, Harald},
  year = {2011},
  journal = {Biometrical Journal},
  volume = {53},
  number = {1},
  pages = {88--112},
  issn = {1521-4036},
  doi = {10.1002/bimj.201000073},
  urldate = {2024-12-11},
  abstract = {Prognostic models for time-to-event data play a prominent role in therapy assignment, risk stratification and inter-hospital quality assurance. The assessment of their prognostic value is vital not only for responsible resource allocation, but also for their widespread acceptance. The additional presence of competing risks to the event of interest requires proper handling not only on the model building side, but also during assessment. Research into methods for the evaluation of the prognostic potential of models accounting for competing risks is still needed, as most proposed methods measure either their discrimination or calibration, but do not examine both simultaneously. We adapt the prediction error proposal of Graf et al. (Statistics in Medicine 1999, 18, 2529--2545) and Gerds and Schumacher (Biometrical Journal 2006, 48, 1029--1040) to handle models with competing risks, i.e. more than one possible event type, and introduce a consistent estimator. A simulation study investigating the behaviour of the estimator in small sample size situations and for different levels of censoring together with a real data application follows.},
  copyright = {Copyright {\copyright} 2011 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  langid = {french},
  keywords = {Brier score,IPCW,Prediction error,Quadratic loss},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\4LVLGH6M\\Schoop et al. - 2011 - Quantifying the predictive accuracy of time-to-eve.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\MANQBNRB\\bimj.html}
}

@article{schultze-floreyClonalExpansionCD82021,
  title = {Clonal Expansion of {{CD8}}+ {{T}} Cells Reflects Graft-versus-Leukemia Activity and Precedes Durable Remission Following {{DLI}}},
  author = {{Schultze-Florey}, Christian R. and Kuhlmann, Leonie and Raha, Solaiman and {Barros-Martins}, Joana and Odak, Ivan and Tan, Likai and Xiao, Yankai and Ravens, Sarina and Hambach, Lothar and Venturini, Letizia and Stadler, Michael and Eder, Matthias and Thol, Felicitas and Heuser, Michael and F{\"o}rster, Reinhold and Ganser, Arnold and Prinz, Immo and Koenecke, Christian},
  year = {2021},
  month = nov,
  journal = {Blood Advances},
  volume = {5},
  number = {21},
  pages = {4485--4499},
  issn = {2473-9529},
  doi = {10.1182/bloodadvances.2020004073},
  urldate = {2023-11-04},
  abstract = {Donor lymphocyte infusion (DLI) is a standard of care for relapse of acute myeloid leukemia after allogeneic hematopoietic stem cell transplantation. Currently it is poorly understood how and when CD8+ {$\alpha\beta$} T cells exert graft-versus-leukemia (GVL) activity after DLI. Also, there is no reliable biomarker to monitor GVL activity of the infused CD8+ T cells. Therefore, we analyzed the dynamics of CD8+ {$\alpha\beta$} T-cell clones in patients with DLI. In this prospective clinical study of 29 patients, we performed deep T-cell receptor {$\beta$} (TRB ) sequencing of sorted CD8+ {$\alpha\beta$} T cells to track patients' repertoire changes in response to DLI. Upon first occurrence of GVL, longitudinal analyses revealed a preferential expansion of distinct CD8+TRB clones (n = 14). This did not occur in samples of patients without signs of GVL (n = 11). Importantly, early repertoire changes 15 days after DLI predicted durable remission for the 36-month study follow-up. Furthermore, absence of clonal outgrowth of the CD8+TRB repertoire after DLI was an early biomarker that predicted relapse at a median time of 11.2 months ahead of actual diagnosis. Additionally, unbiased sample analysis regardless of the clinical outcome revealed that patients with decreasing CD8+TRB diversity at day 15 after DLI (n = 13) had a lower relapse incidence (P = .0040) compared with patients without clonal expansion (n = 6). In conclusion, CD8+TRB analysis may provide a reliable tool for predicting the efficacy of DLI and holds the potential to identify patients at risk for progression and relapse after DLI.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\CUK7EQG3\\Schultze-Florey et al. - 2021 - Clonal expansion of CD8+ T cells reflects graft-ve.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\59RCC5LV\\Clonal-expansion-of-CD8-T-cells-reflects-graft.html}
}

@misc{schwerterEvaluatingTreebasedImputation2024,
  title = {Evaluating Tree-Based Imputation Methods as an Alternative to {{MICE PMM}} for Drawing Inference in Empirical Studies},
  author = {Schwerter, Jakob and Gurtskaia, Ketevan and Romero, Andr{\'e}s and {Zeyer-Gliozzo}, Birgit and Pauly, Markus},
  year = {2024},
  month = jan,
  number = {arXiv:2401.09602},
  eprint = {2401.09602},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.09602},
  urldate = {2024-12-10},
  abstract = {Dealing with missing data is an important problem in statistical analysis that is often addressed with imputation procedures. The performance and validity of such methods are of great importance for their application in empirical studies. While the prevailing method of Multiple Imputation by Chained Equations (MICE) with Predictive Mean Matching (PMM) is considered standard in the social science literature, the increase in complex datasets may require more advanced approaches based on machine learning. In particular, tree-based imputation methods have emerged as very competitive approaches. However, the performance and validity are not completely understood, particularly compared to the standard MICE PMM. This is especially true for inference in linear models. In this study, we investigate the impact of various imputation methods on coefficient estimation, Type I error, and power, to gain insights that can help empirical researchers deal with missingness more effectively. We explore MICE PMM alongside different tree-based methods, such as MICE with Random Forest (RF), Chained Random Forests with and without PMM (missRanger), and Extreme Gradient Boosting (MIXGBoost), conducting a realistic simulation study using the German National Educational Panel Study (NEPS) as the original data source. Our results reveal that Random Forest-based imputations, especially MICE RF and missRanger with PMM, consistently perform better in most scenarios. Standard MICE PMM shows partially increased bias and overly conservative test decisions, particularly with non-true zero coefficients. Our results thus underscore the potential advantages of tree-based imputation methods, albeit with a caveat that all methods perform worse with an increased missingness, particularly missRanger.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Machine Learning},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\4F6QPHI5\\Schwerter et al. - 2024 - Evaluating tree-based imputation methods as an alt.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\GWY8T9IA\\2401.html}
}

@article{seamanWhatMeantMissing2013,
  title = {What {{Is Meant}} by ``{{Missing}} at {{Random}}''?},
  author = {Seaman, Shaun and Galati, John and Jackson, Dan and Carlin, John},
  year = {2013},
  month = may,
  journal = {Statistical Science},
  volume = {28},
  number = {2},
  pages = {257--268},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/13-STS415},
  urldate = {2024-08-22},
  abstract = {The concept of missing at random is central in the literature on statistical analysis with missing data. In general, inference using incomplete data should be based not only on observed data values but should also take account of the pattern of missing values. However, it is often said that if data are missing at random, valid inference using likelihood approaches (including Bayesian) can be obtained ignoring the missingness mechanism. Unfortunately, the term ``missing at random'' has been used inconsistently and not always clearly; there has also been a lack of clarity around the meaning of ``valid inference using likelihood''. These issues have created potential for confusion about the exact conditions under which the missingness mechanism can be ignored, and perhaps fed confusion around the meaning of ``analysis ignoring the missingness mechanism''. Here we provide standardised precise definitions of ``missing at random'' and ``missing completely at random'', in order to promote unification of the theory. Using these definitions we clarify the conditions that suffice for ``valid inference'' to be obtained under a variety of inferential paradigms.},
  keywords = {direct-likelihood inference,frequentist inference,ignorability,missing completely at random,repeated sampling},
  file = {C:\Users\efbonneville\Zotero\storage\7G84W8KD\Seaman et al. - 2013 - What Is Meant by “Missing at Random”.pdf}
}

@article{shahComparisonRandomForest2014,
  title = {Comparison of {{Random Forest}} and {{Parametric Imputation Models}} for {{Imputing Missing Data Using MICE}}: {{A CALIBER Study}}},
  shorttitle = {Comparison of {{Random Forest}} and {{Parametric Imputation Models}} for {{Imputing Missing Data Using MICE}}},
  author = {Shah, Anoop D. and Bartlett, Jonathan W. and Carpenter, James and Nicholas, Owen and Hemingway, Harry},
  year = {2014},
  month = mar,
  journal = {American Journal of Epidemiology},
  volume = {179},
  number = {6},
  pages = {764--774},
  issn = {0002-9262},
  doi = {10.1093/aje/kwt312},
  urldate = {2022-10-10},
  abstract = {Multivariate imputation by chained equations (MICE) is commonly used for imputing missing data in epidemiologic research. The ``true'' imputation model may contain nonlinearities which are not included in default imputation models. Random forest imputation is a machine learning technique which can accommodate nonlinearities and interactions and does not require a particular regression model to be specified. We compared parametric MICE with a random forest-based MICE algorithm in 2 simulation studies. The first study used 1,000 random samples of 2,000 persons drawn from the 10,128 stable angina patients in the CALIBER database (Cardiovascular Disease Research using Linked Bespoke Studies and Electronic Records; 2001--2010) with complete data on all covariates. Variables were artificially made ``missing at random,'' and the bias and efficiency of parameter estimates obtained using different imputation methods were compared. Both MICE methods produced unbiased estimates of (log) hazard ratios, but random forest was more efficient and produced narrower confidence intervals. The second study used simulated data in which the partially observed variable depended on the fully observed variables in a nonlinear way. Parameter estimates were less biased using random forest MICE, and confidence interval coverage was better. This suggests that random forest imputation may be useful for imputing complex epidemiologic data sets in which some patients have missing data.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\EQFGJT9Z\\Shah et al. - 2014 - Comparison of Random Forest and Parametric Imputat.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\8ZNXESK3\\107562.html}
}

@article{sharmaOutcomesPediatricPatients2021,
  title = {Outcomes of Pediatric Patients with Therapy-Related Myeloid Neoplasms},
  author = {Sharma, A. and {Huang S.} and {Li Y.} and {Brooke R.J.} and {Ahmed I.} and {Allewelt H.B.} and {Amrolia P.} and {Bertaina A.} and {Bhatt N.S.} and {Bierings M.B.} and {Bies J.} and {Brisset C.} and {Brondon J.E.} and {Dahlberg A.} and {Dalle J.-H.} and {Eissa H.} and {Fahd M.} and {Gassas A.} and {Gloude N.J.} and {Goebel W.S.} and {Goeckerman E.S.} and {Harris K.} and {Ho R.} and {Hudspeth M.P.} and {Huo J.S.} and {Jacobsohn D.} and {Kasow K.A.} and {Katsanis E.} and {Kaviany S.} and {Keating A.K.} and {Kernan N.A.} and {Ktena Y.P.} and {Lauhan C.R.} and {Lopez-Hernandez G.} and {Martin P.L.} and {Myers K.C.} and {Naik S.} and {Olaya-Vargas A.} and {Onishi T.} and {Radhi M.} and {Ramachandran S.} and {Ramos K.} and {Rangarajan H.G.} and {Roehrs P.A.} and {Sampson M.E.} and {Shaw P.J.} and {Skiles J.L.} and {Somers K.} and {Symons H.J.} and {de Tersant M.} and {Uber A.N.} and {Versluys B.} and {Cheng C.} and {Triplett B.M.}},
  year = {2021},
  journal = {Bone Marrow Transplantation},
  volume = {56},
  number = {12},
  pages = {2997--3007},
  publisher = {Springer Nature},
  address = {United Kingdom},
  issn = {0268-3369},
  doi = {10.1038/s41409-021-01448-x},
  abstract = {Long-term outcomes after allogeneic hematopoietic cell transplantation (HCT) for therapy-related myeloid neoplasms (tMNs) are dismal. There are few multicenter studies defining prognostic factors in pediatric patients with tMNs. We have accumulated the largest cohort of pediatric patients who have undergone HCT for a tMN to perform a multivariate analysis defining factors predictive of long-term survival. Sixty-eight percent of the 401 patients underwent HCT using a myeloablative conditioning (MAC) regimen, but there were no statistically significant differences in the overall survival (OS), event-free survival (EFS), or cumulative incidence of relapse and non-relapse mortality based on the conditioning intensity. Among the recipients of MAC regimens, 38.4\% of deaths were from treatment-related causes, especially acute graft versus host disease (GVHD) and end-organ failure, as compared to only 20.9\% of deaths in the reduced-intensity conditioning (RIC) cohort. Exposure to total body irradiation (TBI) during conditioning and experiencing grade III/IV acute GVHD was associated with worse OS. In addition, a diagnosis of therapy-related myelodysplastic syndrome and having a structurally complex karyotype at tMN diagnosis were associated with worse EFS. Reduced-toxicity (but not reduced-intensity) regimens might help to decrease relapse while limiting mortality associated with TBI-based HCT conditioning in pediatric patients with tMNs.Copyright {\copyright} 2021, The Author(s), under exclusive licence to Springer Nature Limited.},
  langid = {english},
  keywords = {*bone marrow tumor,*bone marrow tumor/co [Complication],*bone marrow tumor/di [Diagnosis],*bone marrow tumor/si [Side Effect],*clinical outcome,*pediatric patient,*relapse,acute graft versus host disease,acute graft versus host disease/co [Complication],adolescent,adverse outcome,article,busulfan/iv [Intravenous Drug Administration],busulfan/pv [Special Situation for Pharmacovigilance],cause of death,child,cohort analysis,controlled study,cumulative incidence,cytotoxic agent/ae [Adverse Drug Reaction],cytotoxic agent/pv [Special Situation for Pharmacovigilance],event free survival,female,graft recipient,hematopoietic stem cell transplantation,human,incidence,infant,karyotype,long term survival,major clinical study,male,melphalan/iv [Intravenous Drug Administration],melphalan/pv [Special Situation for Pharmacovigilance],mortality,mortality rate,multicenter study (topic),multivariate analysis,myeloablative conditioning,myelodysplastic syndrome,outcome assessment,overall survival,radiation exposure,recurrence risk,reduced intensity conditioning,survival time,whole body radiation},
  file = {C:\Users\efbonneville\Zotero\storage\4BJXXE74\Sharma A. et al. - 2021 - Outcomes of pediatric patients with therapy-relate.pdf}
}

@article{shawImportanceHLADPB1Unrelated2007,
  title = {The Importance of {{HLA-DPB1}} in Unrelated Donor Hematopoietic Cell Transplantation},
  author = {Shaw, Bronwen E. and Gooley, Theodore A. and Malkki, Mari and Madrigal, J. Alejandro and Begovich, Ann B. and Horowitz, Mary M. and Gratwohl, Alois and Ringd{\'e}n, Olle and Marsh, Steven G. E. and Petersdorf, Effie W.},
  year = {2007},
  month = dec,
  journal = {Blood},
  volume = {110},
  number = {13},
  pages = {4560--4566},
  issn = {0006-4971},
  doi = {10.1182/blood-2007-06-095265},
  urldate = {2023-11-04},
  abstract = {Hematopoietic cell transplantation (HCT) from an HLA-A, HLA-B, HLA-C, HLA-DRB1, and HLA-DQB1 allele--matched unrelated donor is a well-recognized life-saving treatment modality for patients with hematologic disorders. The morbidity and mortality from clinically significant acute graft-versus-host disease (aGVHD) remains a limitation. The extent to which transplantation outcome may be improved with donor matching for HLA-DP is not well defined. The risks of aGVHD, relapse, and mortality associated with HLA-DPB1 allele mismatching were determined in 5929 patients who received a myeloablative HCT from an HLA-A--, HLA-B--, HLA-C--, HLA-DRB1--, and HLA-DQB1--matched or --mismatched donor. There was a statistically significantly higher risk of both grades 2 to 4 aGVHD (odds ratio [OR] = 1.33; P \&lt; .001) and grades 3 to 4 aGVHD (OR = 1.26; P \&lt; .001) after HCT from an HLA-DPB1--mismatched donor compared with a matched donor. The increased risk of aGVHD was accompanied by a statistically significantly decrease in disease relapse (hazard ratio [HR] = 0.82; P = .01). HLA-DPB1 functions as a classical transplantation antigen. The increased risk of GVHD associated with HLA-DPB1 mismatching is accompanied by a lower risk of relapse. Knowledge of the DPB1 matching status prior to transplantation will aid in more precise risk stratification for the individual patient.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\LQ3R5CZF\\Shaw et al. - 2007 - The importance of HLA-DPB1 in unrelated donor hema.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\KFG6LGDI\\The-importance-of-HLA-DPB1-in-unrelated-donor.html}
}

@article{shiConstrainedParametricModel2013,
  title = {Constrained Parametric Model for Simultaneous Inference of Two Cumulative Incidence Functions},
  author = {Shi, Haiwen and Cheng, Yu and Jeong, Jong-Hyeon},
  year = {2013},
  journal = {Biometrical Journal},
  volume = {55},
  number = {1},
  pages = {82--96},
  issn = {1521-4036},
  doi = {10.1002/bimj.201200011},
  urldate = {2023-10-07},
  abstract = {We propose a parametric regression model for the cumulative incidence functions (CIFs) commonly used for competing risks data. The model adopts a modified logistic model as the baseline CIF and a generalized odds-rate model for covariate effects, and it explicitly takes into account the constraint that a subject with any given prognostic factors should eventually fail from one of the causes such that the asymptotes of the CIFs should add up to one. This constraint intrinsically holds in a nonparametric analysis without covariates, but is easily overlooked in a semiparametric or parametric regression setting. We hence model the CIF from the primary cause assuming the generalized odds-rate transformation and the modified logistic function as the baseline CIF. Under the additivity constraint, the covariate effects on the competing cause are modeled by a function of the asymptote of the baseline distribution and the covariate effects on the primary cause. The inference procedure is straightforward by using the standard maximum likelihood theory. We demonstrate desirable finite-sample performance of our model by simulation studies in comparison with existing methods. Its practical utility is illustrated in an analysis of a breast cancer dataset to assess the treatment effect of tamoxifen, adjusting for age and initial pathological tumor size, on breast cancer recurrence that is subject to dependent censoring by second primary cancers and deaths.},
  copyright = {{\copyright} 2012 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  langid = {english},
  keywords = {Cause-specific hazard function,Cumulative incidence function,Long-term incidence,Modified three-parameter logistic model,Parametric modeling and odds rate},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\6D4GRZGS\\Shi et al. - 2013 - Constrained parametric model for simultaneous infe.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\69GS3ZVG\\bimj.html}
}

@article{shimoniLongtermSurvivalLate2016,
  title = {Long-Term Survival and Late Events after Allogeneic Stem Cell Transplantation from {{HLA-matched}} Siblings for Acute Myeloid Leukemia with Myeloablative Compared to Reduced-Intensity Conditioning: A Report on Behalf of the Acute Leukemia Working Party of {{European}} Group for Blood and Marrow Transplantation},
  shorttitle = {Long-Term Survival and Late Events after Allogeneic Stem Cell Transplantation from {{HLA-matched}} Siblings for Acute Myeloid Leukemia with Myeloablative Compared to Reduced-Intensity Conditioning},
  author = {Shimoni, Avichai and Labopin, Myriam and Savani, Bipin and Volin, Liisa and Ehninger, Gerhard and Kuball, Jurgen and Bunjes, Donald and Schaap, Nicolaas and Vigouroux, Stephane and Bacigalupo, Andrea and Veelken, Hendrik and Sierra, Jorge and Eder, Matthias and Niederwieser, Dietger and Mohty, Mohamad and Nagler, Arnon},
  year = {2016},
  month = nov,
  journal = {Journal of Hematology \& Oncology},
  volume = {9},
  number = {1},
  pages = {118},
  issn = {1756-8722},
  doi = {10.1186/s13045-016-0347-1},
  urldate = {2023-10-09},
  abstract = {Myeloablative (MAC) and reduced-intensity conditioning (RIC) are established approaches for allogeneic stem cell transplantation (SCT) in acute myeloid leukemia (AML). Most deaths after MAC occur within the first 2~years after SCT, while patients surviving leukemia-free for 2~years can expect a favorable long-term outcome. However, there is paucity of data on the long-term outcome (beyond 10~years) and the pattern of late events following RIC due to the relative recent introduction of this approach.},
  langid = {english},
  keywords = {Acute myeloid leukemia,Allogeneic stem cell transplantation,Long-term outcome,Myeloablative conditioning,Reduced-intensity conditioning},
  file = {C:\Users\efbonneville\Zotero\storage\Q4YNZLUF\Shimoni et al. - 2016 - Long-term survival and late events after allogenei.pdf}
}

@article{simonettaNaturalKillerCells2017,
  title = {Natural {{Killer Cells}} in {{Graft-versus-Host-Disease}} after {{Allogeneic Hematopoietic Cell Transplantation}}},
  author = {Simonetta, Federico and Alvarez, Maite and Negrin, Robert S.},
  year = {2017},
  journal = {Frontiers in Immunology},
  volume = {8},
  issn = {1664-3224},
  urldate = {2023-11-04},
  abstract = {Allogeneic hematopoietic cell transplantation (HCT) is a well-established therapeutic modality effective for a variety of hematological malignancies but, unfortunately, is associated with significant morbidity and mortality related to cancer relapse as well as to transplant-related complications including graft-versus-host-disease (GvHD). Natural killer (NK) cells are the first donor-derived lymphocyte subset to recover after HCT, and their crucial role in protection against cancer relapse and infections is well established. Conversely, the role played by NK cells in GvHD is still controversial. Early studies suggested a participation of NK cells in GvHD induction or exacerbation. Subsequently, experimental evidence obtained in mice as well observational studies performed in humans led to a model in which NK cells play a regulatory role in GvHD by repressing alloreactive T cell responses. This widely accepted model has been recently challenged by clinical evidence indicating that NK cells can in some cases promote GvHD. In this review, we summarize available knowledge about the role of NK cells in GVHD pathogenesis. We review studies uncovering cellular mechanisms through which NK cells interact with other immune cell subsets during GvHD leading to a model in which NK cells naturally suppress GvHD through their cytotoxic ability to inhibit T cell activation unless exogenous hyperactivation lead them to produce proinflammatory cytokines that can conversely sustain T cell-mediated GvHD induction.},
  file = {C:\Users\efbonneville\Zotero\storage\G3SGGRIQ\Simonetta et al. - 2017 - Natural Killer Cells in Graft-versus-Host-Disease .pdf}
}

@article{siskImputationMissingIndicators2023,
  title = {Imputation and Missing Indicators for Handling Missing Data in the Development and Deployment of Clinical Prediction Models: {{A}} Simulation Study},
  shorttitle = {Imputation and Missing Indicators for Handling Missing Data in the Development and Deployment of Clinical Prediction Models},
  author = {Sisk, Rose and Sperrin, Matthew and Peek, Niels and {van Smeden}, Maarten and Martin, Glen Philip},
  year = {2023},
  month = aug,
  journal = {Statistical Methods in Medical Research},
  volume = {32},
  number = {8},
  pages = {1461--1477},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/09622802231165001},
  urldate = {2023-11-06},
  abstract = {Background: In clinical prediction modelling, missing data can occur at any stage of the model pipeline; development, validation or deployment. Multiple imputation is often recommended yet challenging to apply at deployment; for example, the outcome cannot be in the imputation model, as recommended under multiple imputation. Regression imputation uses a fitted model to impute the predicted value of missing predictors from observed data, and could offer a pragmatic alternative at deployment. Moreover, the use of missing indicators has been proposed to handle informative missingness, but it is currently unknown how well this method performs in the context of clinical prediction models. Methods: We simulated data under various missing data mechanisms to compare the predictive performance of clinical prediction models developed using both imputation methods. We consider deployment scenarios where missing data is permitted or prohibited, imputation models that use or omit the outcome, and clinical prediction models that include or omit missing indicators. We assume that the missingness mechanism remains constant across the model pipeline. We also apply the proposed strategies to critical care data. Results: With complete data available at deployment, our findings were in line with existing recommendations; that the outcome should be used to impute development data when using multiple imputation and omitted under regression imputation. When missingness is allowed at deployment, omitting the outcome from the imputation model at the development was preferred. Missing indicators improved model performance in many cases but can be harmful under outcome-dependent missingness. Conclusion: We provide evidence that commonly taught principles of handling missing data via multiple imputation may not apply to clinical prediction models, particularly when data can be missing at deployment. We observed comparable predictive performance under multiple imputation and regression imputation. The performance of the missing data handling method must be evaluated on a study-by-study basis, and the most appropriate strategy for handling missing data at development should consider whether missing data are allowed at deployment. Some guidance is provided.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\B32D8HNN\Sisk et al. - 2023 - Imputation and missing indicators for handling mis.pdf}
}

@article{smedenCautionaryNoteUse2020,
  title = {A Cautionary Note on the Use of the Missing Indicator Method for Handling Missing Data in Prediction Research},
  author = {van Smeden, Maarten and Groenwold, Rolf H. H. and Moons, Karel GM},
  year = {2020},
  month = sep,
  journal = {Journal of Clinical Epidemiology},
  volume = {125},
  pages = {188--190},
  publisher = {Elsevier},
  issn = {0895-4356, 1878-5921},
  doi = {10.1016/j.jclinepi.2020.06.007},
  urldate = {2023-11-07},
  langid = {english},
  pmid = {32565213},
  keywords = {Measurement,Missing data,Missing indicator method,Missingness mechanisms,Prediction models,Transportability},
  file = {C:\Users\efbonneville\Zotero\storage\7T9LMHB6\Smeden et al. - 2020 - A cautionary note on the use of the missing indica.pdf}
}

@article{snowdenBenchmarkingSurvivalOutcomes2020,
  title = {Benchmarking of Survival Outcomes Following Haematopoietic Stem Cell Transplantation: {{A}} Review of Existing Processes and the Introduction of an International System from the {{European Society}} for {{Blood}} and {{Marrow Transplantation}} ({{EBMT}}) and the {{Joint Accreditation Committee}} of {{ISCT}} and {{EBMT}} ({{JACIE}})},
  shorttitle = {Benchmarking of Survival Outcomes Following Haematopoietic Stem Cell Transplantation},
  author = {Snowden, John A. and Saccardi, Riccardo and Orchard, Kim and Ljungman, Per and Duarte, Rafael F. and Labopin, Myriam and McGrath, Eoin and Brook, Nigel and {de Elvira}, Carmen Ruiz and Gordon, Debra and Poirel, H{\'e}l{\`e}ne A. and Ayuk, Francis and Beguin, Yves and Bonifazi, Francesca and Gratwohl, Alois and Milpied, Noel and Moore, John and Passweg, Jakob and Rizzo, J. Douglas and Spellman, Stephen R. and Sierra, Jorge and Solano, Carlos and {Sanchez-Guijo}, Fermin and Worel, Nina and Gusi, Andreu and Adams, Gillian and Balan, Theodor and Baldomero, Helen and Macq, Gilles and Marry, Evelyne and Mesnil, Florence and Oldani, Elena and Pearce, Rachel and Perry, Julia and Raus, Nicole and Schanz, Urs and Tran, Steven and Wilcox, Leonie and Basak, Grzegorz W. and Chabannon, Christian and Corbacioglu, Selim and Dolstra, Harry and Kuball, J{\"u}rgen and Mohty, Mohamad and Lankester, Arjan and Montoto, Sylvia and Nagler, Arnon and Styczynski, Jan and {Yakoub-Agha}, Ibrahim and {de Latour}, Regis Peffault and Kroeger, Nicolaus and Brand, Ronald and {de Wreede}, Liesbeth C. and {van Zwet}, Erik and Putter, Hein},
  year = {2020},
  journal = {Bone Marrow Transplantation},
  volume = {55},
  number = {4},
  pages = {681--694},
  issn = {0268-3369},
  doi = {10.1038/s41409-019-0718-7},
  urldate = {2022-02-28},
  abstract = {In many healthcare settings, benchmarking for complex procedures has become a mandatory requirement by competent authorities, regulators, payers and patients to assure clinical performance, cost-effectiveness and safe care of patients. In several countries inside and outside Europe, benchmarking systems have been established for haematopoietic stem cell transplantation (HSCT), but access is not universal. As benchmarking is now integrated into the FACT-JACIE standards, the EBMT and JACIE established a Clinical Outcomes Group (COG) to develop and introduce a universal system accessible across EBMT members. Established systems from seven European countries (United Kingdom, Italy, Belgium, France, Germany, Spain, Switzerland), USA and Australia were appraised, revealing similarities in process, but wide variations in selection criteria and statistical methods. In tandem, the COG developed the first phase of a bespoke risk-adapted international benchmarking model for one-year survival following allogeneic and autologous HSCT based on current capabilities within the EBMT registry core dataset. Data completeness, which has a critical impact on validity of centre comparisons, is also assessed. Ongoing development will include further scientific validation of the model, incorporation of further variables (when appropriate) alongside implementation of systems for clinically meaningful interpretation and governance aiming to maximise acceptance to centres, clinicians, payers and patients across EBMT.},
  pmcid = {PMC7113189},
  pmid = {31636397}
}

@article{sorrorHematopoieticCellTransplantation2005a,
  title = {Hematopoietic Cell Transplantation ({{HCT}})-Specific Comorbidity Index: A New Tool for Risk Assessment before Allogeneic {{HCT}}},
  shorttitle = {Hematopoietic Cell Transplantation ({{HCT}})-Specific Comorbidity Index},
  author = {Sorror, Mohamed L. and Maris, Michael B. and Storb, Rainer and Baron, Frederic and Sandmaier, Brenda M. and Maloney, David G. and Storer, Barry},
  year = {2005},
  month = oct,
  journal = {Blood},
  volume = {106},
  number = {8},
  pages = {2912--2919},
  issn = {0006-4971},
  doi = {10.1182/blood-2005-05-2004},
  urldate = {2024-10-14},
  abstract = {We previously reported that the Charlson Comorbidity Index (CCI) was useful for predicting outcomes in patients undergoing allogeneic hematopoietic cell transplantation (HCT). However, the sample size of patients with scores of 1 or more, captured by the CCI, did not exceed 35\%. Further, some comorbidities were rarely found among patients who underwent HCT. Therefore, the current study was designed to (1) better define previously identified comorbidities using pretransplant laboratory data, (2) investigate additional HCT-related comorbidities, and (3) establish comorbidity scores that were suited for HCT. Data were collected from 1055 patients, and then randomly divided into training and validation sets. Weights were assigned to individual comorbidities according to their prognostic significance in Cox proportional hazard models. The new index was then validated. The new index proved to be more sensitive than the CCI since it captured 62\% of patients with scores more than 0 compared with 12\%, respectively. Further, the new index showed better survival prediction than the CCI (likelihood ratio of 23.7 versus 7.1 and c statistics of 0.661 versus 0.561, respectively, P \&lt; .001). In conclusion, the new simple index provided valid and reliable scoring of pretransplant comorbidities that predicted nonrelapse mortality and survival. This index will be useful for clinical trials and patient counseling before HCT. (Blood. 2005;106: 2912-2919)},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\TM28AFV2\\Sorror et al. - 2005 - Hematopoietic cell transplantation (HCT)-specific .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\J3S59K5X\\Hematopoietic-cell-transplantation-HCT-specific.html}
}

@article{sperrinMissingDataShould2020,
  title = {Missing Data Should Be Handled Differently for Prediction than for Description or Causal Explanation},
  author = {Sperrin, Matthew and Martin, Glen P. and Sisk, Rose and Peek, Niels},
  year = {2020},
  month = sep,
  journal = {Journal of Clinical Epidemiology},
  volume = {125},
  pages = {183--187},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2020.03.028},
  urldate = {2023-11-06},
  abstract = {Missing data are much studied in epidemiology and statistics. Theoretical development and application of methods for handling missing data have mostly been conducted in the context of prospective research data and with a goal of description or causal explanation. However, it is now common to build predictive models using routinely collected data, where missing patterns may convey important information, and one might take a pragmatic approach to optimizing prediction. Therefore, different methods to handle missing data may be preferred. Furthermore, an underappreciated issue in prediction modeling is that the missing data method used in model development may not match the method used when a model is deployed. This may lead to overoptimistic assessments of model performance. For prediction, particularly with routinely collected data, methods for handling missing data that incorporate information within the missingness pattern should be explored and further developed. Where missing data methods differ between model development and model deployment, the implications of this must be explicitly evaluated. The trade-off between building a prediction model that is causally principled, and building a prediction model that maximizes the use of all available information, should be carefully considered and will depend on the intended use of the model.},
  keywords = {Clinical prediction models,Missing data,Model performance,Multiple imputation,Prognostic model,Routinely collected data},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\24936K52\\Sperrin et al. - 2020 - Missing data should be handled differently for pre.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\QLR9IFXD\\S0895435619307668.html}
}

@article{stensrudSeparableEffectsCausal2022,
  title = {Separable {{Effects}} for {{Causal Inference}} in the {{Presence}} of {{Competing Events}}},
  author = {Stensrud, Mats J. and Young, Jessica G. and Didelez, Vanessa and Robins, James M. and Hern{\'a}n, Miguel A.},
  year = {2022},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {117},
  number = {537},
  pages = {175--183},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2020.1765783},
  urldate = {2024-06-18},
  abstract = {In time-to-event settings, the presence of competing events complicates the definition of causal effects. Here we propose the new separable effects to study the causal effect of a treatment on an event of interest. The separable direct effect is the treatment effect on the event of interest not mediated by its effect on the competing event. The separable indirect effect is the treatment effect on the event of interest only through its effect on the competing event. Similar to Robins and Richardson's extended graphical approach for mediation analysis, the separable effects can only be identified under the assumption that the treatment can be decomposed into two distinct components that exert their effects through distinct causal pathways. Unlike existing definitions of causal effects in the presence of competing events, our estimands do not require cross-world contrasts or hypothetical interventions to prevent death. As an illustration, we apply our approach to a randomized clinical trial on estrogen therapy in individuals with prostate cancer. Supplementary materials for this article are available online.},
  keywords = {Competing risks,Direct and indirect effects,Failure time analysis,Lifetime and survival analysis,Separable effects},
  file = {C:\Users\efbonneville\Zotero\storage\KDQUCASZ\Stensrud et al. - 2022 - Separable Effects for Causal Inference in the Pres.pdf}
}

@article{sterneMultipleImputationMissing2009,
  title = {Multiple Imputation for Missing Data in Epidemiological and Clinical Research: Potential and Pitfalls},
  shorttitle = {Multiple Imputation for Missing Data in Epidemiological and Clinical Research},
  author = {Sterne, Jonathan A. C. and White, Ian R. and Carlin, John B. and Spratt, Michael and Royston, Patrick and Kenward, Michael G. and Wood, Angela M. and Carpenter, James R.},
  year = {2009},
  month = jun,
  journal = {BMJ},
  volume = {338},
  pages = {b2393},
  publisher = {British Medical Journal Publishing Group},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.b2393},
  urldate = {2022-10-16},
  abstract = {{$<$}p{$>$}Most studies have some missing data. \textbf{Jonathan Sterne and colleagues} describe the appropriate use and reporting of the multiple imputation approach to dealing with them {$<$}/p{$>$}},
  chapter = {Research Methods \&amp; Reporting},
  copyright = {{\copyright}  . This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits use, distribution, and reproduction in any medium, provided the original work is properly cited, the use is non commercial and is otherwise in compliance with the license. See: http://creativecommons.org/licenses/by-nc/2.0/  and  http://creativecommons.org/licenses/by-nc/2.0/legalcode.},
  langid = {english},
  pmid = {19564179},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\DYVB8S35\\Sterne et al. - 2009 - Multiple imputation for missing data in epidemiolo.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\CRW6MFRB\\bmj.html}
}

@article{sterneROBINSIToolAssessing2016,
  title = {{{ROBINS-I}}: A Tool for Assessing Risk of Bias in Non-Randomised Studies of Interventions},
  shorttitle = {{{ROBINS-I}}},
  author = {Sterne, Jonathan AC and Hern{\'a}n, Miguel A. and Reeves, Barnaby C. and Savovi{\'c}, Jelena and Berkman, Nancy D. and Viswanathan, Meera and Henry, David and Altman, Douglas G. and Ansari, Mohammed T. and Boutron, Isabelle and Carpenter, James R. and Chan, An-Wen and Churchill, Rachel and Deeks, Jonathan J. and Hr{\'o}bjartsson, Asbj{\o}rn and Kirkham, Jamie and J{\"u}ni, Peter and Loke, Yoon K. and Pigott, Theresa D. and Ramsay, Craig R. and Regidor, Deborah and Rothstein, Hannah R. and Sandhu, Lakhbir and Santaguida, Pasqualina L. and Sch{\"u}nemann, Holger J. and Shea, Beverly and Shrier, Ian and Tugwell, Peter and Turner, Lucy and Valentine, Jeffrey C. and Waddington, Hugh and Waters, Elizabeth and Wells, George A. and Whiting, Penny F. and Higgins, Julian PT},
  year = {2016},
  month = oct,
  journal = {BMJ},
  volume = {355},
  pages = {i4919},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj.i4919},
  urldate = {2022-12-04},
  abstract = {{$<$}p{$>$}Non-randomised studies of the effects of interventions are critical to many areas of healthcare evaluation, but their results may be biased. It is therefore important to understand and appraise their strengths and weaknesses. We developed ROBINS-I (``Risk Of Bias In Non-randomised Studies - of Interventions''), a new tool for evaluating risk of bias in estimates of the comparative effectiveness (harm or benefit) of interventions from studies that did not use randomisation to allocate units (individuals or clusters of individuals) to comparison groups. The tool will be particularly useful to those undertaking systematic reviews that include non-randomised studies.{$<$}/p{$>$}},
  chapter = {Research Methods \&amp; Reporting},
  copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 3.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/3.0/.},
  langid = {english},
  pmid = {27733354},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\T7V3XMMX\\Sterne et al. - 2016 - ROBINS-I a tool for assessing risk of bias in non.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\JYSSQVDQ\\bmj.html}
}

@article{sternImmunoprofilingRevealsCell2022,
  title = {Immunoprofiling Reveals Cell Subsets Associated with the Trajectory of Cytomegalovirus Reactivation Post Stem Cell Transplantation},
  author = {Stern, Lauren and McGuire, Helen M. and Avdic, Selmir and {Fazekas de St Groth}, Barbara and Gottlieb, David and Abendroth, Allison and Blyth, Emily and Slobedman, Barry},
  year = {2022},
  month = may,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {2603},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-29943-9},
  urldate = {2023-11-04},
  abstract = {Human cytomegalovirus reactivation is a major opportunistic infection after allogeneic haematopoietic stem cell transplantation and has a complex relationship with post-transplant immune reconstitution. Here, we use mass cytometry to define patterns of innate and adaptive immune cell reconstitution at key phases of human cytomegalovirus reactivation in the first 100 days post haematopoietic stem cell transplantation. Human cytomegalovirus reactivation is associated with the development of activated, memory T-cell profiles, with faster effector-memory CD4+ T-cell recovery in patients with low-level versus high-level human cytomegalovirus DNAemia. Mucosal-associated invariant T cell levels at the initial detection of human cytomegalovirus DNAemia are significantly lower in patients who subsequently develop high-level versus low-level human cytomegalovirus reactivation. Our data describe distinct immune signatures that emerged with human cytomegalovirus reactivation after haematopoietic stem cell transplantation, and highlight Mucosal-associated invariant T cell levels at the first detection of reactivation as a marker that may be useful to anticipate the magnitude of human cytomegalovirus DNAemia.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Herpes virus,Infection,Transplant immunology,Viral infection},
  file = {C:\Users\efbonneville\Zotero\storage\7ICCMLRK\Stern et al. - 2022 - Immunoprofiling reveals cell subsets associated wi.pdf}
}

@article{sullivanShouldMultipleImputation2018,
  title = {Should Multiple Imputation Be the Method of Choice for Handling Missing Data in Randomized Trials?},
  author = {Sullivan, Thomas R and White, Ian R and Salter, Amy B and Ryan, Philip and Lee, Katherine J},
  year = {2018},
  month = sep,
  journal = {Statistical Methods in Medical Research},
  volume = {27},
  number = {9},
  pages = {2610--2626},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280216683570},
  urldate = {2023-01-27},
  abstract = {The use of multiple imputation has increased markedly in recent years, and journal reviewers may expect to see multiple imputation used to handle missing data. However in randomized trials, where treatment group is always observed and independent of baseline covariates, other approaches may be preferable. Using data simulation we evaluated multiple imputation, performed both overall and separately by randomized group, across a range of commonly encountered scenarios. We considered both missing outcome and missing baseline data, with missing outcome data induced under missing at random mechanisms. Provided the analysis model was correctly specified, multiple imputation produced unbiased treatment effect estimates, but alternative unbiased approaches were often more efficient. When the analysis model overlooked an interaction effect involving randomized group, multiple imputation produced biased estimates of the average treatment effect when applied to missing outcome data, unless imputation was performed separately by randomized group. Based on these results, we conclude that multiple imputation should not be seen as the only acceptable way to handle missing data in randomized trials. In settings where multiple imputation is adopted, we recommend that imputation is carried out separately by randomized group.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\3FTUBQTF\Sullivan et al. - 2018 - Should multiple imputation be the method of choice.pdf}
}

@article{sullivanTreatmentMissingData2017,
  title = {Treatment of Missing Data in Follow-up Studies of Randomised Controlled Trials: {{A}} Systematic Review of the Literature},
  shorttitle = {Treatment of Missing Data in Follow-up Studies of Randomised Controlled Trials},
  author = {Sullivan, Thomas R and Yelland, Lisa N and Lee, Katherine J and Ryan, Philip and Salter, Amy B},
  year = {2017},
  month = aug,
  journal = {Clinical Trials},
  volume = {14},
  number = {4},
  pages = {387--395},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/1740774517703319},
  urldate = {2022-10-10},
  abstract = {Background/aims:After completion of a randomised controlled trial, an extended follow-up period may be initiated to learn about longer term impacts of the intervention. Since extended follow-up studies often involve additional eligibility restrictions and consent processes for participation, and a longer duration of follow-up entails a greater risk of participant attrition, missing data can be a considerable threat in this setting. As a potential source of bias, it is critical that missing data are appropriately handled in the statistical analysis, yet little is known about the treatment of missing data in extended follow-up studies. The aims of this review were to summarise the extent of missing data in extended follow-up studies and the use of statistical approaches to address this potentially serious problem.Methods:We performed a systematic literature search in PubMed to identify extended follow-up studies published from January to June 2015. Studies were eligible for inclusion if the original randomised controlled trial results were also published and if the main objective of extended follow-up was to compare the original randomised groups. We recorded information on the extent of missing data and the approach used to treat missing data in the statistical analysis of the primary outcome of the extended follow-up study.Results:Of the 81 studies included in the review, 36 (44\%) reported additional eligibility restrictions and 24 (30\%) consent processes for entry into extended follow-up. Data were collected at a median of 7?years after randomisation. Excluding 28 studies with a time to event primary outcome, 51/53 studies (96\%) reported missing data on the primary outcome. The median percentage of randomised participants with complete data on the primary outcome was just 66\% in these studies. The most common statistical approach to address missing data was complete case analysis (51\% of studies), while likelihood-based analyses were also well represented (25\%). Sensitivity analyses around the missing data mechanism were rarely performed (25\% of studies), and when they were, they often involved unrealistic assumptions about the mechanism.Conclusion:Despite missing data being a serious problem in extended follow-up studies, statistical approaches to addressing missing data were often inadequate. We recommend researchers clearly specify all sources of missing data in follow-up studies and use statistical methods that are valid under a plausible assumption about the missing data mechanism. Sensitivity analyses should also be undertaken to assess the robustness of findings to assumptions about the missing data mechanism.},
  langid = {english},
  file = {C:\Users\efbonneville\Zotero\storage\57EYLEJL\Sullivan et al. - 2017 - Treatment of missing data in follow-up studies of .pdf}
}

@manual{survival-package,
  type = {Manual},
  title = {A Package for Survival Analysis in {{R}}},
  author = {Therneau, Terry M},
  year = {2023}
}

@article{sweetingJointModellingLongitudinal2011,
  title = {Joint Modelling of Longitudinal and Time-to-Event Data with Application to Predicting Abdominal Aortic Aneurysm Growth and Rupture},
  author = {Sweeting, Michael J. and Thompson, Simon G.},
  year = {2011},
  journal = {Biometrical Journal},
  volume = {53},
  number = {5},
  pages = {750--763},
  issn = {1521-4036},
  doi = {10.1002/bimj.201100052},
  urldate = {2024-08-27},
  abstract = {Shared random effects joint models are becoming increasingly popular for investigating the relationship between longitudinal and time-to-event data. Although appealing, such complex models are computationally intensive, and quick, approximate methods may provide a reasonable alternative. In this paper, we first compare the shared random effects model with two approximate approaches: a na{\"i}ve proportional hazards model with time-dependent covariate and a two-stage joint model, which uses plug-in estimates of the fitted values from a longitudinal analysis as covariates in a survival model. We show that the approximate approaches should be avoided since they can severely underestimate any association between the current underlying longitudinal value and the event hazard. We present classical and Bayesian implementations of the shared random effects model and highlight the advantages of the latter for making predictions. We then apply the models described to a study of abdominal aortic aneurysms (AAA) to investigate the association between AAA diameter and the hazard of AAA rupture. Out-of-sample predictions of future AAA growth and hazard of rupture are derived from Bayesian posterior predictive distributions, which are easily calculated within an MCMC framework. Finally, using a multivariate survival sub-model we show that underlying diameter rather than the rate of growth is the most important predictor of AAA rupture.},
  copyright = {Copyright {\copyright} 2011 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  langid = {english},
  keywords = {Abdominal aortic aneurysm,Hierarchical model,Joint model,Prediction,Shared random effects},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\6PN7BZ5G\\Sweeting and Thompson - 2011 - Joint modelling of longitudinal and time-to-event .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\DIVE7ATD\\bimj.html}
}

@article{tangIncreasingChimerismAllogeneic2014a,
  title = {Increasing {{Chimerism}} after {{Allogeneic Stem Cell Transplantation Is Associated}} with {{Longer Survival Time}}},
  author = {Tang, Xiaowen and Alatrash, Gheath and Ning, Jing and Jakher, Haroon and Stafford, Patricia and Zope, Madhushree and Shpall, Elizabeth J. and Jones, Roy B. and Champlin, Richard E. and Thall, Peter F. and Andersson, Borje S.},
  year = {2014},
  month = aug,
  journal = {Biology of Blood and Marrow Transplantation},
  volume = {20},
  number = {8},
  pages = {1139--1144},
  issn = {1083-8791},
  doi = {10.1016/j.bbmt.2014.04.003},
  urldate = {2024-10-04},
  abstract = {Donor chimerism after allogeneic stem cell transplantation (allo-SCT) is commonly used to predict overall survival (OS) and disease-free survival (DFS). Because chimerism is observed at 1 or more times after allo-SCT and not at baseline, if chimerism is in fact associated with OS or DFS, then the occurrence of either disease progression or death informatively censors (terminates) the observed chimerism process. This violates the assumptions underlying standard statistical regression methods for survival analysis, which may lead to biased conclusions. To assess the association between the longitudinal post--allo-SCT donor chimerism process and OS or DFS, we analyzed data from 195 patients with acute myelogenous leukemia (n~=~157) or myelodysplastic syndrome (n~=~38) who achieved complete remission after allo-SCT following a reduced-toxicity conditioning regimen of fludarabine/intravenous busulfan. Median follow-up was 31~months (range, 1.1 to 105~months). Fitted joint longitudinal-survival time models showed that a binary indicator of complete (100\%) donor chimerism and increasing percent of donor T~cells were significantly associated with longer OS, whereas decreasing percent of donor T~cells was highly significantly associated with shorter OS. Our analyses illustrate the usefulness of modeling repeated post--allo-SCT chimerism measurements as individual longitudinal processes jointly with OS and DFS to estimate their relationships.},
  keywords = {Allogeneic stem cell transplantation,AML,Chimerism,MDS},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\XRLPUZAH\\Tang et al. - 2014 - Increasing Chimerism after Allogeneic Stem Cell Tr.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\Y7JD23PQ\\S1083879114002195.html}
}

@article{taylor2002survival,
  title = {Survival Estimation and Testing via Multiple Imputation},
  author = {Taylor, Jeremy M G and Murray, Susan and Hsu, Chiu-Hsieh},
  year = {2002},
  journal = {Statistics \& probability letters},
  volume = {58},
  number = {3},
  pages = {221--232},
  publisher = {Elsevier}
}

@article{tefferiPrimaryMyelofibrosis20232023,
  title = {Primary Myelofibrosis: 2023 Update on Diagnosis, Risk-Stratification, and Management},
  shorttitle = {Primary Myelofibrosis},
  author = {Tefferi, Ayalew},
  year = {2023},
  journal = {American Journal of Hematology},
  volume = {98},
  number = {5},
  pages = {801--821},
  issn = {1096-8652},
  doi = {10.1002/ajh.26857},
  urldate = {2024-02-26},
  abstract = {Disease Overview Primary myelofibrosis (PMF) is a myeloproliferative neoplasm (MPN) characterized by stem cell-derived clonal myeloproliferation that is often but not always accompanied by JAK2, CALR, or MPL mutations; additional features include bone marrow reticulin/collagen fibrosis, aberrant inflammatory cytokine expression, anemia, hepatosplenomegaly, extramedullary hematopoiesis (EMH), constitutional symptoms, cachexia, risk of leukemic progression, and shortened survival. Diagnosis Bone marrow examination with cytogenetic and mutation studies provides integrated diagnostic information; presence of JAK2, CALR or MPL mutation is expected but not required. New Classification System The International Consensus Classification distinguishes ``prefibrotic'' from ``overtly fibrotic'' PMF; the former might mimic essential thrombocythemia (ET) in its presentation. Approximately 15\% of patients with ET or polycythemia vera (PV) might progress into post-ET/PV MF. Mutations SRSF2, ASXL1, and U2AF1-Q157 mutations predict inferior survival in PMF; RAS/CBL mutations predict resistance to ruxolitinib therapy. Type 1/like CALR mutation is associated with superior survival. Karyotype Very high-risk abnormalities include -7, inv (3), i(17q), +21, +19, 12p- and 11q-. Favorable risk abnormalities include normal karyotype or isolated +9, 13q-, 20q-, 1q abnormalities and loss of Y chromosome. Risk Stratification Contemporary prognostic systems include GIPSS (genetically-inspired prognostic scoring system) and MIPSS70+ version 2.0 (MIPSSv2; mutation-and karyotype-enhanced international prognostic scoring system). GIPSS is based exclusively on mutations and karyotype; MIPSSv2 includes, in addition, clinical risk factors. Risk-Adapted Therapy Observation alone is advised for MIPSSv2 ``low'' and ``very low'' risk disease (estimated 10-year survival 56\%--92\%); allogeneic hematopoietic stem cell transplant (AHSCT) is the preferred treatment of choice for ``very high'' and ``high'' risk disease (estimated 10-year survival 0--13\%), as well as in carefully selected patients with intermediate-risk disease (estimated 10-year survival 30\%). Drug therapy in MF is currently palliative and targets anemia, splenomegaly, and constitutional symptoms. JAK2 Inhibitors Ruxolitinib, fedratinib, and pacritinib are FDA approved and respectfully utilized in patients failing treatment with hydroxyurea, ruxolitinib, or with platelet count {$<$}50 {\texttimes} 10 (9)/L. Momelotinib is another JAK2 inhibitor that is poised for approval sometime in 2023 and has shown erythropoietic benefits, in addition to affecting spleen and symptom responses. Other Treatment Modalities Splenectomy is considered for drug-refractory splenomegaly and involved field radiotherapy for non-hepatosplenic EMH and extremity bone pain. New Directions New agents, alone or in combination with ruxolitinib, are currently under clinical trial investigation (ClinicalTrials.gov) and preliminary results were presented at the 2022 ASH annual meeting and highlighted in the current review.},
  copyright = {{\copyright} 2023 Wiley Periodicals LLC.},
  langid = {english},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\MKQ8NAIX\\Tefferi - 2023 - Primary myelofibrosis 2023 update on diagnosis, r.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\S6GL2JPP\\ajh.html}
}

@article{thomadakisJointModelingLongitudinal2022,
  title = {Joint Modeling of Longitudinal and Competing-Risk Data Using Cumulative Incidence Functions for the Failure Submodels Accounting for Potential Failure Cause Misclassification through Double Sampling},
  author = {Thomadakis, Christos and Meligkotsidou, Loukia and Yiannoutsos, Constantin T and Touloumi, Giota},
  year = {2022},
  month = nov,
  journal = {Biostatistics},
  pages = {kxac043},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxac043},
  urldate = {2023-10-12},
  abstract = {Most of the literature on joint modeling of longitudinal and competing-risk data is based on cause-specific hazards, although modeling of the cumulative incidence function (CIF) is an easier and more direct approach to evaluate the prognosis of an event. We propose a flexible class of shared parameter models to jointly model a normally distributed marker over time and multiple causes of failure using CIFs for the survival submodels, with CIFs depending on the ``true'' marker value over time (i.e., removing the measurement error). The generalized odds rate transformation is applied, thus a proportional subdistribution hazards model is a special case. The requirement that the all-cause CIF should be bounded by 1 is formally considered. The proposed models are extended to account for potential failure cause misclassification, where the true failure causes are available in a small random sample of individuals. We also provide a multistate representation of the whole population by defining mutually exclusive states based on the marker values and the competing risks. Based solely on the assumed joint model, we derive fully Bayesian posterior samples for state occupation and transition probabilities. The proposed approach is evaluated in a simulation study and, as an illustration, it is fitted to real data from people with HIV.},
  file = {C:\Users\efbonneville\Zotero\storage\SAURPSH5\Thomadakis et al. - 2022 - Joint modeling of longitudinal and competing-risk .pdf}
}

@article{tompsettUseNotrandomFully2018,
  title = {On the Use of the Not-at-Random Fully Conditional Specification ({{NARFCS}}) Procedure in Practice},
  author = {Tompsett, Daniel Mark and Leacy, Finbarr and {Moreno-Betancur}, Margarita and Heron, Jon and White, Ian R.},
  year = {2018},
  journal = {Statistics in Medicine},
  volume = {37},
  number = {15},
  pages = {2338--2353},
  issn = {1097-0258},
  doi = {10.1002/sim.7643},
  urldate = {2024-10-04},
  abstract = {The not-at-random fully conditional specification (NARFCS) procedure provides a flexible means for the imputation of multivariable missing data under missing-not-at-random conditions. Recent work has outlined difficulties with eliciting the sensitivity parameters of the procedure from expert opinion due to their conditional nature. Failure to adequately account for this conditioning will generate imputations that are inconsistent with the assumptions of the user. In this paper, we clarify the importance of correct conditioning of NARFCS sensitivity parameters and develop procedures to calibrate these sensitivity parameters by relating them to more easily elicited quantities, in particular, the sensitivity parameters from simpler pattern mixture models. Additionally, we consider how to include the missingness indicators as part of the imputation models of NARFCS, recommending including all of them in each model as default practice. Algorithms are developed to perform the calibration procedure and demonstrated on data from the Avon Longitudinal Study of Parents and Children, as well as with simulation studies.},
  copyright = {{\copyright} 2018 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {ALSPAC,FCS,MICE,MNAR,multiple imputations},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\G926KN42\\Tompsett et al. - 2018 - On the use of the not-at-random fully conditional .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\ABQMYME4\\sim.html}
}

@article{toorDynamicalSystemModeling2015,
  title = {Dynamical {{System Modeling}} of {{Immune Reconstitution}} after {{Allogeneic Stem Cell Transplantation Identifies Patients}} at {{Risk}} for {{Adverse Outcomes}}},
  author = {Toor, Amir A. and Sabo, Roy T. and Roberts, Catherine H. and Moore, Bonny L. and Salman, Salman R. and Scalora, Allison F. and Aziz, May T. and Shubar Ali, Ali S. and Hall, Charles E. and Meier, Jeremy and Thorn, Radhika M. and Wang, Elaine and Song, Shiyu and Miller, Kristin and Rizzo, Kathryn and Clark, William B. and McCarty, John M. and Chung, Harold M. and Manjili, Masoud H. and Neale, Michael C.},
  year = {2015},
  month = jul,
  journal = {Biology of Blood and Marrow Transplantation},
  volume = {21},
  number = {7},
  pages = {1237--1245},
  issn = {1083-8791},
  doi = {10.1016/j.bbmt.2015.03.011},
  urldate = {2023-11-04},
  abstract = {Systems that evolve over time and follow mathematical laws as they evolve are called dynamical systems. Lymphocyte recovery and clinical outcomes in 41 allograft recipients conditioned using antithymocyte globulin (ATG) and 4.5-Gy total body irradiation were studied to determine if immune reconstitution could be described as a dynamical system. Survival, relapse, and graft-versus-host disease (GVHD) were not significantly different in 2 cohorts of patients receiving different doses of ATG. However, donor-derived CD3+ cell reconstitution was superior in the lower ATG dose cohort, and there were fewer instances of donor lymphocyte infusion (DLI). Lymphoid recovery was plotted in each individual over time and demonstrated 1 of 3 sigmoid growth patterns: Pattern A (n~= 15) had rapid growth with high lymphocyte counts, pattern B (n~= 14) had slower growth with intermediate recovery, and pattern C (n~= 10) had poor lymphocyte reconstitution. There was a significant association between lymphocyte recovery patterns and both the rate of change of donor-derived CD3+ at day 30 after stem cell transplantation (SCT) and clinical outcomes. GVHD was observed more frequently with pattern A, relapse and DLI more so with pattern C, with a consequent survival advantage in patients with patterns A and B. We conclude that evaluating immune reconstitution after SCT as a dynamical system may differentiate patients at risk of adverse outcomes and allow early intervention to modulate that risk.},
  keywords = {Dynamical system,Graft-versus-host disease,Immune reconstitution,Stem cell transplant,T cells},
  file = {C:\Users\efbonneville\Zotero\storage\2REQHHAB\Toor et al. - 2015 - Dynamical System Modeling of Immune Reconstitution.pdf}
}

@incollection{tsiatis2005encycl,
  title = {Competing Risks},
  booktitle = {Encyclopedia of Biostatistics},
  author = {Tsiatis, A. A.},
  year = {2005},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/0470011815.b2a03035},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/0470011815.b2a03035},
  abstract = {Abstract ``Competing risks'' refers to the study of mortality patterns in a population of individuals, all subject to the same k {$\geq$} 2 causes of death. The object is to isolate the effects of a given risk or subset of risks. Certain quantities, such as the ``crude probability'' of dying of a particular disease in the presence of the other competing risks, and cause-specific hazard rates, and functions of them, are estimable from observable data without unverifiable assumptions. Other quantities, such as the ``net probability'' of dying of a disease in the absence of other causes of death cannot be estimated without unverifiable assumptions on the joint distribution of times to death from the various causes of death. Bounds on the net probability are available, however. We review these concepts and related methods of estimation and inference.},
  isbn = {978-0-470-01181-2},
  keywords = {absolute risk,cause-specific hazard rate,cause-specific subdistribution function,competing risks,crude probability,net probability,partial net probability,Peterson bounds},
  file = {C:\Users\efbonneville\Zotero\storage\77PUZI6C\Tsiatis - 2005 - Competing risks.pdf}
}

@article{tsvetanovaMissingDataWas2021,
  title = {Missing Data Was Handled Inconsistently in {{UK}} Prediction Models: A Review of Method Used},
  shorttitle = {Missing Data Was Handled Inconsistently in {{UK}} Prediction Models},
  author = {Tsvetanova, Antonia and Sperrin, Matthew and Peek, Niels and Buchan, Iain and Hyland, Stephanie and Martin, Glen P.},
  year = {2021},
  month = dec,
  journal = {Journal of Clinical Epidemiology},
  volume = {140},
  pages = {149--158},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2021.09.008},
  urldate = {2022-10-10},
  abstract = {Objectives No clear guidance exists on handling missing data at each stage of developing, validating and implementing a clinical prediction model (CPM). We aimed to review the approaches to handling missing data that underly the CPMs currently recommended for use in UK healthcare. Study Design and Setting A descriptive cross-sectional meta-epidemiological study aiming to identify CPMs recommended by the National Institute for Health and Care Excellence (NICE), which summarized how missing data is handled across their pipelines. Results A total of 23 CPMs were included through ``sampling strategy.'' Six missing data strategies were identified: complete case analysis (CCA), multiple imputation, imputation of mean values, k-nearest neighbours imputation, using an additional category for missingness, considering missing values as risk-factor-absent. 52\% of the development articles and 48\% of the validation articles did not report how missing data were handled. CCA was the most common approach used for development (40\%) and validation (44\%). At implementation, 57\% of the CPMs required complete data entry, whilst 43\% allowed missing values. Three CPMs had consistent paths in their pipelines. Conclusion A broad variety of methods for handling missing data underly the CPMs currently recommended for use in UK healthcare. Missing data handling strategies were generally inconsistent. Better quality assurance of CPMs needs greater clarity and consistency in handling of missing data.},
  langid = {english},
  keywords = {Imputation,Missing data,Missing data handling approaches,Predictive medicine,Prognosis,Statistical models},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\QVT4ESC8\\Tsvetanova et al. - 2021 - Missing data was handled inconsistently in UK pred.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\IJG9NJ4A\\S0895435621002882.html}
}

@article{uhlMetabolicReprogrammingDonor2020,
  title = {Metabolic Reprogramming of Donor {{T}} Cells Enhances Graft-versus-Leukemia Effects in Mice and Humans},
  author = {Uhl, Franziska M. and Chen, Sophia and O'Sullivan, David and {Edwards-Hicks}, Joy and Richter, Gesa and Haring, Eileen and Andrieux, Geoffroy and Halbach, Sebastian and Apostolova, Petya and B{\"u}scher, J{\"o}rg and Duquesne, Sandra and Melchinger, Wolfgang and Sauer, Barbara and Shoumariyeh, Khalid and {Schmitt-Graeff}, Annette and Kreutz, Marina and L{\"u}bbert, Michael and Duyster, Justus and Brummer, Tilman and Boerries, Melanie and Madl, Tobias and Blazar, Bruce R. and Gro{\ss}, Olaf and Pearce, Erika L. and Zeiser, Robert},
  year = {2020},
  month = oct,
  journal = {Science Translational Medicine},
  volume = {12},
  number = {567},
  pages = {eabb8969},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/scitranslmed.abb8969},
  urldate = {2023-11-04},
  abstract = {Acute myeloid leukemia (AML) relapse after allogeneic hematopoietic cell transplantation (allo-HCT) has a dismal prognosis. We found that T cells of patients relapsing with AML after allo-HCT exhibited reduced glycolysis and interferon-{$\gamma$} production. Functional studies in multiple mouse models of leukemia showed that leukemia-derived lactic acid (LA) interfered with T cell glycolysis and proliferation. Mechanistically, LA reduced intracellular pH in T cells, led to lower transcription of glycolysis-related enzymes, and decreased activity of essential metabolic pathways. Metabolic reprogramming by sodium bicarbonate (NaBi) reversed the LA-induced low intracellular pH, restored metabolite concentrations, led to incorporation of LA into the tricarboxylic acid cycle as an additional energy source, and enhanced graft-versus-leukemia activity of murine and human T cells. NaBi treatment of post--allo-HCT patients with relapsed AML improved metabolic fitness and interferon-{$\gamma$} production in T cells. Overall, we show that metabolic reprogramming of donor T cells is a pharmacological strategy for patients with relapsed AML after allo-HCT.},
  file = {C:\Users\efbonneville\Zotero\storage\VWEZIJI7\Uhl et al. - 2020 - Metabolic reprogramming of donor T cells enhances .pdf}
}

@article{unkelEstimandsAnalysisAdverse2019,
  title = {On Estimands and the Analysis of Adverse Events in the Presence of Varying Follow-up Times within the Benefit Assessment of Therapies},
  author = {Unkel, Steffen and Amiri, Marjan and Benda, Norbert and Beyersmann, Jan and Knoerzer, Dietrich and Kupas, Katrin and Langer, Frank and Leverkus, Friedhelm and Loos, Anja and Ose, Claudia and Proctor, Tanja and Schmoor, Claudia and Schwenke, Carsten and Skipka, Guido and Unnebrink, Kristina and Voss, Florian and Friede, Tim},
  year = {2019},
  journal = {Pharmaceutical Statistics},
  volume = {18},
  number = {2},
  pages = {166--183},
  issn = {1539-1612},
  doi = {10.1002/pst.1915},
  urldate = {2024-12-11},
  abstract = {The analysis of adverse events (AEs) is a key component in the assessment of a drug's safety profile. Inappropriate analysis methods may result in misleading conclusions about a therapy's safety and consequently its benefit-risk ratio. The statistical analysis of AEs is complicated by the fact that the follow-up times can vary between the patients included in a clinical trial. This paper takes as its focus the analysis of AE data in the presence of varying follow-up times within the benefit assessment of therapeutic interventions. Instead of approaching this issue directly and solely from an analysis point of view, we first discuss what should be estimated in the context of safety data, leading to the concept of estimands. Although the current discussion on estimands is mainly related to efficacy evaluation, the concept is applicable to safety endpoints as well. Within the framework of estimands, we present statistical methods for analysing AEs with the focus being on the time to the occurrence of the first AE of a specific type. We give recommendations which estimators should be used for the estimands described. Furthermore, we state practical implications of the analysis of AEs in clinical trials and give an overview of examples across different indications. We also provide a review of current practices of health technology assessment (HTA) agencies with respect to the evaluation of safety data. Finally, we describe problems with meta-analyses of AE data and sketch possible solutions.},
  copyright = {{\copyright} 2018 The Authors. Pharmaceutical Statistics Published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {adverse events,benefit assessment,clinical trials,estimands,safety data},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\5WK6K5EB\\Unkel et al. - 2019 - On estimands and the analysis of adverse events in.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\Z5E5AM2Q\\pst.html}
}

@techreport{vanbuurenFlexibleMultivariateImputation1999,
  title = {Flexible {{Multivariate Imputation}} by {{MICE}}},
  author = {{van Buuren}, S and {Oudshoorn, K.} and {TNO Preventie en Gezondheid}},
  year = {1999},
  month = jan,
  institution = {TNO},
  urldate = {2020-11-09},
  langid = {english},
  keywords = {Health,Historisch materiaal TNO}
}

@article{vanbuurenFullyConditionalSpecification2006,
  title = {Fully Conditional Specification in Multivariate Imputation},
  author = {{van Buuren}, S. and Brand, J. P. L. and {Groothuis-Oudshoorn}, C. G. M. and Rubin, D. B.},
  year = {2006},
  month = dec,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {76},
  number = {12},
  pages = {1049--1064},
  publisher = {Taylor \& Francis},
  issn = {0094-9655},
  doi = {10.1080/10629360600810434},
  urldate = {2020-11-08},
  abstract = {The use of the Gibbs sampler with fully conditionally specified models, where the distribution of each variable given the other variables is the starting point, has become a popular method to create imputations in incomplete multivariate data. The theoretical weakness of this approach is that the specified conditional densities can be incompatible, and therefore the stationary distribution to which the Gibbs sampler attempts to converge may not exist. This study investigates practical consequences of this problem by means of simulation. Missing data are created under four different missing data mechanisms. Attention is given to the statistical behavior under compatible and incompatible models. The results indicate that multiple imputation produces essentially unbiased estimates with appropriate coverage in the simple cases investigated, even for the incompatible models. Of particular interest is that these results were produced using only five Gibbs iterations starting from a simple draw from observed marginal distributions. It thus appears that, despite the theoretical weaknesses, the actual performance of conditional model specification for multivariate imputation can be quite good, and therefore deserves further study.},
  keywords = {Distributional compatibility,Gibbs sampling,Multiple imputation,Multivariate missing data,Proper imputation,Simulation}
}

@article{vanbuurenMiceMultivariateImputation2011,
  title = {Mice: {{Multivariate Imputation}} by {{Chained Equations}} in {{R}}},
  shorttitle = {Mice},
  author = {{van Buuren}, S and {Groothuis-Oudshoorn}, Karin},
  year = {2011},
  month = dec,
  journal = {Journal of Statistical Software},
  volume = {45},
  pages = {1--67},
  issn = {1548-7660},
  doi = {10.18637/jss.v045.i03},
  urldate = {2022-10-17},
  abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In mice, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. mice adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
  copyright = {Copyright (c) 2009 Stef van Buuren, Karin Groothuis-Oudshoorn},
  langid = {english}
}

@article{vandenbrouckeStrengtheningReportingObservational2007,
  title = {Strengthening the {{Reporting}} of {{Observational Studies}} in {{Epidemiology}} ({{STROBE}}): {{Explanation}} and {{Elaboration}}},
  shorttitle = {Strengthening the {{Reporting}} of {{Observational Studies}} in {{Epidemiology}} ({{STROBE}})},
  author = {Vandenbroucke, Jan P. and von Elm, Erik and Altman, Douglas G. and G{\o}tzsche, Peter C. and Mulrow, Cynthia D. and Pocock, Stuart J. and Poole, Charles and Schlesselman, James J. and Egger, Matthias and Initiative, for the STROBE},
  year = {2007},
  month = oct,
  journal = {PLOS Medicine},
  volume = {4},
  number = {10},
  pages = {e297},
  publisher = {Public Library of Science},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0040297},
  urldate = {2022-12-04},
  abstract = {Much medical research is observational. The reporting of observational studies is often of insufficient quality. Poor reporting hampers the assessment of the strengths and weaknesses of a study and the generalisability of its results. Taking into account empirical evidence and theoretical considerations, a group of methodologists, researchers, and editors developed the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) recommendations to improve the quality of reporting of observational studies. The STROBE Statement consists of a checklist of 22 items, which relate to the title, abstract, introduction, methods, results and discussion sections of articles. Eighteen items are common to cohort studies, case-control studies and cross-sectional studies and four are specific to each of the three study designs. The STROBE Statement provides guidance to authors about how to improve the reporting of observational studies and facilitates critical appraisal and interpretation of studies by reviewers, journal editors and readers. This explanatory and elaboration document is intended to enhance the use, understanding, and dissemination of the STROBE Statement. The meaning and rationale for each checklist item are presented. For each item, one or several published examples and, where possible, references to relevant empirical studies and methodological literature are provided. Examples of useful flow diagrams are also included. The STROBE Statement, this document, and the associated Web site (http://www.strobe-statement.org/) should be helpful resources to improve reporting of observational research.},
  langid = {english},
  keywords = {Cancer risk factors,Cardiovascular disease risk,Case-control studies,Cohort studies,Epidemiology,Medical risk factors,Observational studies,Oral contraceptive therapy},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\MBF3HYJR\\Vandenbroucke et al. - 2007 - Strengthening the Reporting of Observational Studi.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\RET78NZ9\\article.html}
}

@article{vanderpasDifferentCompetingRisks2018,
  title = {Different Competing Risks Models for Different Questions May Give Similar Results in Arthroplasty Registers in the Presence of Few Events},
  author = {{van der Pas}, St{\'e}phanie and Nelissen, Rob and Fiocco, Marta},
  year = {2018},
  month = mar,
  journal = {Acta Orthopaedica},
  volume = {89},
  number = {2},
  pages = {145--151},
  publisher = {Taylor \& Francis},
  issn = {1745-3674},
  doi = {10.1080/17453674.2018.1427314},
  urldate = {2024-02-20},
  abstract = {Background and purpose --- In arthroplasty registry studies, the analysis of time to revision is complicated by the competing risk of death. There are no clear guidelines for the choice between the 2 main adjusted analysis methods, cause-specific Cox and Fine--Gray regression, for orthopedic data. We investigated whether there are benefits, such as insight into different aspects of progression to revision, to using either 1 or both regression methods in arthroplasty registry studies in general, and specifically when the length of follow-up is short relative to the expected survival of the implants. Patients and methods --- Cause-specific Cox regression and Fine--Gray regression were performed on total hip (138,234 hips, 124,560 patients) and knee (139,070 knees, 125,213 patients) replacement data from the Dutch Arthroplasty Register (median follow-up 3.1 years, maximum 8 years), with sex, age, ASA score, diagnosis, and type of fixation as explanatory variables. The similarity of the resulting hazard ratios and confidence intervals was assessed visually and by computing the relative differences of the resulting subdistribution and cause-specific hazard ratios. Results --- The outcomes of the cause-specific Cox and Fine--Gray regressions were numerically very close. The largest relative difference between the hazard ratios was 3.5\%. Interpretation --- The most likely explanation for the similarity is that there are relatively few events (revisions and deaths), due to the short follow-up compared with the expected failure-free survival of the hip and knee prostheses. Despite the similarity, we recommend always performing both cause-specific Cox and Fine--Gray regression. In this way, both etiology and prediction can be investigated.},
  pmid = {29388452},
  file = {C:\Users\efbonneville\Zotero\storage\99LPRG5P\Van Der Pas et al. - 2018 - Different competing risks models for different que.pdf}
}

@book{vanhouwelingenDynamicPredictionClinical2012,
  title = {Dynamic {{Prediction}} in {{Clinical Survival Analysis}}},
  author = {{van Houwelingen}, Hans and Putter, Hein},
  year = {2012},
  month = jan,
  publisher = {CRC Press},
  address = {Boca Raton},
  doi = {10.1201/b11311},
  abstract = {There is a huge amount of literature on statistical models for the prediction of survival after diagnosis of a wide range of diseases like cancer, cardiovascular disease, and chronic kidney disease. Current practice is to use prediction models based on the Cox proportional hazards model and to present those as static models for remaining lifetime a},
  isbn = {978-0-429-09433-0}
}

@article{vazquezAnalyzingLeftTruncatedSamples2024,
  title = {Analyzing {{Left-Truncated Samples}} with the {{Cox Model}} in the {{Presence}} of {{Missing Covariates}}},
  author = {Vazquez, Omar and Locke, Hayley M. and Xie, Sharon X.},
  year = {2024},
  month = jul,
  journal = {Statistics in Biosciences},
  issn = {1867-1772},
  doi = {10.1007/s12561-024-09442-9},
  urldate = {2024-07-05},
  abstract = {Delayed enrollment of subjects into a time-to-event study may result in a sample with biased outcome and covariate distributions. Additionally, missing covariate data may arise in these studies when information is difficult to collect due to patient burden or high testing costs. Some common missing data strategies, such as multiple imputation (MI) and augmented inverse probability weighting (AIPW), involve modeling the distribution of the missing covariate, which may be inaccurate with a left-truncated sample. Through simulation studies, we explore the performance of these methods in estimating Cox regression parameters under a variety of truncation and missing data scenarios. We find that MI and AIPW may be approximately unbiased when truncation is very low. Otherwise, biased estimation of the covariate distribution can cause MI to perform poorly. Similarly, AIPW may rely more heavily on correct estimation of the probability of having non-missing covariates in some settings. We apply these approaches to a Parkinson's disease dementia biomarker study. In analyzing left-truncated data with missing covariates, careful consideration of the data characteristics and method assumptions is needed to obtain valid results.},
  langid = {english},
  keywords = {Augmented inverse probability weighting,Cox regression,Left truncation,Missing covariates,Multiple imputation},
  file = {C:\Users\efbonneville\Zotero\storage\VH87AR2A\Vazquez et al. - 2024 - Analyzing Left-Truncated Samples with the Cox Mode.pdf}
}

@article{vicente-saezOpenScienceNow2018,
  title = {Open {{Science}} Now: {{A}} Systematic Literature Review for an Integrated Definition},
  shorttitle = {Open {{Science}} Now},
  author = {{Vicente-Saez}, Ruben and {Martinez-Fuentes}, Clara},
  year = {2018},
  month = jul,
  journal = {Journal of Business Research},
  volume = {88},
  pages = {428--436},
  issn = {0148-2963},
  doi = {10.1016/j.jbusres.2017.12.043},
  urldate = {2024-10-22},
  abstract = {Open Science is a disruptive phenomenon that is emerging around the world and especially in Europe. Open Science brings about socio-cultural and technological change, based on openness and connectivity, on how research is designed, performed, captured, and assessed. Several studies show that there is a lack of awareness about what Open Science is, mainly due to the fact that there is no formal definition of Open Science. The purpose of this paper is to build a rigorous, integrated, and up-to-date definition of the Open Science phenomenon through a systematic literature review. The resulting definition ``Open Science is transparent and accessible knowledge that is shared and developed through collaborative networks'' helps the scientific community, the business world, political actors, and citizens to have a common and clear understanding about what Open Science is, and stimulates an open debate about the social, economic, and human added value of this phenomenon.},
  keywords = {Definition,Open access,Open innovation,Open science,Research and innovation management,Responsible research and innovation},
  file = {C:\Users\efbonneville\Zotero\storage\629KHQ37\S0148296317305441.html}
}

@article{vondemborneReducedintensityConditioningAllogeneic2009a,
  title = {Reduced-Intensity Conditioning Allogeneic Stem Cell Transplantation with Donor {{T-cell}} Depletion Using Alemtuzumab Added to the Graft ('{{Campath}} in the Bag')},
  author = {{von dem Borne}, Peter A. and Starrenburg, C. W. J. Ingrid and Halkes, Stijn J. M. and Marijt, W. A. Erik and Fibbe, Willem E. and Falkenburg, J. H. Frederik and Willemze, Roel},
  year = {2009},
  month = jun,
  journal = {Current Opinion in Oncology},
  volume = {21 Suppl 1},
  pages = {S27-29},
  issn = {1531-703X},
  doi = {10.1097/01.cco.0000357472.76337.0e},
  abstract = {Reduced-intensity conditioning (RIC) has allowed the use of allogeneic stem cell transplantation (alloSCT) for haematological malignancies in elderly patients. A major problem of this type of transplantation is the high incidence of persisting chronic graft-versus-host disease (GvHD), leading to increased morbidity and mortality. The inclusion of alemtuzumab added to the graft ('Campath in the bag') for donor T-cell depletion offers an easy procedure to diminish the incidence of GvHD. Good engraftment is observed in most patients, whereas almost no GvHD is observed after transplantation. Most patients become mixed chimeric after transplantation, requiring donor lymphocyte infusion for conversion to full donor chimerism. Although subsequent acute and chronic GvHD is observed in 50-60\% of patients, it is responsive to therapy in many patients, resulting in a low incidence of persisting chronic GvHD. AlloSCT with RIC and alemtuzumab-induced T-cell depletion offers a suitable platform for the investigation of novel cellular immunotherapy.},
  langid = {english},
  pmid = {19561408},
  keywords = {Adult,Aged,Alemtuzumab,Antibodies Monoclonal,Antibodies Monoclonal Humanized,Antibodies Neoplasm,Antineoplastic Agents,Graft vs Host Disease,Humans,Leukemia,Lymphocyte Depletion,Middle Aged,Stem Cell Transplantation,T-Lymphocytes,Transplantation Conditioning,Transplantation Homologous,Treatment Outcome}
}

@article{vonhippelHowManyImputations2020,
  title = {How {{Many Imputations Do You Need}}? {{A Two-stage Calculation Using}} a {{Quadratic Rule}}},
  shorttitle = {How {{Many Imputations Do You Need}}?},
  author = {{von Hippel}, Paul T.},
  year = {2020},
  month = aug,
  journal = {Sociological Methods \& Research},
  volume = {49},
  number = {3},
  pages = {699--718},
  publisher = {SAGE Publications Inc},
  issn = {0049-1241},
  doi = {10.1177/0049124117747303},
  urldate = {2020-12-13},
  abstract = {When using multiple imputation, users often want to know how many imputations they need. An old answer is that 2--10 imputations usually suffice, but this recommendation only addresses the efficiency of point estimates. You may need more imputations if, in addition to efficient point estimates, you also want standard error (SE) estimates that would not change (much) if you imputed the data again. For replicable SE estimates, the required number of imputations increases quadratically with the fraction of missing information (not linearly, as previous studies have suggested). I recommend a two-stage procedure in which you conduct a pilot analysis using a small-to-moderate number of imputations, then use the results to calculate the number of imputations that are needed for a final analysis whose SE estimates will have the desired level of replicability. I implement the two-stage procedure using a new SAS macro called \%mi\_combine and a new Stata command called how\_many\_imputations.},
  langid = {english},
  keywords = {imputation,incomplete data,missing data,missing values,multiple imputation}
}

@article{wangImputationMissingData2024,
  title = {Imputation of {{Missing Data}} for {{Time-to-Event Endpoints Using Retrieved Dropouts}}},
  author = {Wang, Shuai and Frederich, Robert and Mancuso, James P.},
  year = {2024},
  month = jan,
  journal = {Therapeutic Innovation \& Regulatory Science},
  volume = {58},
  number = {1},
  pages = {114--126},
  issn = {2168-4804},
  doi = {10.1007/s43441-023-00575-5},
  urldate = {2024-12-10},
  abstract = {We have explored several statistical approaches to impute missing time-to-event data that arise from outcome trials with relatively long follow-up periods. Aligning with the primary estimand, such analyses evaluate the robustness of results by imposing an assumption different from censoring at random (CAR). Although there have been debates over which assumption and which method is more appropriate to be applied to the imputation, we propose to use the collection of retrieved dropouts as the basis of missing data imputation. As retrieved dropouts share a similar disposition, such as treatment discontinuation, with subjects who have missing data, they can reasonably be assumed to characterize the distribution of time-to-event among subjects with missing data. In terms of computational intensity and robustness to violation of underlying distributional assumption, we have compared parametric approaches via MCMC or MLE multivariate sampling procedures to a non-parametric bootstrap approach with respect to baseline hazard function. Each of these approaches follows a process of multiple imputation (``proper imputations''), analysis of complete datasets, and final combination. The type-I error, and power rates are examined under a wide range of scenarios to inform the performance characteristics. A subset of a real unblinded phase III CVOT is used to demonstrate the application of the proposed approaches, compared to the Cox proportional hazards model and jump-to-reference multiple imputation.},
  langid = {english},
  keywords = {Cardiovascular outcome trial,Estimand,Informative censoring,Missing data,Multiple imputation,Proper imputation,Time to event},
  file = {C:\Users\efbonneville\Zotero\storage\RFKWEV26\Wang et al. - 2024 - Imputation of Missing Data for Time-to-Event Endpo.pdf}
}

@article{whiteAdjustingPartiallyMissing2005,
  title = {Adjusting for Partially Missing Baseline Measurements in Randomized Trials},
  author = {White, Ian R. and Thompson, Simon G.},
  year = {2005},
  journal = {Statistics in Medicine},
  volume = {24},
  number = {7},
  pages = {993--1007},
  issn = {1097-0258},
  doi = {10.1002/sim.1981},
  urldate = {2023-01-27},
  abstract = {Adjustment for baseline variables in a randomized trial can increase power to detect a treatment effect. However, when baseline data are partly missing, analysis of complete cases is inefficient. We consider various possible improvements in the case of normally distributed baseline and outcome variables. Joint modelling of baseline and outcome is the most efficient method. Mean imputation is an excellent alternative, subject to three conditions. Firstly, if baseline and outcome are correlated more than about 0.6 then weighting should be used to allow for the greater information from complete cases. Secondly, imputation should be carried out in a deterministic way, using other baseline variables if possible, but not using randomized arm or outcome. Thirdly, if baselines are not missing completely at random, then a dummy variable for missingness should be included as a covariate (the missing indicator method). The methods are illustrated in a randomized trial in community psychiatry. Copyright {\copyright} 2004 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {analysis of covariance,imputation,missing covariate,power,randomized trials},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\DVPWMXNZ\\White and Thompson - 2005 - Adjusting for partially missing baseline measureme.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\WENUGDGR\\sim.html}
}

@article{whiteBiasEfficiencyMultiple2010,
  title = {Bias and Efficiency of Multiple Imputation Compared with Complete-Case Analysis for Missing Covariate Values},
  author = {White, Ian R. and Carlin, John B.},
  year = {2010},
  journal = {Statistics in Medicine},
  volume = {29},
  number = {28},
  pages = {2920--2931},
  issn = {1097-0258},
  doi = {10.1002/sim.3944},
  urldate = {2020-10-25},
  abstract = {When missing data occur in one or more covariates in a regression model, multiple imputation (MI) is widely advocated as an improvement over complete-case analysis (CC). We use theoretical arguments and simulation studies to compare these methods with MI implemented under a missing at random assumption. When data are missing completely at random, both methods have negligible bias, and MI is more efficient than CC across a wide range of scenarios. For other missing data mechanisms, bias arises in one or both methods. In our simulation setting, CC is biased towards the null when data are missing at random. However, when missingness is independent of the outcome given the covariates, CC has negligible bias and MI is biased away from the null. With more general missing data mechanisms, bias tends to be smaller for MI than for CC. Since MI is not always better than CC for missing covariate problems, the choice of method should take into account what is known about the missing data mechanism in a particular substantive application. Importantly, the choice of method should not be based on comparison of standard errors. We propose new ways to understand empirical differences between MI and CC, which may provide insights into the appropriateness of the assumptions underlying each method, and we propose a new index for assessing the likely gain in precision from MI: the fraction of incomplete cases among the observed values of a covariate (FICO). Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {complete-case analysis,missing covariates,missing data,multiple imputation}
}

@article{whiteImputingMissingCovariate2009,
  title = {Imputing Missing Covariate Values for the {{Cox}} Model},
  author = {White, Ian R. and Royston, Patrick},
  year = {2009},
  journal = {Statistics in Medicine},
  volume = {28},
  number = {15},
  pages = {1982--1998},
  issn = {1097-0258},
  doi = {10.1002/sim.3618},
  urldate = {2020-03-31},
  abstract = {Multiple imputation is commonly used to impute missing data, and is typically more efficient than complete cases analysis in regression analysis when covariates have missing values. Imputation may be performed using a regression model for the incomplete covariates on other covariates and, importantly, on the outcome. With a survival outcome, it is a common practice to use the event indicator D and the log of the observed event or censoring time T in the imputation model, but the rationale is not clear. We assume that the survival outcome follows a proportional hazards model given covariates X and Z. We show that a suitable model for imputing binary or Normal X is a logistic or linear regression on the event indicator D, the cumulative baseline hazard H0(T), and the other covariates Z. This result is exact in the case of a single binary covariate; in other cases, it is approximately valid for small covariate effects and/or small cumulative incidence. If we do not know H0(T), we approximate it by the Nelson--Aalen estimator of H(T) or estimate it by Cox regression. We compare the methods using simulation studies. We find that using logT biases covariate-outcome associations towards the null, while the new methods have lower bias. Overall, we recommend including the event indicator and the Nelson--Aalen estimator of H(T) in the imputation model. Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {survival},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\CBXUZGJ7\\White and Royston - 2009 - Imputing missing covariate values for the Cox mode.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\3K63MTRJ\\sim.html}
}

@article{whiteMissingDataPart2022,
  title = {Missing Data, Part 7. {{Pitfalls}} in Doing Multiple Imputation},
  author = {White, Ian R. and Pandis, Nikolaos and Pham, Tra My},
  year = {2022},
  month = dec,
  journal = {American Journal of Orthodontics and Dentofacial Orthopedics},
  volume = {162},
  number = {6},
  pages = {975--977},
  issn = {0889-5406},
  doi = {10.1016/j.ajodo.2022.08.013},
  urldate = {2022-12-04},
  langid = {english},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\LQVZNN9K\\White et al. - 2022 - Missing data, part 7. Pitfalls in doing multiple i.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\24T8QSGA\\S0889540622005406.html}
}

@misc{WhyYouShould,
  title = {Why You Should Avoid Using Multiple {{Fine}}--{{Gray}} Models: Insights from (Attempts at) Simulating Proportional Subdistribution Hazards Data {\textbar} {{Journal}} of the {{Royal Statistical Society Series A}}: {{Statistics}} in {{Society}} {\textbar} {{Oxford Academic}}},
  urldate = {2024-08-28},
  howpublished = {https://academic.oup.com/jrsssa/advance-article/doi/10.1093/jrsssa/qnae056/7700140},
  file = {C:\Users\efbonneville\Zotero\storage\2B9C8P3J\7700140.html}
}

@article{wolbersPrognosticModelsCompeting2009,
  title = {Prognostic {{Models With Competing Risks}}: {{Methods}} and {{Application}} to {{Coronary Risk Prediction}}},
  shorttitle = {Prognostic {{Models With Competing Risks}}},
  author = {Wolbers, Marcel and Koller, Michael T. and Witteman, Jacqueline C. M. and Steyerberg, Ewout W.},
  year = {2009},
  journal = {Epidemiology},
  volume = {20},
  number = {4},
  pages = {555--561},
  publisher = {Lippincott Williams \& Wilkins},
  issn = {1044-3983},
  urldate = {2023-10-06},
  abstract = {Clinical decision-making often relies on a subject's absolute risk of a disease event of interest. However, in a frail population, competing risk events may preclude the occurrence of the event of interest. We review competing-risk regression models with a view toward predictive modeling. We show how measures of prognostic performance (such as calibration and discrimination) can be adapted to the competing-risks setting. An example of coronary heart disease (CHD) prediction in women aged 55--90 years in the Rotterdam study is used to illustrate the proposed methods, and to compare the Fine and Gray regression model to 2 alternative approaches: (1) a standard Cox survival model, which ignores the competing risk of non-CHD death, and (2) a cause-specific hazards model, which combines proportional hazards models for the event of interest and the competing event. The Fine and Gray model and the cause-specific hazards model perform similarly. However, the standard Cox model substantially overestimates 10-year risk of CHD; it classifies 18\% of the individuals as high risk ({$>$}20\%), compared with only 8\% according to the Fine and Gray model. We conclude that competing risks have to be considered explicitly in frail populations such as the elderly.},
  file = {C:\Users\efbonneville\Zotero\storage\N2U9SYBJ\Wolbers et al. - 2009 - Prognostic Models With Competing Risks Methods an.pdf}
}

@article{woodEstimationUsePredictions2015,
  title = {The Estimation and Use of Predictions for the Assessment of Model Performance Using Large Samples with Multiply Imputed Data},
  author = {Wood, Angela M. and Royston, Patrick and White, Ian R.},
  year = {2015},
  journal = {Biometrical Journal},
  volume = {57},
  number = {4},
  pages = {614--632},
  issn = {1521-4036},
  doi = {10.1002/bimj.201400004},
  urldate = {2020-10-23},
  abstract = {Multiple imputation can be used as a tool in the process of constructing prediction models in medical and epidemiological studies with missing covariate values. Such models can be used to make predictions for model performance assessment, but the task is made more complicated by the multiple imputation structure. We summarize various predictions constructed from covariates, including multiply imputed covariates, and either the set of imputation-specific prediction model coefficients or the pooled prediction model coefficients. We further describe approaches for using the predictions to assess model performance. We distinguish between ideal model performance and pragmatic model performance, where the former refers to the model's performance in an ideal clinical setting where all individuals have fully observed predictors and the latter refers to the model's performance in a real-world clinical setting where some individuals have missing predictors. The approaches are compared through an extensive simulation study based on the UK700 trial. We determine that measures of ideal model performance can be estimated within imputed datasets and subsequently pooled to give an overall measure of model performance. Alternative methods to evaluate pragmatic model performance are required and we propose constructing predictions either from a second set of covariate imputations which make no use of observed outcomes, or from a set of partial prediction models constructed for each potential observed pattern of covariate. Pragmatic model performance is generally lower than ideal model performance. We focus on model performance within the derivation data, but describe how to extend all the methods to a validation dataset.},
  copyright = {{\copyright} 2015 The Author. Biometrical Journal published by WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  langid = {english}
}

@article{wrightPersonalisedNeedCare2021,
  title = {Personalised Need of {{Care}} in an {{Ageing Society}}: {{The Making}} of a {{Prediction Tool Based}} on {{Register Data}}},
  shorttitle = {Personalised Need of {{Care}} in an {{Ageing Society}}},
  author = {Wright, Marvin N. and Kusumastuti, Sasmita and Mortensen, Laust H. and Westendorp, Rudi G. J. and Gerds, Thomas A.},
  year = {2021},
  month = oct,
  journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
  volume = {184},
  number = {4},
  pages = {1199--1219},
  issn = {0964-1998},
  doi = {10.1111/rssa.12644},
  urldate = {2023-10-06},
  abstract = {Danish municipalities monitor older persons who are at high risk of declining health and would later need home care services. However, there is no established strategy yet on how to accurately identify those who are at high risk. Therefore, there is great potential to optimise the municipalities' prevention strategies. Denmark's comprehensive set of electronic population registers provide longitudinal data that cover individual and household socio-demographics and medical history. Using these data, we developed and applied recurrent neural networks to predict the risk of a need of care services in the future and thus identify individuals who would benefit the most from the municipalities' prevention strategies. We compared our recurrent neural network model to prediction models based on Cox regression and Fine--Gray regression in terms of calibration and discrimination. Challenges for the prediction modelling were the competing risk of death and the longitudinal information on the registered life course data.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\AIXMLUK4\\Wright et al. - 2021 - Personalised need of Care in an Ageing Society Th.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\JC2TA472\\7068833.html}
}

@article{yanirImmuneReconstitutionAllogeneic2022,
  title = {Immune {{Reconstitution After Allogeneic Haematopoietic Cell Transplantation}}: {{From Observational Studies}} to {{Targeted Interventions}}},
  shorttitle = {Immune {{Reconstitution After Allogeneic Haematopoietic Cell Transplantation}}},
  author = {Yanir, Asaf and Schulz, Ansgar and Lawitschka, Anita and Nierkens, Stefan and Eyrich, Matthias},
  year = {2022},
  journal = {Frontiers in Pediatrics},
  volume = {9},
  issn = {2296-2360},
  urldate = {2023-11-04},
  abstract = {Immune reconstitution (IR) after allogeneic haematopoietic cell transplantation (HCT) represents a central determinant of the clinical post-transplant course, since the majority of transplant-related outcome parameters such as graft-vs.-host disease (GvHD), infectious complications, and relapse are related to the velocity, quantity and quality of immune cell recovery. Younger age at transplant has been identified as the most important positive prognostic factor for favourable IR post-transplant and, indeed, accelerated immune cell recovery in children is most likely the pivotal contributing factor to lower incidences of GvHD and infectious complications in paediatric allogeneic HCT. Although our knowledge about the mechanisms of IR has significantly increased over the recent years, strategies to influence IR are just evolving. In this review, we will discuss different patterns of IR during various time points post-transplant and their impact on outcome. Besides IR patterns and cellular phenotypes, recovery of antigen-specific immune cells, for example virus-specific T cells, has recently gained increasing interest, as certain threshold levels of antigen-specific T cells seem to confer protection against severe viral disease courses. In contrast, the association between IR and a possible graft-vs. leukaemia effect is less well-understood. Finally, we will present current concepts of how to improve IR and how this could change transplant procedures in the near future.},
  file = {C:\Users\efbonneville\Zotero\storage\IEYDRAUE\Yanir et al. - 2022 - Immune Reconstitution After Allogeneic Haematopoie.pdf}
}

@article{youngCausalFrameworkClassical2020a,
  title = {A Causal Framework for Classical Statistical Estimands in Failure-Time Settings with Competing Events},
  author = {Young, Jessica G. and Stensrud, Mats J. and Tchetgen Tchetgen, Eric J. and Hern{\'a}n, Miguel A.},
  year = {2020},
  journal = {Statistics in Medicine},
  volume = {39},
  number = {8},
  pages = {1199--1236},
  issn = {1097-0258},
  doi = {10.1002/sim.8471},
  urldate = {2024-08-22},
  abstract = {In failure-time settings, a competing event is any event that makes it impossible for the event of interest to occur. For example, cardiovascular disease death is a competing event for prostate cancer death because an individual cannot die of prostate cancer once he has died of cardiovascular disease. Various statistical estimands have been defined as possible targets of inference in the classical competing risks literature. Many reviews have described these statistical estimands and their estimating procedures with recommendations about their use. However, this previous work has not used a formal framework for characterizing causal effects and their identifying conditions, which makes it difficult to interpret effect estimates and assess recommendations regarding analytic choices. Here we use a counterfactual framework to explicitly define each of these classical estimands. We clarify that, depending on whether competing events are defined as censoring events, contrasts of risks can define a total effect of the treatment on the event of interest or a direct effect of the treatment on the event of interest not mediated by the competing event. In contrast, regardless of whether competing events are defined as censoring events, counterfactual hazard contrasts cannot generally be interpreted as causal effects. We illustrate how identifying assumptions for all of these counterfactual estimands can be represented in causal diagrams, in which competing events are depicted as time-varying covariates. We present an application of these ideas to data from a randomized trial designed to estimate the effect of estrogen therapy on prostate cancer mortality.},
  copyright = {{\copyright} 2020 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {causal inference,competing risks,g-formula,inverse probability weighting,longitudinal data,survival analysis},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\5JDNAN3G\\Young et al. - 2020 - A causal framework for classical statistical estim.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\EE9ZA8T9\\sim.html}
}

@article{yunFindingSweetSpot2013,
  title = {Finding the {{Sweet Spot}} for {{Donor Lymphocyte Infusions}}},
  author = {Yun, Hyun Don and Waller, Edmund K.},
  year = {2013},
  month = apr,
  journal = {Biology of Blood and Marrow Transplantation},
  volume = {19},
  number = {4},
  pages = {507--508},
  publisher = {Elsevier},
  issn = {1083-8791, 1523-6536},
  doi = {10.1016/j.bbmt.2013.02.005},
  urldate = {2023-11-04},
  langid = {english},
  pmid = {23416853},
  file = {C:\Users\efbonneville\Zotero\storage\CTMGYA9X\Yun and Waller - 2013 - Finding the Sweet Spot for Donor Lymphocyte Infusi.pdf}
}

@article{zhuConvergencePropertiesSequential2015,
  title = {Convergence {{Properties}} of a {{Sequential Regression Multiple Imputation Algorithm}}},
  author = {Zhu, Jian and Raghunathan, Trivellore E.},
  year = {2015},
  month = jul,
  journal = {Journal of the American Statistical Association},
  volume = {110},
  number = {511},
  pages = {1112--1124},
  publisher = {ASA Website},
  issn = {0162-1459},
  doi = {10.1080/01621459.2014.948117},
  urldate = {2024-08-23},
  abstract = {A sequential regression or chained equations imputation approach uses a Gibbs sampling-type iterative algorithm that imputes the missing values using a sequence of conditional regression models. It is a flexible approach for handling different types of variables and complex data structures. Many simulation studies have shown that the multiple imputation inferences based on this procedure have desirable repeated sampling properties. However, a theoretical weakness of this approach is that the specification of a set of conditional regression models may not be compatible with a joint distribution of the variables being imputed. Hence, the convergence properties of the iterative algorithm are not well understood. This article develops conditions for convergence and assesses the properties of inferences from both compatible and incompatible sequence of regression models. The results are established for the missing data pattern where each subject may be missing a value on at most one variable. The sequence of regression models are assumed to be empirically good fit for the data chosen by the imputer based on appropriate model diagnostics. The results are used to develop criteria for the choice of regression models. Supplementary materials for this article are available online.},
  keywords = {Bayesian analysis,Chained equations,Compatible conditionals,Conditional specifications,Exponential family,Gibbs sampling,Missing data.},
  file = {C:\Users\efbonneville\Zotero\storage\BQJWJ2C4\Zhu and Raghunathan - 2015 - Convergence Properties of a Sequential Regression .pdf}
}
